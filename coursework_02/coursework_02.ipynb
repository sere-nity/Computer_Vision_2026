{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XRxHiKdGHiT"
      },
      "source": [
        "# Coursework 2: Image segmentation\n",
        "\n",
        "In this coursework you will develop and train a convolutional neural network for brain tumour segmentation. Please read both the text and the code in this notebook to get an idea what you are expected to implement. Pay attention to the missing code blocks that look like this:\n",
        "\n",
        "```\n",
        "### Insert your code ###\n",
        "...\n",
        "### End of your code ###\n",
        "```\n",
        "## What is expected?\n",
        "\n",
        "* Complete and run the code using `jupyter-lab`.\n",
        "\n",
        "* Export (File | Save and Export Notebook As...) the notebook as a PDF file, which contains your code, results and answers, and upload the PDF file onto [Scientia](https://scientia.doc.ic.ac.uk).\n",
        "\n",
        "* If Jupyter complains issues during exporting, it is likely that [pandoc](https://pandoc.org/installing.html) or latex is not installed, or their paths have not been included. You can install the relevant libraries and retry. Alternatively, use the Print function of your browser to export the PDF file.\n",
        "\n",
        "* If Jupyter-lab does not work for you at the end, alternatively, you can use Google Colab to write the code and export the PDF file.\n",
        "\n",
        "## Dependencies\n",
        "\n",
        "You need to install Jupyter-Lab (https://jupyterlab.readthedocs.io/en/stable/getting_started/installation.html) and other libraries used in this coursework, such as by running the command:\n",
        "`pip3 install [package_name]`\n",
        "\n",
        "## GPU resource\n",
        "\n",
        "The coursework is developed to be able to run on CPU, as all images have been pre-processed to be 2D and of a smaller size, compared to original 3D volumes.\n",
        "\n",
        "However, to save training time, you may want to use GPU. In that case, you can run this notebook on Google Colab. On Google Colab, go to the menu, Runtime - Change runtime type, and select **GPU** as the hardware acceleartor. At the end, please still export everything and submit as a PDF file on Scientia.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting imageio\n",
            "  Obtaining dependency information for imageio from https://files.pythonhosted.org/packages/fb/fe/301e0936b79bcab4cacc7548bf2853fc28dced0a578bab1f7ef53c9aa75b/imageio-2.37.2-py3-none-any.whl.metadata\n",
            "  Downloading imageio-2.37.2-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: torch in c:\\users\\serene\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.3.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\serene\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.26.0)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\serene\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.8.0)\n",
            "Requirement already satisfied: pillow>=8.3.2 in c:\\users\\serene\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from imageio) (10.0.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\serene\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\serene\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (4.12.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\serene\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\serene\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\serene\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in c:\\users\\serene\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (2024.5.0)\n",
            "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\serene\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (2021.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\serene\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\serene\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\serene\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\serene\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\serene\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (23.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\serene\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\serene\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\serene\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
            "Requirement already satisfied: tbb==2021.* in c:\\users\\serene\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.12.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\serene\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\serene\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\serene\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
            "Downloading imageio-2.37.2-py3-none-any.whl (317 kB)\n",
            "   ---------------------------------------- 0.0/317.6 kB ? eta -:--:--\n",
            "   - -------------------------------------- 10.2/317.6 kB ? eta -:--:--\n",
            "   --------------- ------------------------ 122.9/317.6 kB 1.4 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 215.0/317.6 kB 1.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------  317.4/317.6 kB 1.8 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 317.6/317.6 kB 1.8 MB/s eta 0:00:00\n",
            "Installing collected packages: imageio\n",
            "Successfully installed imageio-2.37.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 26.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# Install required packages (run this cell once if you get ModuleNotFoundError)\n",
        "!pip install imageio torch numpy matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Eq1KWmR3HWYV"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "# These libraries should be sufficient for this tutorial.\n",
        "# However, if any other library is needed, please install by yourself.\n",
        "import tarfile\n",
        "import imageio\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4TX-CXBHW4c"
      },
      "source": [
        "## Q1. Download and visualise the imaging dataset.\n",
        "\n",
        "The dataset is a public brain imaging dataset from [Medical Decathlon Challenge](http://medicaldecathlon.com/). To save the storage and reduce the computational cost for this tutorial, we extract 2D image slices from the original 3D volumes (T1-Gd contrast enhanced imaging) and downsample the 2D images.\n",
        "\n",
        "The dataset consists of a training set and a test set. Each image is of dimension 120 x 120, with a corresponding label map of the same dimension. There are four number of classes in the label map:\n",
        "\n",
        "- 0: background\n",
        "- 1: edema\n",
        "- 2: non-enhancing tumour\n",
        "- 3: enhancing tumour"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mt93oQ8xZkE9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset\n",
        "# If you use Ubuntu, wget would natively work.\n",
        "# If you use Mac or Windows, which does not have the wget command, you can copy the URL to the web browser and download the file.\n",
        "!wget https://www.dropbox.com/s/zmytk2yu284af6t/Task01_BrainTumour_2D.tar.gz\n",
        "\n",
        "# Unzip the '.tar.gz' file to the current directory\n",
        "datafile = tarfile.open('Task01_BrainTumour_2D.tar.gz')\n",
        "datafile.extractall()\n",
        "datafile.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vu_BTL0x6o5a"
      },
      "source": [
        "## Visualise a random set of 4 training images along with their label maps.\n",
        "\n",
        "Suggested colour map for brain MR image:\n",
        "```\n",
        "cmap = 'gray'\n",
        "```\n",
        "\n",
        "Suggested colour map for segmentation map:\n",
        "```\n",
        "cmap = colors.ListedColormap(['black', 'green', 'blue', 'red'])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3fgubCRC6m4k"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Serene\\AppData\\Local\\Temp\\ipykernel_28428\\1869802806.py:24: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
            "  img = imageio.imread(img_path)\n",
            "C:\\Users\\Serene\\AppData\\Local\\Temp\\ipykernel_28428\\1869802806.py:28: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
            "  mask = imageio.imread(mask_path)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x1200 with 8 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "### Insert your code ###\n",
        "data_dir = 'Task01_BrainTumour_2D'\n",
        "\n",
        "image_dir = os.path.join(data_dir, 'training_images')\n",
        "label_dir = os.path.join(data_dir, 'training_labels')\n",
        "\n",
        "images = sorted(os.listdir(image_dir))\n",
        "labels = sorted(os.listdir(label_dir))\n",
        "\n",
        "def visualise_random_samples(image_dir, label_dir, num_samples=4):\n",
        "    \"\"\" Visualise a random set of 4 training images along with their label maps \"\"\"\n",
        "    # Get a random set of 4 images and labels\n",
        "    random_indices = random.sample(range(len(images)), num_samples)\n",
        "\n",
        "    # define figure \n",
        "    fig, axes = plt.subplots(num_samples, 2, figsize=(8, 12))\n",
        "\n",
        "    # define segmentation colormap\n",
        "    cmap = colors.ListedColormap(['black', 'green', 'blue', 'red'])\n",
        "\n",
        "    for i, idx in enumerate(random_indices):\n",
        "        # load MRI image\n",
        "        img_path = os.path.join(image_dir, images[idx])\n",
        "        img = imageio.imread(img_path)\n",
        "\n",
        "        #load segmentation mask \n",
        "        mask_path = os.path.join(label_dir, labels[idx])\n",
        "        mask = imageio.imread(mask_path)\n",
        "\n",
        "        # plot MRI image\n",
        "        axes[i, 0].imshow(img, cmap='gray')\n",
        "        axes[i, 0].set_title(f\"MRI Image {idx}\")\n",
        "        axes[i, 0].axis('off')\n",
        "\n",
        "        # plot segmentation mask\n",
        "        axes[i, 1].imshow(mask, cmap=cmap)\n",
        "        axes[i, 1].set_title(f\"Segmentation Mask {idx}\")\n",
        "        axes[i, 1].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# call the function\n",
        "visualise_random_samples(image_dir, label_dir)\n",
        "        \n",
        "    \n",
        "### End of your code ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xWGT3KaML-D"
      },
      "source": [
        "## Q2. Implement a dataset class.\n",
        "\n",
        "It can read the imaging dataset and get items, pairs of images and label maps, to be used as training batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6p6wFZ3na5z9"
      },
      "outputs": [],
      "source": [
        "def normalise_intensity(image, thres_roi=1.0):\n",
        "    \"\"\" Normalise the image intensity by the mean and standard deviation \"\"\"\n",
        "    # ROI defines the image foreground\n",
        "    val_l = np.percentile(image, thres_roi)\n",
        "    roi = (image >= val_l)\n",
        "    mu, sigma = np.mean(image[roi]), np.std(image[roi])\n",
        "    eps = 1e-6\n",
        "    image2 = (image - mu) / (sigma + eps)\n",
        "    return image2\n",
        "\n",
        "\n",
        "class BrainImageSet(Dataset):\n",
        "    \"\"\" Brain image set \"\"\"\n",
        "    def __init__(self, image_path, label_path='', deploy=False):\n",
        "        self.image_path = image_path\n",
        "        self.deploy = deploy\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "\n",
        "        image_names = sorted(os.listdir(image_path))\n",
        "        for image_name in image_names:\n",
        "            # Read the image\n",
        "            image = imageio.v2.imread(os.path.join(image_path, image_name))\n",
        "            self.images += [image]\n",
        "\n",
        "            # Read the label map\n",
        "            if not self.deploy:\n",
        "                label_name = os.path.join(label_path, image_name)\n",
        "                label = imageio.v2.imread(label_name)\n",
        "                self.labels += [label]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get an image and perform intensity normalisation\n",
        "        # Dimension: XY\n",
        "        image = normalise_intensity(self.images[idx])\n",
        "\n",
        "        # Get its label map\n",
        "        # Dimension: XY\n",
        "        label = self.labels[idx]\n",
        "        return image, label\n",
        "\n",
        "    def get_random_batch(self, batch_size):\n",
        "        # Get a batch of paired images and label maps\n",
        "        # Dimension of images: NCXY\n",
        "        # Dimension of labels: NXY\n",
        "        images, labels = [], []\n",
        "\n",
        "        ### Insert your code ###\n",
        "        \n",
        "        random_indices = random.sample(range(len(self.images)), batch_size)\n",
        "\n",
        "        for idx in random_indices:\n",
        "            #normalise image\n",
        "            img = normalise_intensity(self.images[idx])\n",
        "            images.append(img)\n",
        "\n",
        "            if not self.deploy:\n",
        "                labels.append(self.labels[idx])\n",
        "        # Return numpy arrays: images (N,1,X,Y) = NCXY, labels (N,X,Y) = NXY\n",
        "        images = np.stack(images, axis=0)\n",
        "        images = np.expand_dims(images, axis=1)  # add channel dim -> NCXY\n",
        "        if not self.deploy:\n",
        "            labels = np.stack(labels, axis=0)   # NXY\n",
        "        else:\n",
        "            labels = None\n",
        "        ### End of your code ###\n",
        "        return images, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pa4ZpawDNmwu"
      },
      "source": [
        "## Q3. Build a U-net architecture.\n",
        "\n",
        "Implement a U-net architecture for image segmentation. If you are not familiar with U-net, you can read this paper:\n",
        "\n",
        "[1] Olaf Ronneberger et al. [U-Net: Convolutional networks for biomedical image segmentation](https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28). MICCAI, 2015.\n",
        "\n",
        "For the first convolutional layer, you can start with 16 filters. We have implemented the encoder path. Please complete the decoder path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IMPmBZVGb1aI"
      },
      "outputs": [],
      "source": [
        "\"\"\" U-net \"\"\"\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, input_channel=1, output_channel=1, num_filter=16):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        # BatchNorm: by default during training this layer keeps running estimates\n",
        "        # of its computed mean and variance, which are then used for normalization\n",
        "        # during evaluation.\n",
        "\n",
        "        # Encoder path\n",
        "        n = num_filter  # 16\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(input_channel, n, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(n),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(n, n, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(n),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        n *= 2  # 32\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(int(n / 2), n, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(n),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(n, n, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(n),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        n *= 2  # 64\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(int(n / 2), n, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(n),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(n, n, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(n),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        n *= 2  # 128\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(int(n / 2), n, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(n),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(n, n, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(n),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        # Decoder path\n",
        "        ### Insert your code ###\n",
        "        n //= 2  # 64\n",
        "        self.upconv4 = nn.ConvTranspose2d(n * 2, n, kernel_size=2, stride=2)\n",
        "        self.conv5 = nn.Sequential(\n",
        "            nn.Conv2d(n * 2, n, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(n),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(n, n, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(n),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        n //= 2  # 32\n",
        "        self.upconv3 = nn.ConvTranspose2d(n * 2, n, kernel_size=2, stride=2)\n",
        "        self.conv6 = nn.Sequential(\n",
        "            nn.Conv2d(n * 2, n, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(n),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(n, n, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(n),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        n //= 2  # 16\n",
        "        self.upconv2 = nn.ConvTranspose2d(n * 2, n, kernel_size=2, stride=2)\n",
        "        self.conv7 = nn.Sequential(\n",
        "            nn.Conv2d(n * 2, n, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(n),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(n, n, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(n),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.final_conv = nn.Conv2d(n, output_channel, kernel_size=1)\n",
        "\n",
        "        ### End of your code ###\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Use the convolutional operators defined above to build the U-net\n",
        "        # The encoder part is already done for you.\n",
        "        # You need to complete the decoder part.\n",
        "        # Encoder\n",
        "        x = self.conv1(x)\n",
        "        conv1_skip = x\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        conv2_skip = x\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        conv3_skip = x\n",
        "\n",
        "        x = self.conv4(x)\n",
        "\n",
        "        # Decoder\n",
        "        ### Insert your code ###\n",
        "        x = self.upconv4(x)\n",
        "        x = torch.cat([x, conv3_skip], dim=1)\n",
        "        x = self.conv5(x)\n",
        "\n",
        "        x = self.upconv3(x)\n",
        "        x = torch.cat([x, conv2_skip], dim=1)\n",
        "        x = self.conv6(x)\n",
        "\n",
        "        x = self.upconv2(x)\n",
        "        x = torch.cat([x, conv1_skip], dim=1)\n",
        "        x = self.conv7(x)\n",
        "\n",
        "        x = self.final_conv(x)\n",
        "        ### End of your code ###\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcNWZS08d47P"
      },
      "source": [
        "## Q4. Train the segmentation model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xaGGkKQndIaR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "CUDA available: True\n",
            "GPU: NVIDIA GeForce RTX 3060\n",
            "Iteration 1, Loss: 1.1862388849258423\n",
            "Iteration 2, Loss: 1.1342580318450928\n",
            "Iteration 3, Loss: 1.103920817375183\n",
            "Iteration 4, Loss: 1.089455008506775\n",
            "Iteration 5, Loss: 1.0679941177368164\n",
            "Iteration 6, Loss: 1.0447206497192383\n",
            "Iteration 7, Loss: 1.026786208152771\n",
            "Iteration 8, Loss: 1.0029100179672241\n",
            "Iteration 9, Loss: 0.9969102144241333\n",
            "Iteration 10, Loss: 0.9821696877479553\n",
            "Iteration 11, Loss: 0.9659085869789124\n",
            "Iteration 12, Loss: 0.9534114599227905\n",
            "Iteration 13, Loss: 0.9483023285865784\n",
            "Iteration 14, Loss: 0.9241408705711365\n",
            "Iteration 15, Loss: 0.9155722260475159\n",
            "Iteration 16, Loss: 0.912213146686554\n",
            "Iteration 17, Loss: 0.8919978737831116\n",
            "Iteration 18, Loss: 0.8812609314918518\n",
            "Iteration 19, Loss: 0.8667731285095215\n",
            "Iteration 20, Loss: 0.8666262626647949\n",
            "Iteration 21, Loss: 0.8493056893348694\n",
            "Iteration 22, Loss: 0.842046320438385\n",
            "Iteration 23, Loss: 0.8269772529602051\n",
            "Iteration 24, Loss: 0.8241056799888611\n",
            "Iteration 25, Loss: 0.8118734359741211\n",
            "Iteration 26, Loss: 0.7913380265235901\n",
            "Iteration 27, Loss: 0.791562557220459\n",
            "Iteration 28, Loss: 0.7770792841911316\n",
            "Iteration 29, Loss: 0.770963191986084\n",
            "Iteration 30, Loss: 0.7558601498603821\n",
            "Iteration 31, Loss: 0.7415261268615723\n",
            "Iteration 32, Loss: 0.7415607571601868\n",
            "Iteration 33, Loss: 0.7271167039871216\n",
            "Iteration 34, Loss: 0.7092379927635193\n",
            "Iteration 35, Loss: 0.7125838398933411\n",
            "Iteration 36, Loss: 0.6927027106285095\n",
            "Iteration 37, Loss: 0.6859261989593506\n",
            "Iteration 38, Loss: 0.6787290573120117\n",
            "Iteration 39, Loss: 0.6677591800689697\n",
            "Iteration 40, Loss: 0.6642352342605591\n",
            "Iteration 41, Loss: 0.6490482687950134\n",
            "Iteration 42, Loss: 0.6368042826652527\n",
            "Iteration 43, Loss: 0.630522608757019\n",
            "Iteration 44, Loss: 0.6282880902290344\n",
            "Iteration 45, Loss: 0.6156424880027771\n",
            "Iteration 46, Loss: 0.6117119789123535\n",
            "Iteration 47, Loss: 0.5951321125030518\n",
            "Iteration 48, Loss: 0.5858280658721924\n",
            "Iteration 49, Loss: 0.5817088484764099\n",
            "Iteration 50, Loss: 0.5828755497932434\n",
            "Iteration 51, Loss: 0.5702263116836548\n",
            "Iteration 52, Loss: 0.5703040361404419\n",
            "Iteration 53, Loss: 0.5423349738121033\n",
            "Iteration 54, Loss: 0.5432164072990417\n",
            "Iteration 55, Loss: 0.5507365465164185\n",
            "Iteration 56, Loss: 0.5369706153869629\n",
            "Iteration 57, Loss: 0.5290282368659973\n",
            "Iteration 58, Loss: 0.517404317855835\n",
            "Iteration 59, Loss: 0.5022653341293335\n",
            "Iteration 60, Loss: 0.5023024082183838\n",
            "Iteration 61, Loss: 0.48885786533355713\n",
            "Iteration 62, Loss: 0.4784415066242218\n",
            "Iteration 63, Loss: 0.47447121143341064\n",
            "Iteration 64, Loss: 0.46024495363235474\n",
            "Iteration 65, Loss: 0.47407039999961853\n",
            "Iteration 66, Loss: 0.4688934087753296\n",
            "Iteration 67, Loss: 0.4553525149822235\n",
            "Iteration 68, Loss: 0.4631226360797882\n",
            "Iteration 69, Loss: 0.4460104703903198\n",
            "Iteration 70, Loss: 0.4205833375453949\n",
            "Iteration 71, Loss: 0.4275209307670593\n",
            "Iteration 72, Loss: 0.4203428030014038\n",
            "Iteration 73, Loss: 0.41493991017341614\n",
            "Iteration 74, Loss: 0.4018918573856354\n",
            "Iteration 75, Loss: 0.44136008620262146\n",
            "Iteration 76, Loss: 0.4147651195526123\n",
            "Iteration 77, Loss: 0.4042789041996002\n",
            "Iteration 78, Loss: 0.39075639843940735\n",
            "Iteration 79, Loss: 0.38769209384918213\n",
            "Iteration 80, Loss: 0.3933104872703552\n",
            "Iteration 81, Loss: 0.3681296706199646\n",
            "Iteration 82, Loss: 0.38053059577941895\n",
            "Iteration 83, Loss: 0.3588621914386749\n",
            "Iteration 84, Loss: 0.36785921454429626\n",
            "Iteration 85, Loss: 0.3623378872871399\n",
            "Iteration 86, Loss: 0.35400423407554626\n",
            "Iteration 87, Loss: 0.3464656472206116\n",
            "Iteration 88, Loss: 0.3598089814186096\n",
            "Iteration 89, Loss: 0.3492920696735382\n",
            "Iteration 90, Loss: 0.35445231199264526\n",
            "Iteration 91, Loss: 0.3477775752544403\n",
            "Iteration 92, Loss: 0.3186017870903015\n",
            "Iteration 93, Loss: 0.3257141411304474\n",
            "Iteration 94, Loss: 0.32739633321762085\n",
            "Iteration 95, Loss: 0.3293107748031616\n",
            "Iteration 96, Loss: 0.32047954201698303\n",
            "Iteration 97, Loss: 0.2975996434688568\n",
            "Iteration 98, Loss: 0.3184282183647156\n",
            "Iteration 99, Loss: 0.30199816823005676\n",
            "Iteration 100, Loss: 0.3125474750995636\n",
            "Iteration 101, Loss: 0.3129097819328308\n",
            "Iteration 102, Loss: 0.3046141564846039\n",
            "Iteration 103, Loss: 0.2780645489692688\n",
            "Iteration 104, Loss: 0.2818877398967743\n",
            "Iteration 105, Loss: 0.29305580258369446\n",
            "Iteration 106, Loss: 0.2704162299633026\n",
            "Iteration 107, Loss: 0.2701582610607147\n",
            "Iteration 108, Loss: 0.28589415550231934\n",
            "Iteration 109, Loss: 0.2742559611797333\n",
            "Iteration 110, Loss: 0.2809240221977234\n",
            "Iteration 111, Loss: 0.27818524837493896\n",
            "Iteration 112, Loss: 0.2928934395313263\n",
            "Iteration 113, Loss: 0.262267529964447\n",
            "Iteration 114, Loss: 0.2497727870941162\n",
            "Iteration 115, Loss: 0.269106388092041\n",
            "Iteration 116, Loss: 0.25500860810279846\n",
            "Iteration 117, Loss: 0.2464456707239151\n",
            "Iteration 118, Loss: 0.24732404947280884\n",
            "Iteration 119, Loss: 0.24086129665374756\n",
            "Iteration 120, Loss: 0.254019558429718\n",
            "Iteration 121, Loss: 0.23437874019145966\n",
            "Iteration 122, Loss: 0.25346675515174866\n",
            "Iteration 123, Loss: 0.234797865152359\n",
            "Iteration 124, Loss: 0.23080657422542572\n",
            "Iteration 125, Loss: 0.22249887883663177\n",
            "Iteration 126, Loss: 0.25866377353668213\n",
            "Iteration 127, Loss: 0.22901616990566254\n",
            "Iteration 128, Loss: 0.2327217310667038\n",
            "Iteration 129, Loss: 0.23291954398155212\n",
            "Iteration 130, Loss: 0.21463125944137573\n",
            "Iteration 131, Loss: 0.22301560640335083\n",
            "Iteration 132, Loss: 0.1976127177476883\n",
            "Iteration 133, Loss: 0.20651599764823914\n",
            "Iteration 134, Loss: 0.22551436722278595\n",
            "Iteration 135, Loss: 0.21533724665641785\n",
            "Iteration 136, Loss: 0.1835823804140091\n",
            "Iteration 137, Loss: 0.23087288439273834\n",
            "Iteration 138, Loss: 0.21669359505176544\n",
            "Iteration 139, Loss: 0.1934439241886139\n",
            "Iteration 140, Loss: 0.21702785789966583\n",
            "Iteration 141, Loss: 0.18915310502052307\n",
            "Iteration 142, Loss: 0.21058683097362518\n",
            "Iteration 143, Loss: 0.19770479202270508\n",
            "Iteration 144, Loss: 0.21157968044281006\n",
            "Iteration 145, Loss: 0.18615807592868805\n",
            "Iteration 146, Loss: 0.19185848534107208\n",
            "Iteration 147, Loss: 0.19484947621822357\n",
            "Iteration 148, Loss: 0.19675949215888977\n",
            "Iteration 149, Loss: 0.18616841733455658\n",
            "Iteration 150, Loss: 0.1998715102672577\n",
            "Iteration 151, Loss: 0.1769842803478241\n",
            "Iteration 152, Loss: 0.1910725086927414\n",
            "Iteration 153, Loss: 0.18078795075416565\n",
            "Iteration 154, Loss: 0.19435817003250122\n",
            "Iteration 155, Loss: 0.18255241215229034\n",
            "Iteration 156, Loss: 0.17785796523094177\n",
            "Iteration 157, Loss: 0.16809776425361633\n",
            "Iteration 158, Loss: 0.15354371070861816\n",
            "Iteration 159, Loss: 0.1823168396949768\n",
            "Iteration 160, Loss: 0.14691102504730225\n",
            "Iteration 161, Loss: 0.1776849329471588\n",
            "Iteration 162, Loss: 0.18294119834899902\n",
            "Iteration 163, Loss: 0.16603268682956696\n",
            "Iteration 164, Loss: 0.21107766032218933\n",
            "Iteration 165, Loss: 0.15693669021129608\n",
            "Iteration 166, Loss: 0.16212819516658783\n",
            "Iteration 167, Loss: 0.17094482481479645\n",
            "Iteration 168, Loss: 0.16103872656822205\n",
            "Iteration 169, Loss: 0.17923974990844727\n",
            "Iteration 170, Loss: 0.17657440900802612\n",
            "Iteration 171, Loss: 0.17544923722743988\n",
            "Iteration 172, Loss: 0.15594065189361572\n",
            "Iteration 173, Loss: 0.16576126217842102\n",
            "Iteration 174, Loss: 0.1424962431192398\n",
            "Iteration 175, Loss: 0.14793454110622406\n",
            "Iteration 176, Loss: 0.1568443328142166\n",
            "Iteration 177, Loss: 0.16263365745544434\n",
            "Iteration 178, Loss: 0.1569783240556717\n",
            "Iteration 179, Loss: 0.16106419265270233\n",
            "Iteration 180, Loss: 0.15916568040847778\n",
            "Iteration 181, Loss: 0.14858859777450562\n",
            "Iteration 182, Loss: 0.13332368433475494\n",
            "Iteration 183, Loss: 0.13578028976917267\n",
            "Iteration 184, Loss: 0.14740896224975586\n",
            "Iteration 185, Loss: 0.1276032030582428\n",
            "Iteration 186, Loss: 0.14749257266521454\n",
            "Iteration 187, Loss: 0.15629775822162628\n",
            "Iteration 188, Loss: 0.15289729833602905\n",
            "Iteration 189, Loss: 0.13360407948493958\n",
            "Iteration 190, Loss: 0.1459580808877945\n",
            "Iteration 191, Loss: 0.17060567438602448\n",
            "Iteration 192, Loss: 0.14866529405117035\n",
            "Iteration 193, Loss: 0.1395287960767746\n",
            "Iteration 194, Loss: 0.14749647676944733\n",
            "Iteration 195, Loss: 0.15335693955421448\n",
            "Iteration 196, Loss: 0.1330510377883911\n",
            "Iteration 197, Loss: 0.11860194057226181\n",
            "Iteration 198, Loss: 0.12574492394924164\n",
            "Iteration 199, Loss: 0.145817369222641\n",
            "Iteration 200, Loss: 0.14098520576953888\n",
            "Iteration 201, Loss: 0.14281409978866577\n",
            "Iteration 202, Loss: 0.1639992743730545\n",
            "Iteration 203, Loss: 0.1259371042251587\n",
            "Iteration 204, Loss: 0.12011086940765381\n",
            "Iteration 205, Loss: 0.13417531549930573\n",
            "Iteration 206, Loss: 0.15326599776744843\n",
            "Iteration 207, Loss: 0.13687025010585785\n",
            "Iteration 208, Loss: 0.11450302600860596\n",
            "Iteration 209, Loss: 0.11555368453264236\n",
            "Iteration 210, Loss: 0.11883736401796341\n",
            "Iteration 211, Loss: 0.12376994639635086\n",
            "Iteration 212, Loss: 0.12994146347045898\n",
            "Iteration 213, Loss: 0.1288413256406784\n",
            "Iteration 214, Loss: 0.1288181096315384\n",
            "Iteration 215, Loss: 0.13384874165058136\n",
            "Iteration 216, Loss: 0.1143622100353241\n",
            "Iteration 217, Loss: 0.13413329422473907\n",
            "Iteration 218, Loss: 0.11715663224458694\n",
            "Iteration 219, Loss: 0.1438819020986557\n",
            "Iteration 220, Loss: 0.1368159055709839\n",
            "Iteration 221, Loss: 0.11204952001571655\n",
            "Iteration 222, Loss: 0.12556074559688568\n",
            "Iteration 223, Loss: 0.1371762454509735\n",
            "Iteration 224, Loss: 0.15177315473556519\n",
            "Iteration 225, Loss: 0.11438218504190445\n",
            "Iteration 226, Loss: 0.12079719454050064\n",
            "Iteration 227, Loss: 0.11121206730604172\n",
            "Iteration 228, Loss: 0.12754467129707336\n",
            "Iteration 229, Loss: 0.11973533034324646\n",
            "Iteration 230, Loss: 0.12286374717950821\n",
            "Iteration 231, Loss: 0.1352907419204712\n",
            "Iteration 232, Loss: 0.10284996777772903\n",
            "Iteration 233, Loss: 0.11422538757324219\n",
            "Iteration 234, Loss: 0.11316999047994614\n",
            "Iteration 235, Loss: 0.10981112718582153\n",
            "Iteration 236, Loss: 0.11127221584320068\n",
            "Iteration 237, Loss: 0.1282801777124405\n",
            "Iteration 238, Loss: 0.10294387489557266\n",
            "Iteration 239, Loss: 0.11531675606966019\n",
            "Iteration 240, Loss: 0.11421240121126175\n",
            "Iteration 241, Loss: 0.11089102178812027\n",
            "Iteration 242, Loss: 0.09782683849334717\n",
            "Iteration 243, Loss: 0.0966670885682106\n",
            "Iteration 244, Loss: 0.12495632469654083\n",
            "Iteration 245, Loss: 0.10743863135576248\n",
            "Iteration 246, Loss: 0.11405862867832184\n",
            "Iteration 247, Loss: 0.08732157200574875\n",
            "Iteration 248, Loss: 0.12262792885303497\n",
            "Iteration 249, Loss: 0.09768085926771164\n",
            "Iteration 250, Loss: 0.09670194238424301\n",
            "Iteration 251, Loss: 0.09215760976076126\n",
            "Iteration 252, Loss: 0.09281474351882935\n",
            "Iteration 253, Loss: 0.1127413958311081\n",
            "Iteration 254, Loss: 0.11625142395496368\n",
            "Iteration 255, Loss: 0.10619761049747467\n",
            "Iteration 256, Loss: 0.11611167341470718\n",
            "Iteration 257, Loss: 0.11250140517950058\n",
            "Iteration 258, Loss: 0.0985347330570221\n",
            "Iteration 259, Loss: 0.11145251989364624\n",
            "Iteration 260, Loss: 0.08642612397670746\n",
            "Iteration 261, Loss: 0.089866504073143\n",
            "Iteration 262, Loss: 0.13635675609111786\n",
            "Iteration 263, Loss: 0.09098204970359802\n",
            "Iteration 264, Loss: 0.10522209852933884\n",
            "Iteration 265, Loss: 0.10274216532707214\n",
            "Iteration 266, Loss: 0.09156633168458939\n",
            "Iteration 267, Loss: 0.10664048045873642\n",
            "Iteration 268, Loss: 0.09177608042955399\n",
            "Iteration 269, Loss: 0.13219870626926422\n",
            "Iteration 270, Loss: 0.08670886605978012\n",
            "Iteration 271, Loss: 0.12151655554771423\n",
            "Iteration 272, Loss: 0.09049999713897705\n",
            "Iteration 273, Loss: 0.10837197303771973\n",
            "Iteration 274, Loss: 0.0859837755560875\n",
            "Iteration 275, Loss: 0.09265486150979996\n",
            "Iteration 276, Loss: 0.09232338517904282\n",
            "Iteration 277, Loss: 0.10297746956348419\n",
            "Iteration 278, Loss: 0.09781837463378906\n",
            "Iteration 279, Loss: 0.13903817534446716\n",
            "Iteration 280, Loss: 0.0915311649441719\n",
            "Iteration 281, Loss: 0.09298061579465866\n",
            "Iteration 282, Loss: 0.08952022343873978\n",
            "Iteration 283, Loss: 0.1043795496225357\n",
            "Iteration 284, Loss: 0.09092268347740173\n",
            "Iteration 285, Loss: 0.09889375418424606\n",
            "Iteration 286, Loss: 0.10011335462331772\n",
            "Iteration 287, Loss: 0.0996842086315155\n",
            "Iteration 288, Loss: 0.08387971669435501\n",
            "Iteration 289, Loss: 0.08581335097551346\n",
            "Iteration 290, Loss: 0.08442608267068863\n",
            "Iteration 291, Loss: 0.09802258759737015\n",
            "Iteration 292, Loss: 0.07974536716938019\n",
            "Iteration 293, Loss: 0.08454640954732895\n",
            "Iteration 294, Loss: 0.10574695467948914\n",
            "Iteration 295, Loss: 0.07233171910047531\n",
            "Iteration 296, Loss: 0.09292048960924149\n",
            "Iteration 297, Loss: 0.09005695581436157\n",
            "Iteration 298, Loss: 0.10046122223138809\n",
            "Iteration 299, Loss: 0.06122135743498802\n",
            "Iteration 300, Loss: 0.07976259291172028\n",
            "Iteration 301, Loss: 0.10696552693843842\n",
            "Iteration 302, Loss: 0.10020322352647781\n",
            "Iteration 303, Loss: 0.08167637139558792\n",
            "Iteration 304, Loss: 0.09068992733955383\n",
            "Iteration 305, Loss: 0.0816744938492775\n",
            "Iteration 306, Loss: 0.08403896540403366\n",
            "Iteration 307, Loss: 0.08699031174182892\n",
            "Iteration 308, Loss: 0.0994017943739891\n",
            "Iteration 309, Loss: 0.09030832350254059\n",
            "Iteration 310, Loss: 0.08306626975536346\n",
            "Iteration 311, Loss: 0.10115845501422882\n",
            "Iteration 312, Loss: 0.08766963332891464\n",
            "Iteration 313, Loss: 0.06084408238530159\n",
            "Iteration 314, Loss: 0.07223139703273773\n",
            "Iteration 315, Loss: 0.09517878293991089\n",
            "Iteration 316, Loss: 0.12212826311588287\n",
            "Iteration 317, Loss: 0.09703434258699417\n",
            "Iteration 318, Loss: 0.0812874287366867\n",
            "Iteration 319, Loss: 0.07938150316476822\n",
            "Iteration 320, Loss: 0.06910112500190735\n",
            "Iteration 321, Loss: 0.12185811996459961\n",
            "Iteration 322, Loss: 0.08535243570804596\n",
            "Iteration 323, Loss: 0.07516114413738251\n",
            "Iteration 324, Loss: 0.07489961385726929\n",
            "Iteration 325, Loss: 0.06981198489665985\n",
            "Iteration 326, Loss: 0.08929888904094696\n",
            "Iteration 327, Loss: 0.07866169512271881\n",
            "Iteration 328, Loss: 0.09934154897928238\n",
            "Iteration 329, Loss: 0.07684722542762756\n",
            "Iteration 330, Loss: 0.09927569329738617\n",
            "Iteration 331, Loss: 0.07319165021181107\n",
            "Iteration 332, Loss: 0.07647814601659775\n",
            "Iteration 333, Loss: 0.08636190742254257\n",
            "Iteration 334, Loss: 0.09200365841388702\n",
            "Iteration 335, Loss: 0.07946239411830902\n",
            "Iteration 336, Loss: 0.11254477500915527\n",
            "Iteration 337, Loss: 0.07271091639995575\n",
            "Iteration 338, Loss: 0.08771005272865295\n",
            "Iteration 339, Loss: 0.08303346484899521\n",
            "Iteration 340, Loss: 0.07599832117557526\n",
            "Iteration 341, Loss: 0.07798551768064499\n",
            "Iteration 342, Loss: 0.07910121232271194\n",
            "Iteration 343, Loss: 0.07067056745290756\n",
            "Iteration 344, Loss: 0.06449640542268753\n",
            "Iteration 345, Loss: 0.08024762570858002\n",
            "Iteration 346, Loss: 0.08582116663455963\n",
            "Iteration 347, Loss: 0.07176902890205383\n",
            "Iteration 348, Loss: 0.07249105721712112\n",
            "Iteration 349, Loss: 0.06826350092887878\n",
            "Iteration 350, Loss: 0.07502289116382599\n",
            "Iteration 351, Loss: 0.09028550237417221\n",
            "Iteration 352, Loss: 0.06307351589202881\n",
            "Iteration 353, Loss: 0.0822761058807373\n",
            "Iteration 354, Loss: 0.08170817792415619\n",
            "Iteration 355, Loss: 0.10004455596208572\n",
            "Iteration 356, Loss: 0.07365722954273224\n",
            "Iteration 357, Loss: 0.08118148893117905\n",
            "Iteration 358, Loss: 0.07081738859415054\n",
            "Iteration 359, Loss: 0.07933341711759567\n",
            "Iteration 360, Loss: 0.06474188715219498\n",
            "Iteration 361, Loss: 0.0718153566122055\n",
            "Iteration 362, Loss: 0.09544243663549423\n",
            "Iteration 363, Loss: 0.07217617332935333\n",
            "Iteration 364, Loss: 0.08640838414430618\n",
            "Iteration 365, Loss: 0.08418650925159454\n",
            "Iteration 366, Loss: 0.06911073625087738\n",
            "Iteration 367, Loss: 0.09785214811563492\n",
            "Iteration 368, Loss: 0.0686749815940857\n",
            "Iteration 369, Loss: 0.07369820773601532\n",
            "Iteration 370, Loss: 0.07381714135408401\n",
            "Iteration 371, Loss: 0.06467264890670776\n",
            "Iteration 372, Loss: 0.07700982689857483\n",
            "Iteration 373, Loss: 0.06882213801145554\n",
            "Iteration 374, Loss: 0.07640361040830612\n",
            "Iteration 375, Loss: 0.07061698287725449\n",
            "Iteration 376, Loss: 0.06666409969329834\n",
            "Iteration 377, Loss: 0.06741367280483246\n",
            "Iteration 378, Loss: 0.06904800236225128\n",
            "Iteration 379, Loss: 0.07945422828197479\n",
            "Iteration 380, Loss: 0.0703849047422409\n",
            "Iteration 381, Loss: 0.060519445687532425\n",
            "Iteration 382, Loss: 0.08454176783561707\n",
            "Iteration 383, Loss: 0.0691215917468071\n",
            "Iteration 384, Loss: 0.07424405962228775\n",
            "Iteration 385, Loss: 0.08202435076236725\n",
            "Iteration 386, Loss: 0.06992243230342865\n",
            "Iteration 387, Loss: 0.07427240163087845\n",
            "Iteration 388, Loss: 0.06423375755548477\n",
            "Iteration 389, Loss: 0.06353349983692169\n",
            "Iteration 390, Loss: 0.07155517488718033\n",
            "Iteration 391, Loss: 0.07086894661188126\n",
            "Iteration 392, Loss: 0.08026783168315887\n",
            "Iteration 393, Loss: 0.07355230301618576\n",
            "Iteration 394, Loss: 0.07330694049596786\n",
            "Iteration 395, Loss: 0.05831153318285942\n",
            "Iteration 396, Loss: 0.08302117139101028\n",
            "Iteration 397, Loss: 0.0853177085518837\n",
            "Iteration 398, Loss: 0.08958710730075836\n",
            "Iteration 399, Loss: 0.074489064514637\n",
            "Iteration 400, Loss: 0.06777666509151459\n",
            "Iteration 401, Loss: 0.05197443813085556\n",
            "Iteration 402, Loss: 0.07596791535615921\n",
            "Iteration 403, Loss: 0.06743553280830383\n",
            "Iteration 404, Loss: 0.038450971245765686\n",
            "Iteration 405, Loss: 0.07597087323665619\n",
            "Iteration 406, Loss: 0.07253250479698181\n",
            "Iteration 407, Loss: 0.08033157885074615\n",
            "Iteration 408, Loss: 0.06870172917842865\n",
            "Iteration 409, Loss: 0.06942557543516159\n",
            "Iteration 410, Loss: 0.08111970126628876\n",
            "Iteration 411, Loss: 0.08148572593927383\n",
            "Iteration 412, Loss: 0.06403995305299759\n",
            "Iteration 413, Loss: 0.07534261047840118\n",
            "Iteration 414, Loss: 0.08057930320501328\n",
            "Iteration 415, Loss: 0.07735603302717209\n",
            "Iteration 416, Loss: 0.07173246890306473\n",
            "Iteration 417, Loss: 0.09031713753938675\n",
            "Iteration 418, Loss: 0.04840695858001709\n",
            "Iteration 419, Loss: 0.053410399705171585\n",
            "Iteration 420, Loss: 0.07475632429122925\n",
            "Iteration 421, Loss: 0.07302692532539368\n",
            "Iteration 422, Loss: 0.06167786940932274\n",
            "Iteration 423, Loss: 0.06325525045394897\n",
            "Iteration 424, Loss: 0.08055470138788223\n",
            "Iteration 425, Loss: 0.06167333945631981\n",
            "Iteration 426, Loss: 0.07679086178541183\n",
            "Iteration 427, Loss: 0.052688051015138626\n",
            "Iteration 428, Loss: 0.07645459473133087\n",
            "Iteration 429, Loss: 0.058130521327257156\n",
            "Iteration 430, Loss: 0.09926212579011917\n",
            "Iteration 431, Loss: 0.05489368736743927\n",
            "Iteration 432, Loss: 0.06436782330274582\n",
            "Iteration 433, Loss: 0.06534530222415924\n",
            "Iteration 434, Loss: 0.0702870562672615\n",
            "Iteration 435, Loss: 0.07051445543766022\n",
            "Iteration 436, Loss: 0.08042384684085846\n",
            "Iteration 437, Loss: 0.06358373910188675\n",
            "Iteration 438, Loss: 0.06380591541528702\n",
            "Iteration 439, Loss: 0.075131855905056\n",
            "Iteration 440, Loss: 0.047106124460697174\n",
            "Iteration 441, Loss: 0.0659157782793045\n",
            "Iteration 442, Loss: 0.07513293623924255\n",
            "Iteration 443, Loss: 0.04964010789990425\n",
            "Iteration 444, Loss: 0.08238986879587173\n",
            "Iteration 445, Loss: 0.06164270639419556\n",
            "Iteration 446, Loss: 0.05459386855363846\n",
            "Iteration 447, Loss: 0.08464039862155914\n",
            "Iteration 448, Loss: 0.060667734593153\n",
            "Iteration 449, Loss: 0.06494160741567612\n",
            "Iteration 450, Loss: 0.08419866114854813\n",
            "Iteration 451, Loss: 0.05900425463914871\n",
            "Iteration 452, Loss: 0.05768859386444092\n",
            "Iteration 453, Loss: 0.051389358937740326\n",
            "Iteration 454, Loss: 0.0673665702342987\n",
            "Iteration 455, Loss: 0.06745359301567078\n",
            "Iteration 456, Loss: 0.05524522811174393\n",
            "Iteration 457, Loss: 0.05792808532714844\n",
            "Iteration 458, Loss: 0.0826951190829277\n",
            "Iteration 459, Loss: 0.06007245555520058\n",
            "Iteration 460, Loss: 0.04446347430348396\n",
            "Iteration 461, Loss: 0.04383886978030205\n",
            "Iteration 462, Loss: 0.046959225088357925\n",
            "Iteration 463, Loss: 0.059042785316705704\n",
            "Iteration 464, Loss: 0.07116906344890594\n",
            "Iteration 465, Loss: 0.052838146686553955\n",
            "Iteration 466, Loss: 0.05509808287024498\n",
            "Iteration 467, Loss: 0.07002057135105133\n",
            "Iteration 468, Loss: 0.0784500315785408\n",
            "Iteration 469, Loss: 0.05085776373744011\n",
            "Iteration 470, Loss: 0.07065234333276749\n",
            "Iteration 471, Loss: 0.0607067346572876\n",
            "Iteration 472, Loss: 0.08266335725784302\n",
            "Iteration 473, Loss: 0.06493986397981644\n",
            "Iteration 474, Loss: 0.10155216604471207\n",
            "Iteration 475, Loss: 0.05281279981136322\n",
            "Iteration 476, Loss: 0.06804995983839035\n",
            "Iteration 477, Loss: 0.07665844261646271\n",
            "Iteration 478, Loss: 0.05360783636569977\n",
            "Iteration 479, Loss: 0.04630986228585243\n",
            "Iteration 480, Loss: 0.0709768608212471\n",
            "Iteration 481, Loss: 0.05290335416793823\n",
            "Iteration 482, Loss: 0.045162394642829895\n",
            "Iteration 483, Loss: 0.053687285631895065\n",
            "Iteration 484, Loss: 0.053856223821640015\n",
            "Iteration 485, Loss: 0.06783178448677063\n",
            "Iteration 486, Loss: 0.08145225793123245\n",
            "Iteration 487, Loss: 0.06395015865564346\n",
            "Iteration 488, Loss: 0.05028270184993744\n",
            "Iteration 489, Loss: 0.05719025805592537\n",
            "Iteration 490, Loss: 0.07427425682544708\n",
            "Iteration 491, Loss: 0.08098270744085312\n",
            "Iteration 492, Loss: 0.07582057267427444\n",
            "Iteration 493, Loss: 0.060004521161317825\n",
            "Iteration 494, Loss: 0.048733312636613846\n",
            "Iteration 495, Loss: 0.07158499956130981\n",
            "Iteration 496, Loss: 0.055093247443437576\n",
            "Iteration 497, Loss: 0.05993809178471565\n",
            "Iteration 498, Loss: 0.062007836997509\n",
            "Iteration 499, Loss: 0.09316232800483704\n",
            "Iteration 500, Loss: 0.057206086814403534\n",
            "Iteration 501, Loss: 0.07154244184494019\n",
            "Iteration 502, Loss: 0.06440054625272751\n",
            "Iteration 503, Loss: 0.04948689788579941\n",
            "Iteration 504, Loss: 0.058413222432136536\n",
            "Iteration 505, Loss: 0.0438837856054306\n",
            "Iteration 506, Loss: 0.06561464816331863\n",
            "Iteration 507, Loss: 0.0811760425567627\n",
            "Iteration 508, Loss: 0.05634290724992752\n",
            "Iteration 509, Loss: 0.06601210683584213\n",
            "Iteration 510, Loss: 0.05478173866868019\n",
            "Iteration 511, Loss: 0.07283223420381546\n",
            "Iteration 512, Loss: 0.07234731316566467\n",
            "Iteration 513, Loss: 0.05776568129658699\n",
            "Iteration 514, Loss: 0.08063158392906189\n",
            "Iteration 515, Loss: 0.04898766055703163\n",
            "Iteration 516, Loss: 0.05083153024315834\n",
            "Iteration 517, Loss: 0.0711296796798706\n",
            "Iteration 518, Loss: 0.05023524537682533\n",
            "Iteration 519, Loss: 0.05759647116065025\n",
            "Iteration 520, Loss: 0.03857047110795975\n",
            "Iteration 521, Loss: 0.07636136561632156\n",
            "Iteration 522, Loss: 0.06519854068756104\n",
            "Iteration 523, Loss: 0.06360016763210297\n",
            "Iteration 524, Loss: 0.05006999894976616\n",
            "Iteration 525, Loss: 0.06049655005335808\n",
            "Iteration 526, Loss: 0.04255737364292145\n",
            "Iteration 527, Loss: 0.059687063097953796\n",
            "Iteration 528, Loss: 0.04920274019241333\n",
            "Iteration 529, Loss: 0.05018402636051178\n",
            "Iteration 530, Loss: 0.05273033306002617\n",
            "Iteration 531, Loss: 0.06986492872238159\n",
            "Iteration 532, Loss: 0.06606896966695786\n",
            "Iteration 533, Loss: 0.060288939625024796\n",
            "Iteration 534, Loss: 0.05887948349118233\n",
            "Iteration 535, Loss: 0.05916678160429001\n",
            "Iteration 536, Loss: 0.060490094125270844\n",
            "Iteration 537, Loss: 0.07878615707159042\n",
            "Iteration 538, Loss: 0.06601496785879135\n",
            "Iteration 539, Loss: 0.06770750135183334\n",
            "Iteration 540, Loss: 0.05913986265659332\n",
            "Iteration 541, Loss: 0.0420171357691288\n",
            "Iteration 542, Loss: 0.06448173522949219\n",
            "Iteration 543, Loss: 0.08147194981575012\n",
            "Iteration 544, Loss: 0.052931517362594604\n",
            "Iteration 545, Loss: 0.04828508943319321\n",
            "Iteration 546, Loss: 0.10121744871139526\n",
            "Iteration 547, Loss: 0.08364968001842499\n",
            "Iteration 548, Loss: 0.051207296550273895\n",
            "Iteration 549, Loss: 0.07444748282432556\n",
            "Iteration 550, Loss: 0.06096476688981056\n",
            "Iteration 551, Loss: 0.06888802349567413\n",
            "Iteration 552, Loss: 0.06688284873962402\n",
            "Iteration 553, Loss: 0.05255560949444771\n",
            "Iteration 554, Loss: 0.05671803280711174\n",
            "Iteration 555, Loss: 0.06407792121171951\n",
            "Iteration 556, Loss: 0.05289525166153908\n",
            "Iteration 557, Loss: 0.06669475138187408\n",
            "Iteration 558, Loss: 0.04797716066241264\n",
            "Iteration 559, Loss: 0.05612434819340706\n",
            "Iteration 560, Loss: 0.05303672328591347\n",
            "Iteration 561, Loss: 0.06761965155601501\n",
            "Iteration 562, Loss: 0.056228507310152054\n",
            "Iteration 563, Loss: 0.05812640115618706\n",
            "Iteration 564, Loss: 0.05883413180708885\n",
            "Iteration 565, Loss: 0.05015731230378151\n",
            "Iteration 566, Loss: 0.055800288915634155\n",
            "Iteration 567, Loss: 0.06860952079296112\n",
            "Iteration 568, Loss: 0.06491225212812424\n",
            "Iteration 569, Loss: 0.05275779590010643\n",
            "Iteration 570, Loss: 0.06081590801477432\n",
            "Iteration 571, Loss: 0.06089174374938011\n",
            "Iteration 572, Loss: 0.059422750025987625\n",
            "Iteration 573, Loss: 0.04627743363380432\n",
            "Iteration 574, Loss: 0.048531800508499146\n",
            "Iteration 575, Loss: 0.05315143242478371\n",
            "Iteration 576, Loss: 0.061605535447597504\n",
            "Iteration 577, Loss: 0.05002335086464882\n",
            "Iteration 578, Loss: 0.04802389815449715\n",
            "Iteration 579, Loss: 0.05277381092309952\n",
            "Iteration 580, Loss: 0.06259530782699585\n",
            "Iteration 581, Loss: 0.05516406521201134\n",
            "Iteration 582, Loss: 0.08840620517730713\n",
            "Iteration 583, Loss: 0.059765443205833435\n",
            "Iteration 584, Loss: 0.0581241101026535\n",
            "Iteration 585, Loss: 0.05942091718316078\n",
            "Iteration 586, Loss: 0.05580060929059982\n",
            "Iteration 587, Loss: 0.07824825495481491\n",
            "Iteration 588, Loss: 0.06505731493234634\n",
            "Iteration 589, Loss: 0.058672986924648285\n",
            "Iteration 590, Loss: 0.07275529950857162\n",
            "Iteration 591, Loss: 0.05971364304423332\n",
            "Iteration 592, Loss: 0.06559282541275024\n",
            "Iteration 593, Loss: 0.03639337420463562\n",
            "Iteration 594, Loss: 0.05061372369527817\n",
            "Iteration 595, Loss: 0.039915237575769424\n",
            "Iteration 596, Loss: 0.05303360894322395\n",
            "Iteration 597, Loss: 0.06302350014448166\n",
            "Iteration 598, Loss: 0.056397560983896255\n",
            "Iteration 599, Loss: 0.055313222110271454\n",
            "Iteration 600, Loss: 0.04280005395412445\n",
            "Iteration 601, Loss: 0.06737371534109116\n",
            "Iteration 602, Loss: 0.053234633058309555\n",
            "Iteration 603, Loss: 0.05006466805934906\n",
            "Iteration 604, Loss: 0.05475788190960884\n",
            "Iteration 605, Loss: 0.04201396554708481\n",
            "Iteration 606, Loss: 0.04753447696566582\n",
            "Iteration 607, Loss: 0.06710852682590485\n",
            "Iteration 608, Loss: 0.052880771458148956\n",
            "Iteration 609, Loss: 0.05474437028169632\n",
            "Iteration 610, Loss: 0.05343896150588989\n",
            "Iteration 611, Loss: 0.03716655820608139\n",
            "Iteration 612, Loss: 0.06884124130010605\n",
            "Iteration 613, Loss: 0.08056202530860901\n",
            "Iteration 614, Loss: 0.05019986629486084\n",
            "Iteration 615, Loss: 0.05830672010779381\n",
            "Iteration 616, Loss: 0.060075610876083374\n",
            "Iteration 617, Loss: 0.055667728185653687\n",
            "Iteration 618, Loss: 0.05935656279325485\n",
            "Iteration 619, Loss: 0.053912922739982605\n",
            "Iteration 620, Loss: 0.06131044775247574\n",
            "Iteration 621, Loss: 0.05440733954310417\n",
            "Iteration 622, Loss: 0.04963914677500725\n",
            "Iteration 623, Loss: 0.08768705278635025\n",
            "Iteration 624, Loss: 0.06014224514365196\n",
            "Iteration 625, Loss: 0.06587186455726624\n",
            "Iteration 626, Loss: 0.04980950802564621\n",
            "Iteration 627, Loss: 0.045539189130067825\n",
            "Iteration 628, Loss: 0.027453286573290825\n",
            "Iteration 629, Loss: 0.061585500836372375\n",
            "Iteration 630, Loss: 0.0649539977312088\n",
            "Iteration 631, Loss: 0.06945100426673889\n",
            "Iteration 632, Loss: 0.059853121638298035\n",
            "Iteration 633, Loss: 0.06469856202602386\n",
            "Iteration 634, Loss: 0.04239390790462494\n",
            "Iteration 635, Loss: 0.0580853670835495\n",
            "Iteration 636, Loss: 0.04451720044016838\n",
            "Iteration 637, Loss: 0.04227741062641144\n",
            "Iteration 638, Loss: 0.04514597728848457\n",
            "Iteration 639, Loss: 0.039621975272893906\n",
            "Iteration 640, Loss: 0.04749301075935364\n",
            "Iteration 641, Loss: 0.07003592699766159\n",
            "Iteration 642, Loss: 0.04228200018405914\n",
            "Iteration 643, Loss: 0.06986115127801895\n",
            "Iteration 644, Loss: 0.04268460348248482\n",
            "Iteration 645, Loss: 0.05521048605442047\n",
            "Iteration 646, Loss: 0.07703720033168793\n",
            "Iteration 647, Loss: 0.05400598421692848\n",
            "Iteration 648, Loss: 0.055686645209789276\n",
            "Iteration 649, Loss: 0.04566248878836632\n",
            "Iteration 650, Loss: 0.04899873584508896\n",
            "Iteration 651, Loss: 0.05831976979970932\n",
            "Iteration 652, Loss: 0.0657835602760315\n",
            "Iteration 653, Loss: 0.05371773615479469\n",
            "Iteration 654, Loss: 0.041729070246219635\n",
            "Iteration 655, Loss: 0.06411522626876831\n",
            "Iteration 656, Loss: 0.03806049004197121\n",
            "Iteration 657, Loss: 0.054271552711725235\n",
            "Iteration 658, Loss: 0.05539384111762047\n",
            "Iteration 659, Loss: 0.050433918833732605\n",
            "Iteration 660, Loss: 0.051096003502607346\n",
            "Iteration 661, Loss: 0.04423372074961662\n",
            "Iteration 662, Loss: 0.05751306191086769\n",
            "Iteration 663, Loss: 0.07151971012353897\n",
            "Iteration 664, Loss: 0.04666228964924812\n",
            "Iteration 665, Loss: 0.04297206550836563\n",
            "Iteration 666, Loss: 0.06558861583471298\n",
            "Iteration 667, Loss: 0.05760446935892105\n",
            "Iteration 668, Loss: 0.05030319467186928\n",
            "Iteration 669, Loss: 0.044614337384700775\n",
            "Iteration 670, Loss: 0.04750297591090202\n",
            "Iteration 671, Loss: 0.04744694009423256\n",
            "Iteration 672, Loss: 0.05686229467391968\n",
            "Iteration 673, Loss: 0.05537550523877144\n",
            "Iteration 674, Loss: 0.02968299575150013\n",
            "Iteration 675, Loss: 0.04190748929977417\n",
            "Iteration 676, Loss: 0.059384219348430634\n",
            "Iteration 677, Loss: 0.06114578619599342\n",
            "Iteration 678, Loss: 0.053368572145700455\n",
            "Iteration 679, Loss: 0.03547533601522446\n",
            "Iteration 680, Loss: 0.04107341915369034\n",
            "Iteration 681, Loss: 0.0482039637863636\n",
            "Iteration 682, Loss: 0.04564645141363144\n",
            "Iteration 683, Loss: 0.05410705879330635\n",
            "Iteration 684, Loss: 0.055393073707818985\n",
            "Iteration 685, Loss: 0.07202441245317459\n",
            "Iteration 686, Loss: 0.061917081475257874\n",
            "Iteration 687, Loss: 0.0677548497915268\n",
            "Iteration 688, Loss: 0.05483530834317207\n",
            "Iteration 689, Loss: 0.05692474544048309\n",
            "Iteration 690, Loss: 0.05538645386695862\n",
            "Iteration 691, Loss: 0.05531875789165497\n",
            "Iteration 692, Loss: 0.04097370430827141\n",
            "Iteration 693, Loss: 0.054477956146001816\n",
            "Iteration 694, Loss: 0.036377888172864914\n",
            "Iteration 695, Loss: 0.07618612796068192\n",
            "Iteration 696, Loss: 0.047232434153556824\n",
            "Iteration 697, Loss: 0.04353250190615654\n",
            "Iteration 698, Loss: 0.048542771488428116\n",
            "Iteration 699, Loss: 0.05033119022846222\n",
            "Iteration 700, Loss: 0.04443177953362465\n",
            "Iteration 701, Loss: 0.04811810702085495\n",
            "Iteration 702, Loss: 0.05432199314236641\n",
            "Iteration 703, Loss: 0.03878173977136612\n",
            "Iteration 704, Loss: 0.07840082794427872\n",
            "Iteration 705, Loss: 0.05636722967028618\n",
            "Iteration 706, Loss: 0.06036560609936714\n",
            "Iteration 707, Loss: 0.07238669693470001\n",
            "Iteration 708, Loss: 0.058030594140291214\n",
            "Iteration 709, Loss: 0.046244312077760696\n",
            "Iteration 710, Loss: 0.055900074541568756\n",
            "Iteration 711, Loss: 0.06198330223560333\n",
            "Iteration 712, Loss: 0.06512681394815445\n",
            "Iteration 713, Loss: 0.05055968090891838\n",
            "Iteration 714, Loss: 0.06443452090024948\n",
            "Iteration 715, Loss: 0.04590851068496704\n",
            "Iteration 716, Loss: 0.07063539326190948\n",
            "Iteration 717, Loss: 0.044434767216444016\n",
            "Iteration 718, Loss: 0.03822953253984451\n",
            "Iteration 719, Loss: 0.05729037895798683\n",
            "Iteration 720, Loss: 0.06136675551533699\n",
            "Iteration 721, Loss: 0.040690671652555466\n",
            "Iteration 722, Loss: 0.054011307656764984\n",
            "Iteration 723, Loss: 0.054805196821689606\n",
            "Iteration 724, Loss: 0.04618631303310394\n",
            "Iteration 725, Loss: 0.06431524455547333\n",
            "Iteration 726, Loss: 0.05678274855017662\n",
            "Iteration 727, Loss: 0.04758787900209427\n",
            "Iteration 728, Loss: 0.043875474482774734\n",
            "Iteration 729, Loss: 0.056786131113767624\n",
            "Iteration 730, Loss: 0.08804906904697418\n",
            "Iteration 731, Loss: 0.06768873333930969\n",
            "Iteration 732, Loss: 0.06050081178545952\n",
            "Iteration 733, Loss: 0.049184806644916534\n",
            "Iteration 734, Loss: 0.044771403074264526\n",
            "Iteration 735, Loss: 0.040917884558439255\n",
            "Iteration 736, Loss: 0.037147823721170425\n",
            "Iteration 737, Loss: 0.0348912738263607\n",
            "Iteration 738, Loss: 0.03453192859888077\n",
            "Iteration 739, Loss: 0.05863300338387489\n",
            "Iteration 740, Loss: 0.04429326951503754\n",
            "Iteration 741, Loss: 0.03498173505067825\n",
            "Iteration 742, Loss: 0.038927529007196426\n",
            "Iteration 743, Loss: 0.03903574496507645\n",
            "Iteration 744, Loss: 0.04385976493358612\n",
            "Iteration 745, Loss: 0.048430435359478\n",
            "Iteration 746, Loss: 0.05745730549097061\n",
            "Iteration 747, Loss: 0.06372745335102081\n",
            "Iteration 748, Loss: 0.04542811959981918\n",
            "Iteration 749, Loss: 0.046300847083330154\n",
            "Iteration 750, Loss: 0.05092182755470276\n",
            "Iteration 751, Loss: 0.04247232526540756\n",
            "Iteration 752, Loss: 0.05369322746992111\n",
            "Iteration 753, Loss: 0.05714663863182068\n",
            "Iteration 754, Loss: 0.04414752870798111\n",
            "Iteration 755, Loss: 0.049646783620119095\n",
            "Iteration 756, Loss: 0.05165492743253708\n",
            "Iteration 757, Loss: 0.048159729689359665\n",
            "Iteration 758, Loss: 0.03166354447603226\n",
            "Iteration 759, Loss: 0.04569089785218239\n",
            "Iteration 760, Loss: 0.037152886390686035\n",
            "Iteration 761, Loss: 0.04713447019457817\n",
            "Iteration 762, Loss: 0.0583798848092556\n",
            "Iteration 763, Loss: 0.05249568819999695\n",
            "Iteration 764, Loss: 0.054149799048900604\n",
            "Iteration 765, Loss: 0.052843861281871796\n",
            "Iteration 766, Loss: 0.052118416875600815\n",
            "Iteration 767, Loss: 0.02415865659713745\n",
            "Iteration 768, Loss: 0.03774826601147652\n",
            "Iteration 769, Loss: 0.046219274401664734\n",
            "Iteration 770, Loss: 0.04674798250198364\n",
            "Iteration 771, Loss: 0.05690751224756241\n",
            "Iteration 772, Loss: 0.040790531784296036\n",
            "Iteration 773, Loss: 0.03782731667160988\n",
            "Iteration 774, Loss: 0.037547819316387177\n",
            "Iteration 775, Loss: 0.0426005981862545\n",
            "Iteration 776, Loss: 0.05522913858294487\n",
            "Iteration 777, Loss: 0.04367508739233017\n",
            "Iteration 778, Loss: 0.03716840595006943\n",
            "Iteration 779, Loss: 0.06500237435102463\n",
            "Iteration 780, Loss: 0.049020469188690186\n",
            "Iteration 781, Loss: 0.05543413758277893\n",
            "Iteration 782, Loss: 0.05097140371799469\n",
            "Iteration 783, Loss: 0.048725057393312454\n",
            "Iteration 784, Loss: 0.05535726621747017\n",
            "Iteration 785, Loss: 0.04685716703534126\n",
            "Iteration 786, Loss: 0.0462053082883358\n",
            "Iteration 787, Loss: 0.053398508578538895\n",
            "Iteration 788, Loss: 0.07829200476408005\n",
            "Iteration 789, Loss: 0.049557387828826904\n",
            "Iteration 790, Loss: 0.06743398308753967\n",
            "Iteration 791, Loss: 0.045337874442338943\n",
            "Iteration 792, Loss: 0.07358346879482269\n",
            "Iteration 793, Loss: 0.0480162687599659\n",
            "Iteration 794, Loss: 0.035861365497112274\n",
            "Iteration 795, Loss: 0.06920973211526871\n",
            "Iteration 796, Loss: 0.03939410671591759\n",
            "Iteration 797, Loss: 0.056316275149583817\n",
            "Iteration 798, Loss: 0.05172153189778328\n",
            "Iteration 799, Loss: 0.05253828316926956\n",
            "Iteration 800, Loss: 0.04794413968920708\n",
            "Iteration 801, Loss: 0.07348398864269257\n",
            "Iteration 802, Loss: 0.04191865399479866\n",
            "Iteration 803, Loss: 0.05077776685357094\n",
            "Iteration 804, Loss: 0.04966462031006813\n",
            "Iteration 805, Loss: 0.06925272196531296\n",
            "Iteration 806, Loss: 0.06268592178821564\n",
            "Iteration 807, Loss: 0.05057595670223236\n",
            "Iteration 808, Loss: 0.05757750943303108\n",
            "Iteration 809, Loss: 0.05122099444270134\n",
            "Iteration 810, Loss: 0.041339389979839325\n",
            "Iteration 811, Loss: 0.03376399353146553\n",
            "Iteration 812, Loss: 0.04185217246413231\n",
            "Iteration 813, Loss: 0.05791883170604706\n",
            "Iteration 814, Loss: 0.038879889994859695\n",
            "Iteration 815, Loss: 0.05730712041258812\n",
            "Iteration 816, Loss: 0.04597010836005211\n",
            "Iteration 817, Loss: 0.047347474843263626\n",
            "Iteration 818, Loss: 0.0591370090842247\n",
            "Iteration 819, Loss: 0.04369032382965088\n",
            "Iteration 820, Loss: 0.039726197719573975\n",
            "Iteration 821, Loss: 0.06392932683229446\n",
            "Iteration 822, Loss: 0.044078271836042404\n",
            "Iteration 823, Loss: 0.03754401579499245\n",
            "Iteration 824, Loss: 0.06772733479738235\n",
            "Iteration 825, Loss: 0.048374518752098083\n",
            "Iteration 826, Loss: 0.047876276075839996\n",
            "Iteration 827, Loss: 0.049101829528808594\n",
            "Iteration 828, Loss: 0.05837206542491913\n",
            "Iteration 829, Loss: 0.04837081953883171\n",
            "Iteration 830, Loss: 0.059999965131282806\n",
            "Iteration 831, Loss: 0.045325204730033875\n",
            "Iteration 832, Loss: 0.052703939378261566\n",
            "Iteration 833, Loss: 0.046922072768211365\n",
            "Iteration 834, Loss: 0.044084712862968445\n",
            "Iteration 835, Loss: 0.047141749411821365\n",
            "Iteration 836, Loss: 0.04570275917649269\n",
            "Iteration 837, Loss: 0.041398484259843826\n",
            "Iteration 838, Loss: 0.055032264441251755\n",
            "Iteration 839, Loss: 0.04233323037624359\n",
            "Iteration 840, Loss: 0.04251621291041374\n",
            "Iteration 841, Loss: 0.04496106505393982\n",
            "Iteration 842, Loss: 0.06416977196931839\n",
            "Iteration 843, Loss: 0.046117283403873444\n",
            "Iteration 844, Loss: 0.043118879199028015\n",
            "Iteration 845, Loss: 0.03486794978380203\n",
            "Iteration 846, Loss: 0.04067479819059372\n",
            "Iteration 847, Loss: 0.04725618660449982\n",
            "Iteration 848, Loss: 0.055393870919942856\n",
            "Iteration 849, Loss: 0.0352267287671566\n",
            "Iteration 850, Loss: 0.05427585914731026\n",
            "Iteration 851, Loss: 0.052963197231292725\n",
            "Iteration 852, Loss: 0.05067571997642517\n",
            "Iteration 853, Loss: 0.041794463992118835\n",
            "Iteration 854, Loss: 0.06220165640115738\n",
            "Iteration 855, Loss: 0.044958364218473434\n",
            "Iteration 856, Loss: 0.03381648287177086\n",
            "Iteration 857, Loss: 0.04425912722945213\n",
            "Iteration 858, Loss: 0.03379698470234871\n",
            "Iteration 859, Loss: 0.052271075546741486\n",
            "Iteration 860, Loss: 0.03699973225593567\n",
            "Iteration 861, Loss: 0.04231822490692139\n",
            "Iteration 862, Loss: 0.03390473499894142\n",
            "Iteration 863, Loss: 0.055910900235176086\n",
            "Iteration 864, Loss: 0.032146990299224854\n",
            "Iteration 865, Loss: 0.051327675580978394\n",
            "Iteration 866, Loss: 0.04043072462081909\n",
            "Iteration 867, Loss: 0.0418466180562973\n",
            "Iteration 868, Loss: 0.03410561382770538\n",
            "Iteration 869, Loss: 0.043772272765636444\n",
            "Iteration 870, Loss: 0.04162108525633812\n",
            "Iteration 871, Loss: 0.04677050560712814\n",
            "Iteration 872, Loss: 0.034065984189510345\n",
            "Iteration 873, Loss: 0.06759922951459885\n",
            "Iteration 874, Loss: 0.05639244616031647\n",
            "Iteration 875, Loss: 0.04163273423910141\n",
            "Iteration 876, Loss: 0.04521569609642029\n",
            "Iteration 877, Loss: 0.051145490258932114\n",
            "Iteration 878, Loss: 0.0521417111158371\n",
            "Iteration 879, Loss: 0.04307522997260094\n",
            "Iteration 880, Loss: 0.06291943788528442\n",
            "Iteration 881, Loss: 0.053455717861652374\n",
            "Iteration 882, Loss: 0.04991161823272705\n",
            "Iteration 883, Loss: 0.05674370750784874\n",
            "Iteration 884, Loss: 0.0560443140566349\n",
            "Iteration 885, Loss: 0.05050535500049591\n",
            "Iteration 886, Loss: 0.04215365648269653\n",
            "Iteration 887, Loss: 0.05779363960027695\n",
            "Iteration 888, Loss: 0.033770766109228134\n",
            "Iteration 889, Loss: 0.06298942863941193\n",
            "Iteration 890, Loss: 0.05775334686040878\n",
            "Iteration 891, Loss: 0.03691990673542023\n",
            "Iteration 892, Loss: 0.025796443223953247\n",
            "Iteration 893, Loss: 0.060177579522132874\n",
            "Iteration 894, Loss: 0.03357172757387161\n",
            "Iteration 895, Loss: 0.05039454251527786\n",
            "Iteration 896, Loss: 0.07174016535282135\n",
            "Iteration 897, Loss: 0.0653800517320633\n",
            "Iteration 898, Loss: 0.04555465281009674\n",
            "Iteration 899, Loss: 0.03754672035574913\n",
            "Iteration 900, Loss: 0.034310366958379745\n",
            "Iteration 901, Loss: 0.04514727368950844\n",
            "Iteration 902, Loss: 0.04037025570869446\n",
            "Iteration 903, Loss: 0.06098129227757454\n",
            "Iteration 904, Loss: 0.042855456471443176\n",
            "Iteration 905, Loss: 0.030470596626400948\n",
            "Iteration 906, Loss: 0.05436699837446213\n",
            "Iteration 907, Loss: 0.03159730136394501\n",
            "Iteration 908, Loss: 0.03951746225357056\n",
            "Iteration 909, Loss: 0.05191439390182495\n",
            "Iteration 910, Loss: 0.05410486459732056\n",
            "Iteration 911, Loss: 0.056212376803159714\n",
            "Iteration 912, Loss: 0.0299596656113863\n",
            "Iteration 913, Loss: 0.05192025005817413\n",
            "Iteration 914, Loss: 0.05049858242273331\n",
            "Iteration 915, Loss: 0.05771693214774132\n",
            "Iteration 916, Loss: 0.04886295646429062\n",
            "Iteration 917, Loss: 0.0636570006608963\n",
            "Iteration 918, Loss: 0.04187933728098869\n",
            "Iteration 919, Loss: 0.04720295965671539\n",
            "Iteration 920, Loss: 0.0345180369913578\n",
            "Iteration 921, Loss: 0.05018991231918335\n",
            "Iteration 922, Loss: 0.052395448088645935\n",
            "Iteration 923, Loss: 0.035406842827796936\n",
            "Iteration 924, Loss: 0.05095922574400902\n",
            "Iteration 925, Loss: 0.05988435819745064\n",
            "Iteration 926, Loss: 0.036868445575237274\n",
            "Iteration 927, Loss: 0.0565682090818882\n",
            "Iteration 928, Loss: 0.04353165626525879\n",
            "Iteration 929, Loss: 0.05064379423856735\n",
            "Iteration 930, Loss: 0.045194968581199646\n",
            "Iteration 931, Loss: 0.040151938796043396\n",
            "Iteration 932, Loss: 0.05424167215824127\n",
            "Iteration 933, Loss: 0.03132786974310875\n",
            "Iteration 934, Loss: 0.05187427997589111\n",
            "Iteration 935, Loss: 0.03171868249773979\n",
            "Iteration 936, Loss: 0.05158747732639313\n",
            "Iteration 937, Loss: 0.05096124857664108\n",
            "Iteration 938, Loss: 0.027661073952913284\n",
            "Iteration 939, Loss: 0.05663483217358589\n",
            "Iteration 940, Loss: 0.048083964735269547\n",
            "Iteration 941, Loss: 0.05359172821044922\n",
            "Iteration 942, Loss: 0.03475041687488556\n",
            "Iteration 943, Loss: 0.0414421409368515\n",
            "Iteration 944, Loss: 0.03870776668190956\n",
            "Iteration 945, Loss: 0.04221075400710106\n",
            "Iteration 946, Loss: 0.03477136045694351\n",
            "Iteration 947, Loss: 0.0553036667406559\n",
            "Iteration 948, Loss: 0.04623676463961601\n",
            "Iteration 949, Loss: 0.039237670600414276\n",
            "Iteration 950, Loss: 0.03945295512676239\n",
            "Iteration 951, Loss: 0.044960420578718185\n",
            "Iteration 952, Loss: 0.0374528206884861\n",
            "Iteration 953, Loss: 0.04446682706475258\n",
            "Iteration 954, Loss: 0.04429592192173004\n",
            "Iteration 955, Loss: 0.05112263932824135\n",
            "Iteration 956, Loss: 0.05333652347326279\n",
            "Iteration 957, Loss: 0.0522511787712574\n",
            "Iteration 958, Loss: 0.03449399769306183\n",
            "Iteration 959, Loss: 0.03730957210063934\n",
            "Iteration 960, Loss: 0.03789537400007248\n",
            "Iteration 961, Loss: 0.04695881903171539\n",
            "Iteration 962, Loss: 0.04584692046046257\n",
            "Iteration 963, Loss: 0.06820273399353027\n",
            "Iteration 964, Loss: 0.031017648056149483\n",
            "Iteration 965, Loss: 0.059291619807481766\n",
            "Iteration 966, Loss: 0.07093846052885056\n",
            "Iteration 967, Loss: 0.0525902584195137\n",
            "Iteration 968, Loss: 0.05782214552164078\n",
            "Iteration 969, Loss: 0.02980414777994156\n",
            "Iteration 970, Loss: 0.04457098990678787\n",
            "Iteration 971, Loss: 0.057219069451093674\n",
            "Iteration 972, Loss: 0.05225652456283569\n",
            "Iteration 973, Loss: 0.0389278382062912\n",
            "Iteration 974, Loss: 0.032379522919654846\n",
            "Iteration 975, Loss: 0.04279759153723717\n",
            "Iteration 976, Loss: 0.05755334347486496\n",
            "Iteration 977, Loss: 0.05371725559234619\n",
            "Iteration 978, Loss: 0.04756094887852669\n",
            "Iteration 979, Loss: 0.04008597880601883\n",
            "Iteration 980, Loss: 0.025754012167453766\n",
            "Iteration 981, Loss: 0.0658375695347786\n",
            "Iteration 982, Loss: 0.0614384301006794\n",
            "Iteration 983, Loss: 0.061369240283966064\n",
            "Iteration 984, Loss: 0.05048994719982147\n",
            "Iteration 985, Loss: 0.032745394855737686\n",
            "Iteration 986, Loss: 0.04867980629205704\n",
            "Iteration 987, Loss: 0.047216445207595825\n",
            "Iteration 988, Loss: 0.055437907576560974\n",
            "Iteration 989, Loss: 0.03891060873866081\n",
            "Iteration 990, Loss: 0.050213493406772614\n",
            "Iteration 991, Loss: 0.045769136399030685\n",
            "Iteration 992, Loss: 0.044721417129039764\n",
            "Iteration 993, Loss: 0.04137830063700676\n",
            "Iteration 994, Loss: 0.050561387091875076\n",
            "Iteration 995, Loss: 0.03452811762690544\n",
            "Iteration 996, Loss: 0.05561467632651329\n",
            "Iteration 997, Loss: 0.05761296674609184\n",
            "Iteration 998, Loss: 0.06451278924942017\n",
            "Iteration 999, Loss: 0.03737301751971245\n",
            "Iteration 1000, Loss: 0.0376150868833065\n",
            "Test Loss: 0.044918932020664215\n",
            "Iteration 1001, Loss: 0.04090391471982002\n",
            "Iteration 1002, Loss: 0.05851767957210541\n",
            "Iteration 1003, Loss: 0.05184764042496681\n",
            "Iteration 1004, Loss: 0.04828988015651703\n",
            "Iteration 1005, Loss: 0.04523199424147606\n",
            "Iteration 1006, Loss: 0.03772161528468132\n",
            "Iteration 1007, Loss: 0.040128618478775024\n",
            "Iteration 1008, Loss: 0.04802509769797325\n",
            "Iteration 1009, Loss: 0.05307154357433319\n",
            "Iteration 1010, Loss: 0.055798158049583435\n",
            "Iteration 1011, Loss: 0.037789858877658844\n",
            "Iteration 1012, Loss: 0.03517487272620201\n",
            "Iteration 1013, Loss: 0.07113794982433319\n",
            "Iteration 1014, Loss: 0.045892227441072464\n",
            "Iteration 1015, Loss: 0.050579559057950974\n",
            "Iteration 1016, Loss: 0.04396343603730202\n",
            "Iteration 1017, Loss: 0.04808195307850838\n",
            "Iteration 1018, Loss: 0.05042285472154617\n",
            "Iteration 1019, Loss: 0.04532856494188309\n",
            "Iteration 1020, Loss: 0.04680372774600983\n",
            "Iteration 1021, Loss: 0.07308666408061981\n",
            "Iteration 1022, Loss: 0.03700539842247963\n",
            "Iteration 1023, Loss: 0.041729509830474854\n",
            "Iteration 1024, Loss: 0.0428769625723362\n",
            "Iteration 1025, Loss: 0.054676465690135956\n",
            "Iteration 1026, Loss: 0.030769215896725655\n",
            "Iteration 1027, Loss: 0.05690436437726021\n",
            "Iteration 1028, Loss: 0.0422651469707489\n",
            "Iteration 1029, Loss: 0.05974637717008591\n",
            "Iteration 1030, Loss: 0.04510801285505295\n",
            "Iteration 1031, Loss: 0.04149457439780235\n",
            "Iteration 1032, Loss: 0.054947711527347565\n",
            "Iteration 1033, Loss: 0.03976868838071823\n",
            "Iteration 1034, Loss: 0.02897261269390583\n",
            "Iteration 1035, Loss: 0.028335051611065865\n",
            "Iteration 1036, Loss: 0.04599932208657265\n",
            "Iteration 1037, Loss: 0.04943373426795006\n",
            "Iteration 1038, Loss: 0.035937100648880005\n",
            "Iteration 1039, Loss: 0.04090021923184395\n",
            "Iteration 1040, Loss: 0.04477136582136154\n",
            "Iteration 1041, Loss: 0.05472642555832863\n",
            "Iteration 1042, Loss: 0.02525114081799984\n",
            "Iteration 1043, Loss: 0.0405539944767952\n",
            "Iteration 1044, Loss: 0.03608589991927147\n",
            "Iteration 1045, Loss: 0.03413707762956619\n",
            "Iteration 1046, Loss: 0.053792525082826614\n",
            "Iteration 1047, Loss: 0.047817159444093704\n",
            "Iteration 1048, Loss: 0.06978126615285873\n",
            "Iteration 1049, Loss: 0.04277949407696724\n",
            "Iteration 1050, Loss: 0.042765721678733826\n",
            "Iteration 1051, Loss: 0.04407805576920509\n",
            "Iteration 1052, Loss: 0.04602840170264244\n",
            "Iteration 1053, Loss: 0.03739975392818451\n",
            "Iteration 1054, Loss: 0.04967460036277771\n",
            "Iteration 1055, Loss: 0.033272627741098404\n",
            "Iteration 1056, Loss: 0.040866587311029434\n",
            "Iteration 1057, Loss: 0.04261912778019905\n",
            "Iteration 1058, Loss: 0.04806402325630188\n",
            "Iteration 1059, Loss: 0.05302011966705322\n",
            "Iteration 1060, Loss: 0.047493454068899155\n",
            "Iteration 1061, Loss: 0.04997721686959267\n",
            "Iteration 1062, Loss: 0.039746738970279694\n",
            "Iteration 1063, Loss: 0.06598102301359177\n",
            "Iteration 1064, Loss: 0.037315741181373596\n",
            "Iteration 1065, Loss: 0.05034976452589035\n",
            "Iteration 1066, Loss: 0.049448996782302856\n",
            "Iteration 1067, Loss: 0.04078951105475426\n",
            "Iteration 1068, Loss: 0.04618007317185402\n",
            "Iteration 1069, Loss: 0.04701027646660805\n",
            "Iteration 1070, Loss: 0.04302410036325455\n",
            "Iteration 1071, Loss: 0.05233728885650635\n",
            "Iteration 1072, Loss: 0.05283293500542641\n",
            "Iteration 1073, Loss: 0.04725641757249832\n",
            "Iteration 1074, Loss: 0.05598340183496475\n",
            "Iteration 1075, Loss: 0.05221063643693924\n",
            "Iteration 1076, Loss: 0.028476595878601074\n",
            "Iteration 1077, Loss: 0.03583597391843796\n",
            "Iteration 1078, Loss: 0.025036446750164032\n",
            "Iteration 1079, Loss: 0.04462667927145958\n",
            "Iteration 1080, Loss: 0.03501664847135544\n",
            "Iteration 1081, Loss: 0.03518490865826607\n",
            "Iteration 1082, Loss: 0.028724929317831993\n",
            "Iteration 1083, Loss: 0.0490143746137619\n",
            "Iteration 1084, Loss: 0.05551290884613991\n",
            "Iteration 1085, Loss: 0.03078220598399639\n",
            "Iteration 1086, Loss: 0.051713671535253525\n",
            "Iteration 1087, Loss: 0.044474028050899506\n",
            "Iteration 1088, Loss: 0.036671847105026245\n",
            "Iteration 1089, Loss: 0.04774677753448486\n",
            "Iteration 1090, Loss: 0.03296836093068123\n",
            "Iteration 1091, Loss: 0.03155169636011124\n",
            "Iteration 1092, Loss: 0.058844294399023056\n",
            "Iteration 1093, Loss: 0.03185997158288956\n",
            "Iteration 1094, Loss: 0.05786866322159767\n",
            "Iteration 1095, Loss: 0.039550721645355225\n",
            "Iteration 1096, Loss: 0.047447800636291504\n",
            "Iteration 1097, Loss: 0.03375222533941269\n",
            "Iteration 1098, Loss: 0.04482532665133476\n",
            "Iteration 1099, Loss: 0.026794077828526497\n",
            "Iteration 1100, Loss: 0.0418219231069088\n",
            "Iteration 1101, Loss: 0.028380608186125755\n",
            "Iteration 1102, Loss: 0.04098692163825035\n",
            "Iteration 1103, Loss: 0.03187095746397972\n",
            "Iteration 1104, Loss: 0.03737277910113335\n",
            "Iteration 1105, Loss: 0.03574465587735176\n",
            "Iteration 1106, Loss: 0.049168094992637634\n",
            "Iteration 1107, Loss: 0.04421886429190636\n",
            "Iteration 1108, Loss: 0.02885298803448677\n",
            "Iteration 1109, Loss: 0.04427286237478256\n",
            "Iteration 1110, Loss: 0.040500812232494354\n",
            "Iteration 1111, Loss: 0.04505286365747452\n",
            "Iteration 1112, Loss: 0.050123266875743866\n",
            "Iteration 1113, Loss: 0.040368225425481796\n",
            "Iteration 1114, Loss: 0.06192811205983162\n",
            "Iteration 1115, Loss: 0.04665420204401016\n",
            "Iteration 1116, Loss: 0.046389948576688766\n",
            "Iteration 1117, Loss: 0.024426784366369247\n",
            "Iteration 1118, Loss: 0.035674382001161575\n",
            "Iteration 1119, Loss: 0.03859927877783775\n",
            "Iteration 1120, Loss: 0.025253625586628914\n",
            "Iteration 1121, Loss: 0.037620387971401215\n",
            "Iteration 1122, Loss: 0.02715448848903179\n",
            "Iteration 1123, Loss: 0.040266864001750946\n",
            "Iteration 1124, Loss: 0.037966810166835785\n",
            "Iteration 1125, Loss: 0.042509254068136215\n",
            "Iteration 1126, Loss: 0.03146838769316673\n",
            "Iteration 1127, Loss: 0.054909903556108475\n",
            "Iteration 1128, Loss: 0.06492569297552109\n",
            "Iteration 1129, Loss: 0.06318396329879761\n",
            "Iteration 1130, Loss: 0.04220831021666527\n",
            "Iteration 1131, Loss: 0.04860163480043411\n",
            "Iteration 1132, Loss: 0.040773116052150726\n",
            "Iteration 1133, Loss: 0.051596567034721375\n",
            "Iteration 1134, Loss: 0.049881745129823685\n",
            "Iteration 1135, Loss: 0.033377505838871\n",
            "Iteration 1136, Loss: 0.03529918193817139\n",
            "Iteration 1137, Loss: 0.031428851187229156\n",
            "Iteration 1138, Loss: 0.05488326773047447\n",
            "Iteration 1139, Loss: 0.03004245087504387\n",
            "Iteration 1140, Loss: 0.03949941694736481\n",
            "Iteration 1141, Loss: 0.03069528006017208\n",
            "Iteration 1142, Loss: 0.03483790159225464\n",
            "Iteration 1143, Loss: 0.04581383988261223\n",
            "Iteration 1144, Loss: 0.038987576961517334\n",
            "Iteration 1145, Loss: 0.03827223554253578\n",
            "Iteration 1146, Loss: 0.024249734356999397\n",
            "Iteration 1147, Loss: 0.02711397595703602\n",
            "Iteration 1148, Loss: 0.04972807690501213\n",
            "Iteration 1149, Loss: 0.05664198845624924\n",
            "Iteration 1150, Loss: 0.05467110872268677\n",
            "Iteration 1151, Loss: 0.045997124165296555\n",
            "Iteration 1152, Loss: 0.03152121603488922\n",
            "Iteration 1153, Loss: 0.03747188299894333\n",
            "Iteration 1154, Loss: 0.0395754836499691\n",
            "Iteration 1155, Loss: 0.033410049974918365\n",
            "Iteration 1156, Loss: 0.041735146194696426\n",
            "Iteration 1157, Loss: 0.034820470958948135\n",
            "Iteration 1158, Loss: 0.041214197874069214\n",
            "Iteration 1159, Loss: 0.04297024756669998\n",
            "Iteration 1160, Loss: 0.03542042523622513\n",
            "Iteration 1161, Loss: 0.031436823308467865\n",
            "Iteration 1162, Loss: 0.041065655648708344\n",
            "Iteration 1163, Loss: 0.04209991171956062\n",
            "Iteration 1164, Loss: 0.033708978444337845\n",
            "Iteration 1165, Loss: 0.03476468846201897\n",
            "Iteration 1166, Loss: 0.06515953689813614\n",
            "Iteration 1167, Loss: 0.04451099783182144\n",
            "Iteration 1168, Loss: 0.050394315272569656\n",
            "Iteration 1169, Loss: 0.03679713234305382\n",
            "Iteration 1170, Loss: 0.05170941725373268\n",
            "Iteration 1171, Loss: 0.04274189844727516\n",
            "Iteration 1172, Loss: 0.0522211529314518\n",
            "Iteration 1173, Loss: 0.051397405564785004\n",
            "Iteration 1174, Loss: 0.04017031192779541\n",
            "Iteration 1175, Loss: 0.03158242255449295\n",
            "Iteration 1176, Loss: 0.043115925043821335\n",
            "Iteration 1177, Loss: 0.04190357029438019\n",
            "Iteration 1178, Loss: 0.03718942031264305\n",
            "Iteration 1179, Loss: 0.04850969463586807\n",
            "Iteration 1180, Loss: 0.04621380195021629\n",
            "Iteration 1181, Loss: 0.0467614009976387\n",
            "Iteration 1182, Loss: 0.03844229131937027\n",
            "Iteration 1183, Loss: 0.03872661665081978\n",
            "Iteration 1184, Loss: 0.03407561033964157\n",
            "Iteration 1185, Loss: 0.04759160429239273\n",
            "Iteration 1186, Loss: 0.03960447013378143\n",
            "Iteration 1187, Loss: 0.04220009967684746\n",
            "Iteration 1188, Loss: 0.04360203072428703\n",
            "Iteration 1189, Loss: 0.02968788705766201\n",
            "Iteration 1190, Loss: 0.044063229113817215\n",
            "Iteration 1191, Loss: 0.04630115628242493\n",
            "Iteration 1192, Loss: 0.03408946469426155\n",
            "Iteration 1193, Loss: 0.05004797875881195\n",
            "Iteration 1194, Loss: 0.03172694519162178\n",
            "Iteration 1195, Loss: 0.03743424639105797\n",
            "Iteration 1196, Loss: 0.023463299497961998\n",
            "Iteration 1197, Loss: 0.03326427936553955\n",
            "Iteration 1198, Loss: 0.04057670012116432\n",
            "Iteration 1199, Loss: 0.05019211396574974\n",
            "Iteration 1200, Loss: 0.0588407889008522\n",
            "Iteration 1201, Loss: 0.03840149939060211\n",
            "Iteration 1202, Loss: 0.045256856828927994\n",
            "Iteration 1203, Loss: 0.033284593373537064\n",
            "Iteration 1204, Loss: 0.04823747277259827\n",
            "Iteration 1205, Loss: 0.04005900397896767\n",
            "Iteration 1206, Loss: 0.042208198457956314\n",
            "Iteration 1207, Loss: 0.04160882905125618\n",
            "Iteration 1208, Loss: 0.03613569587469101\n",
            "Iteration 1209, Loss: 0.033823732286691666\n",
            "Iteration 1210, Loss: 0.03289458528161049\n",
            "Iteration 1211, Loss: 0.047512296587228775\n",
            "Iteration 1212, Loss: 0.04634222388267517\n",
            "Iteration 1213, Loss: 0.05596019700169563\n",
            "Iteration 1214, Loss: 0.03668646514415741\n",
            "Iteration 1215, Loss: 0.03639979287981987\n",
            "Iteration 1216, Loss: 0.03419321030378342\n",
            "Iteration 1217, Loss: 0.04169021546840668\n",
            "Iteration 1218, Loss: 0.033999476581811905\n",
            "Iteration 1219, Loss: 0.03871176019310951\n",
            "Iteration 1220, Loss: 0.03298294544219971\n",
            "Iteration 1221, Loss: 0.07260237634181976\n",
            "Iteration 1222, Loss: 0.03059079684317112\n",
            "Iteration 1223, Loss: 0.03113376349210739\n",
            "Iteration 1224, Loss: 0.036106694489717484\n",
            "Iteration 1225, Loss: 0.04537037014961243\n",
            "Iteration 1226, Loss: 0.04522698372602463\n",
            "Iteration 1227, Loss: 0.04152151197195053\n",
            "Iteration 1228, Loss: 0.03860998898744583\n",
            "Iteration 1229, Loss: 0.03447255492210388\n",
            "Iteration 1230, Loss: 0.03938259184360504\n",
            "Iteration 1231, Loss: 0.035058069974184036\n",
            "Iteration 1232, Loss: 0.04071524366736412\n",
            "Iteration 1233, Loss: 0.05503137782216072\n",
            "Iteration 1234, Loss: 0.033398933708667755\n",
            "Iteration 1235, Loss: 0.036570049822330475\n",
            "Iteration 1236, Loss: 0.032066985964775085\n",
            "Iteration 1237, Loss: 0.021993068978190422\n",
            "Iteration 1238, Loss: 0.03424737975001335\n",
            "Iteration 1239, Loss: 0.029538260772824287\n",
            "Iteration 1240, Loss: 0.042704787105321884\n",
            "Iteration 1241, Loss: 0.057329729199409485\n",
            "Iteration 1242, Loss: 0.0657331570982933\n",
            "Iteration 1243, Loss: 0.05277111381292343\n",
            "Iteration 1244, Loss: 0.03842070698738098\n",
            "Iteration 1245, Loss: 0.04469706490635872\n",
            "Iteration 1246, Loss: 0.039354730397462845\n",
            "Iteration 1247, Loss: 0.03950825706124306\n",
            "Iteration 1248, Loss: 0.03539412468671799\n",
            "Iteration 1249, Loss: 0.032661281526088715\n",
            "Iteration 1250, Loss: 0.05743497237563133\n",
            "Iteration 1251, Loss: 0.048016514629125595\n",
            "Iteration 1252, Loss: 0.03935359790921211\n",
            "Iteration 1253, Loss: 0.040228623896837234\n",
            "Iteration 1254, Loss: 0.045088231563568115\n",
            "Iteration 1255, Loss: 0.03562523052096367\n",
            "Iteration 1256, Loss: 0.0516803078353405\n",
            "Iteration 1257, Loss: 0.04682488739490509\n",
            "Iteration 1258, Loss: 0.0315125472843647\n",
            "Iteration 1259, Loss: 0.0449005663394928\n",
            "Iteration 1260, Loss: 0.040774229913949966\n",
            "Iteration 1261, Loss: 0.027042202651500702\n",
            "Iteration 1262, Loss: 0.0334685742855072\n",
            "Iteration 1263, Loss: 0.04702947288751602\n",
            "Iteration 1264, Loss: 0.05988898500800133\n",
            "Iteration 1265, Loss: 0.027387047186493874\n",
            "Iteration 1266, Loss: 0.03595234453678131\n",
            "Iteration 1267, Loss: 0.045894090086221695\n",
            "Iteration 1268, Loss: 0.0327705554664135\n",
            "Iteration 1269, Loss: 0.035941313952207565\n",
            "Iteration 1270, Loss: 0.026294119656085968\n",
            "Iteration 1271, Loss: 0.04268435761332512\n",
            "Iteration 1272, Loss: 0.042802587151527405\n",
            "Iteration 1273, Loss: 0.035020384937524796\n",
            "Iteration 1274, Loss: 0.02790318801999092\n",
            "Iteration 1275, Loss: 0.04308365657925606\n",
            "Iteration 1276, Loss: 0.04371574521064758\n",
            "Iteration 1277, Loss: 0.04110424220561981\n",
            "Iteration 1278, Loss: 0.04359236732125282\n",
            "Iteration 1279, Loss: 0.025441216304898262\n",
            "Iteration 1280, Loss: 0.027340611442923546\n",
            "Iteration 1281, Loss: 0.02658279985189438\n",
            "Iteration 1282, Loss: 0.04113809019327164\n",
            "Iteration 1283, Loss: 0.028797762468457222\n",
            "Iteration 1284, Loss: 0.04954550787806511\n",
            "Iteration 1285, Loss: 0.02642051689326763\n",
            "Iteration 1286, Loss: 0.04810748249292374\n",
            "Iteration 1287, Loss: 0.04032590985298157\n",
            "Iteration 1288, Loss: 0.04272187128663063\n",
            "Iteration 1289, Loss: 0.035877104848623276\n",
            "Iteration 1290, Loss: 0.038791511207818985\n",
            "Iteration 1291, Loss: 0.035012345761060715\n",
            "Iteration 1292, Loss: 0.03696364536881447\n",
            "Iteration 1293, Loss: 0.029167041182518005\n",
            "Iteration 1294, Loss: 0.0382051095366478\n",
            "Iteration 1295, Loss: 0.04111466556787491\n",
            "Iteration 1296, Loss: 0.03742401674389839\n",
            "Iteration 1297, Loss: 0.03413108363747597\n",
            "Iteration 1298, Loss: 0.04283860698342323\n",
            "Iteration 1299, Loss: 0.026821942999958992\n",
            "Iteration 1300, Loss: 0.059766460210084915\n",
            "Iteration 1301, Loss: 0.03788209334015846\n",
            "Iteration 1302, Loss: 0.046122051775455475\n",
            "Iteration 1303, Loss: 0.03510076552629471\n",
            "Iteration 1304, Loss: 0.03282548487186432\n",
            "Iteration 1305, Loss: 0.06466321647167206\n",
            "Iteration 1306, Loss: 0.03131122514605522\n",
            "Iteration 1307, Loss: 0.05404479056596756\n",
            "Iteration 1308, Loss: 0.04411784186959267\n",
            "Iteration 1309, Loss: 0.03541792556643486\n",
            "Iteration 1310, Loss: 0.026483749970793724\n",
            "Iteration 1311, Loss: 0.03736795485019684\n",
            "Iteration 1312, Loss: 0.058111559599637985\n",
            "Iteration 1313, Loss: 0.03869856521487236\n",
            "Iteration 1314, Loss: 0.04325178265571594\n",
            "Iteration 1315, Loss: 0.030740533024072647\n",
            "Iteration 1316, Loss: 0.024830391630530357\n",
            "Iteration 1317, Loss: 0.026880081743001938\n",
            "Iteration 1318, Loss: 0.05220592021942139\n",
            "Iteration 1319, Loss: 0.05568627640604973\n",
            "Iteration 1320, Loss: 0.05355352163314819\n",
            "Iteration 1321, Loss: 0.04274166375398636\n",
            "Iteration 1322, Loss: 0.05464836210012436\n",
            "Iteration 1323, Loss: 0.031202644109725952\n",
            "Iteration 1324, Loss: 0.04899359494447708\n",
            "Iteration 1325, Loss: 0.03759434446692467\n",
            "Iteration 1326, Loss: 0.034060701727867126\n",
            "Iteration 1327, Loss: 0.03433657065033913\n",
            "Iteration 1328, Loss: 0.04776573181152344\n",
            "Iteration 1329, Loss: 0.029336607083678246\n",
            "Iteration 1330, Loss: 0.04426879063248634\n",
            "Iteration 1331, Loss: 0.03677084296941757\n",
            "Iteration 1332, Loss: 0.030844200402498245\n",
            "Iteration 1333, Loss: 0.03675234317779541\n",
            "Iteration 1334, Loss: 0.027849426493048668\n",
            "Iteration 1335, Loss: 0.038742486387491226\n",
            "Iteration 1336, Loss: 0.04134688526391983\n",
            "Iteration 1337, Loss: 0.05273578315973282\n",
            "Iteration 1338, Loss: 0.03250115364789963\n",
            "Iteration 1339, Loss: 0.029361864551901817\n",
            "Iteration 1340, Loss: 0.040666721761226654\n",
            "Iteration 1341, Loss: 0.040473561733961105\n",
            "Iteration 1342, Loss: 0.029123147949576378\n",
            "Iteration 1343, Loss: 0.043077804148197174\n",
            "Iteration 1344, Loss: 0.026013007387518883\n",
            "Iteration 1345, Loss: 0.033720459789037704\n",
            "Iteration 1346, Loss: 0.04695842042565346\n",
            "Iteration 1347, Loss: 0.0363924540579319\n",
            "Iteration 1348, Loss: 0.017908215522766113\n",
            "Iteration 1349, Loss: 0.03458366543054581\n",
            "Iteration 1350, Loss: 0.024203432723879814\n",
            "Iteration 1351, Loss: 0.02372003346681595\n",
            "Iteration 1352, Loss: 0.03635486587882042\n",
            "Iteration 1353, Loss: 0.04247676581144333\n",
            "Iteration 1354, Loss: 0.0336286760866642\n",
            "Iteration 1355, Loss: 0.029982933774590492\n",
            "Iteration 1356, Loss: 0.03636341542005539\n",
            "Iteration 1357, Loss: 0.03956146910786629\n",
            "Iteration 1358, Loss: 0.052073705941438675\n",
            "Iteration 1359, Loss: 0.039201777428388596\n",
            "Iteration 1360, Loss: 0.03800670802593231\n",
            "Iteration 1361, Loss: 0.029584674164652824\n",
            "Iteration 1362, Loss: 0.04269137606024742\n",
            "Iteration 1363, Loss: 0.047364816069602966\n",
            "Iteration 1364, Loss: 0.029806727543473244\n",
            "Iteration 1365, Loss: 0.03642409294843674\n",
            "Iteration 1366, Loss: 0.04259370639920235\n",
            "Iteration 1367, Loss: 0.049387261271476746\n",
            "Iteration 1368, Loss: 0.04175982251763344\n",
            "Iteration 1369, Loss: 0.024789869785308838\n",
            "Iteration 1370, Loss: 0.02755753882229328\n",
            "Iteration 1371, Loss: 0.027065256610512733\n",
            "Iteration 1372, Loss: 0.027987882494926453\n",
            "Iteration 1373, Loss: 0.03235607221722603\n",
            "Iteration 1374, Loss: 0.03282248228788376\n",
            "Iteration 1375, Loss: 0.034398119896650314\n",
            "Iteration 1376, Loss: 0.046024784445762634\n",
            "Iteration 1377, Loss: 0.05703248083591461\n",
            "Iteration 1378, Loss: 0.022712863981723785\n",
            "Iteration 1379, Loss: 0.06557343155145645\n",
            "Iteration 1380, Loss: 0.037939321249723434\n",
            "Iteration 1381, Loss: 0.029357001185417175\n",
            "Iteration 1382, Loss: 0.03548920154571533\n",
            "Iteration 1383, Loss: 0.04072001576423645\n",
            "Iteration 1384, Loss: 0.02578354813158512\n",
            "Iteration 1385, Loss: 0.037265844643116\n",
            "Iteration 1386, Loss: 0.02759353071451187\n",
            "Iteration 1387, Loss: 0.05040217936038971\n",
            "Iteration 1388, Loss: 0.056572794914245605\n",
            "Iteration 1389, Loss: 0.04576774686574936\n",
            "Iteration 1390, Loss: 0.026918992400169373\n",
            "Iteration 1391, Loss: 0.03406031057238579\n",
            "Iteration 1392, Loss: 0.03907405957579613\n",
            "Iteration 1393, Loss: 0.04648473113775253\n",
            "Iteration 1394, Loss: 0.023931782692670822\n",
            "Iteration 1395, Loss: 0.03824826702475548\n",
            "Iteration 1396, Loss: 0.0399397574365139\n",
            "Iteration 1397, Loss: 0.047462861984968185\n",
            "Iteration 1398, Loss: 0.028782783076167107\n",
            "Iteration 1399, Loss: 0.029149962589144707\n",
            "Iteration 1400, Loss: 0.028366276994347572\n",
            "Iteration 1401, Loss: 0.029797589406371117\n",
            "Iteration 1402, Loss: 0.032143283635377884\n",
            "Iteration 1403, Loss: 0.035904400050640106\n",
            "Iteration 1404, Loss: 0.01883375458419323\n",
            "Iteration 1405, Loss: 0.04950479418039322\n",
            "Iteration 1406, Loss: 0.053108565509319305\n",
            "Iteration 1407, Loss: 0.03630439192056656\n",
            "Iteration 1408, Loss: 0.026562118902802467\n",
            "Iteration 1409, Loss: 0.03104223497211933\n",
            "Iteration 1410, Loss: 0.03541001304984093\n",
            "Iteration 1411, Loss: 0.034560270607471466\n",
            "Iteration 1412, Loss: 0.0398782342672348\n",
            "Iteration 1413, Loss: 0.027054766193032265\n",
            "Iteration 1414, Loss: 0.0674942135810852\n",
            "Iteration 1415, Loss: 0.043802037835121155\n",
            "Iteration 1416, Loss: 0.04628731682896614\n",
            "Iteration 1417, Loss: 0.028708530589938164\n",
            "Iteration 1418, Loss: 0.04909134283661842\n",
            "Iteration 1419, Loss: 0.042148128151893616\n",
            "Iteration 1420, Loss: 0.03763500601053238\n",
            "Iteration 1421, Loss: 0.04498336836695671\n",
            "Iteration 1422, Loss: 0.03998175263404846\n",
            "Iteration 1423, Loss: 0.051957521587610245\n",
            "Iteration 1424, Loss: 0.032414134591817856\n",
            "Iteration 1425, Loss: 0.03493744134902954\n",
            "Iteration 1426, Loss: 0.0332779623568058\n",
            "Iteration 1427, Loss: 0.04619906470179558\n",
            "Iteration 1428, Loss: 0.03169144690036774\n",
            "Iteration 1429, Loss: 0.04976820945739746\n",
            "Iteration 1430, Loss: 0.032680291682481766\n",
            "Iteration 1431, Loss: 0.05785638466477394\n",
            "Iteration 1432, Loss: 0.03988507017493248\n",
            "Iteration 1433, Loss: 0.035311903804540634\n",
            "Iteration 1434, Loss: 0.024855058640241623\n",
            "Iteration 1435, Loss: 0.04267779365181923\n",
            "Iteration 1436, Loss: 0.03471847251057625\n",
            "Iteration 1437, Loss: 0.02034156396985054\n",
            "Iteration 1438, Loss: 0.03867155686020851\n",
            "Iteration 1439, Loss: 0.034857865422964096\n",
            "Iteration 1440, Loss: 0.03124433383345604\n",
            "Iteration 1441, Loss: 0.02289051190018654\n",
            "Iteration 1442, Loss: 0.04856981337070465\n",
            "Iteration 1443, Loss: 0.03278191760182381\n",
            "Iteration 1444, Loss: 0.03429693728685379\n",
            "Iteration 1445, Loss: 0.03340922296047211\n",
            "Iteration 1446, Loss: 0.02993950992822647\n",
            "Iteration 1447, Loss: 0.03498271480202675\n",
            "Iteration 1448, Loss: 0.049419716000556946\n",
            "Iteration 1449, Loss: 0.044438596814870834\n",
            "Iteration 1450, Loss: 0.03446684032678604\n",
            "Iteration 1451, Loss: 0.028211072087287903\n",
            "Iteration 1452, Loss: 0.048151787370443344\n",
            "Iteration 1453, Loss: 0.03094993531703949\n",
            "Iteration 1454, Loss: 0.048257775604724884\n",
            "Iteration 1455, Loss: 0.03361062705516815\n",
            "Iteration 1456, Loss: 0.03279124200344086\n",
            "Iteration 1457, Loss: 0.03721084073185921\n",
            "Iteration 1458, Loss: 0.02996031939983368\n",
            "Iteration 1459, Loss: 0.03719954565167427\n",
            "Iteration 1460, Loss: 0.048631101846694946\n",
            "Iteration 1461, Loss: 0.04488170146942139\n",
            "Iteration 1462, Loss: 0.0478023886680603\n",
            "Iteration 1463, Loss: 0.03490744158625603\n",
            "Iteration 1464, Loss: 0.05207734927535057\n",
            "Iteration 1465, Loss: 0.04183349013328552\n",
            "Iteration 1466, Loss: 0.034582048654556274\n",
            "Iteration 1467, Loss: 0.036884285509586334\n",
            "Iteration 1468, Loss: 0.033002257347106934\n",
            "Iteration 1469, Loss: 0.032146818935871124\n",
            "Iteration 1470, Loss: 0.028483659029006958\n",
            "Iteration 1471, Loss: 0.03962568938732147\n",
            "Iteration 1472, Loss: 0.036462657153606415\n",
            "Iteration 1473, Loss: 0.04497666284441948\n",
            "Iteration 1474, Loss: 0.041421398520469666\n",
            "Iteration 1475, Loss: 0.03341797739267349\n",
            "Iteration 1476, Loss: 0.04276054725050926\n",
            "Iteration 1477, Loss: 0.04191010445356369\n",
            "Iteration 1478, Loss: 0.03417922556400299\n",
            "Iteration 1479, Loss: 0.04683948680758476\n",
            "Iteration 1480, Loss: 0.02358773723244667\n",
            "Iteration 1481, Loss: 0.02535419724881649\n",
            "Iteration 1482, Loss: 0.04436219111084938\n",
            "Iteration 1483, Loss: 0.04854554310441017\n",
            "Iteration 1484, Loss: 0.029909638687968254\n",
            "Iteration 1485, Loss: 0.04015371948480606\n",
            "Iteration 1486, Loss: 0.032318953424692154\n",
            "Iteration 1487, Loss: 0.050687070935964584\n",
            "Iteration 1488, Loss: 0.03135776147246361\n",
            "Iteration 1489, Loss: 0.035352859646081924\n",
            "Iteration 1490, Loss: 0.03847205638885498\n",
            "Iteration 1491, Loss: 0.03844676911830902\n",
            "Iteration 1492, Loss: 0.026659540832042694\n",
            "Iteration 1493, Loss: 0.04780009016394615\n",
            "Iteration 1494, Loss: 0.024951405823230743\n",
            "Iteration 1495, Loss: 0.05475684627890587\n",
            "Iteration 1496, Loss: 0.03235413506627083\n",
            "Iteration 1497, Loss: 0.04103367403149605\n",
            "Iteration 1498, Loss: 0.026080671697854996\n",
            "Iteration 1499, Loss: 0.027528762817382812\n",
            "Iteration 1500, Loss: 0.023550698533654213\n",
            "Iteration 1501, Loss: 0.057098522782325745\n",
            "Iteration 1502, Loss: 0.034156981855630875\n",
            "Iteration 1503, Loss: 0.046717580407857895\n",
            "Iteration 1504, Loss: 0.03635381907224655\n",
            "Iteration 1505, Loss: 0.03751871734857559\n",
            "Iteration 1506, Loss: 0.03533776104450226\n",
            "Iteration 1507, Loss: 0.03834761306643486\n",
            "Iteration 1508, Loss: 0.04170936346054077\n",
            "Iteration 1509, Loss: 0.0344647578895092\n",
            "Iteration 1510, Loss: 0.02844090759754181\n",
            "Iteration 1511, Loss: 0.03405482694506645\n",
            "Iteration 1512, Loss: 0.03126050904393196\n",
            "Iteration 1513, Loss: 0.04359860345721245\n",
            "Iteration 1514, Loss: 0.04431537166237831\n",
            "Iteration 1515, Loss: 0.03583819046616554\n",
            "Iteration 1516, Loss: 0.030592411756515503\n",
            "Iteration 1517, Loss: 0.03434830531477928\n",
            "Iteration 1518, Loss: 0.04604927822947502\n",
            "Iteration 1519, Loss: 0.030692918226122856\n",
            "Iteration 1520, Loss: 0.032024282962083817\n",
            "Iteration 1521, Loss: 0.043517421931028366\n",
            "Iteration 1522, Loss: 0.043208591639995575\n",
            "Iteration 1523, Loss: 0.04306730628013611\n",
            "Iteration 1524, Loss: 0.03973422944545746\n",
            "Iteration 1525, Loss: 0.041120804846286774\n",
            "Iteration 1526, Loss: 0.031324055045843124\n",
            "Iteration 1527, Loss: 0.03697315230965614\n",
            "Iteration 1528, Loss: 0.03260551393032074\n",
            "Iteration 1529, Loss: 0.03833850100636482\n",
            "Iteration 1530, Loss: 0.0363333597779274\n",
            "Iteration 1531, Loss: 0.03582756966352463\n",
            "Iteration 1532, Loss: 0.042319610714912415\n",
            "Iteration 1533, Loss: 0.03520738333463669\n",
            "Iteration 1534, Loss: 0.03037905879318714\n",
            "Iteration 1535, Loss: 0.01673220470547676\n",
            "Iteration 1536, Loss: 0.03161394223570824\n",
            "Iteration 1537, Loss: 0.028263641521334648\n",
            "Iteration 1538, Loss: 0.03287172690033913\n",
            "Iteration 1539, Loss: 0.03249093517661095\n",
            "Iteration 1540, Loss: 0.03407660499215126\n",
            "Iteration 1541, Loss: 0.03905835375189781\n",
            "Iteration 1542, Loss: 0.031218688935041428\n",
            "Iteration 1543, Loss: 0.034097932279109955\n",
            "Iteration 1544, Loss: 0.023649349808692932\n",
            "Iteration 1545, Loss: 0.03604098781943321\n",
            "Iteration 1546, Loss: 0.04130085930228233\n",
            "Iteration 1547, Loss: 0.03267901390790939\n",
            "Iteration 1548, Loss: 0.030349362641572952\n",
            "Iteration 1549, Loss: 0.04184664785861969\n",
            "Iteration 1550, Loss: 0.0512605682015419\n",
            "Iteration 1551, Loss: 0.025139881297945976\n",
            "Iteration 1552, Loss: 0.04105164483189583\n",
            "Iteration 1553, Loss: 0.034886568784713745\n",
            "Iteration 1554, Loss: 0.02944791503250599\n",
            "Iteration 1555, Loss: 0.027949456125497818\n",
            "Iteration 1556, Loss: 0.0459633395075798\n",
            "Iteration 1557, Loss: 0.02640553005039692\n",
            "Iteration 1558, Loss: 0.026798661798238754\n",
            "Iteration 1559, Loss: 0.033146824687719345\n",
            "Iteration 1560, Loss: 0.02896842733025551\n",
            "Iteration 1561, Loss: 0.0517924427986145\n",
            "Iteration 1562, Loss: 0.03901267051696777\n",
            "Iteration 1563, Loss: 0.05292739346623421\n",
            "Iteration 1564, Loss: 0.03169863298535347\n",
            "Iteration 1565, Loss: 0.03447183966636658\n",
            "Iteration 1566, Loss: 0.038322873413562775\n",
            "Iteration 1567, Loss: 0.03665625676512718\n",
            "Iteration 1568, Loss: 0.030345676466822624\n",
            "Iteration 1569, Loss: 0.04406725615262985\n",
            "Iteration 1570, Loss: 0.03403443470597267\n",
            "Iteration 1571, Loss: 0.037064239382743835\n",
            "Iteration 1572, Loss: 0.028175240382552147\n",
            "Iteration 1573, Loss: 0.043472666293382645\n",
            "Iteration 1574, Loss: 0.035476744174957275\n",
            "Iteration 1575, Loss: 0.05665576085448265\n",
            "Iteration 1576, Loss: 0.040252476930618286\n",
            "Iteration 1577, Loss: 0.021493926644325256\n",
            "Iteration 1578, Loss: 0.03151078149676323\n",
            "Iteration 1579, Loss: 0.027192315086722374\n",
            "Iteration 1580, Loss: 0.04035940021276474\n",
            "Iteration 1581, Loss: 0.050506725907325745\n",
            "Iteration 1582, Loss: 0.027873611077666283\n",
            "Iteration 1583, Loss: 0.04397371783852577\n",
            "Iteration 1584, Loss: 0.03131125494837761\n",
            "Iteration 1585, Loss: 0.03717567399144173\n",
            "Iteration 1586, Loss: 0.03680003806948662\n",
            "Iteration 1587, Loss: 0.03906315565109253\n",
            "Iteration 1588, Loss: 0.040475040674209595\n",
            "Iteration 1589, Loss: 0.02923140674829483\n",
            "Iteration 1590, Loss: 0.03226344287395477\n",
            "Iteration 1591, Loss: 0.030018912628293037\n",
            "Iteration 1592, Loss: 0.030954109504818916\n",
            "Iteration 1593, Loss: 0.03745708614587784\n",
            "Iteration 1594, Loss: 0.04871838912367821\n",
            "Iteration 1595, Loss: 0.02461140602827072\n",
            "Iteration 1596, Loss: 0.03423645719885826\n",
            "Iteration 1597, Loss: 0.029923327267169952\n",
            "Iteration 1598, Loss: 0.03754327818751335\n",
            "Iteration 1599, Loss: 0.040592294186353683\n",
            "Iteration 1600, Loss: 0.0362614281475544\n",
            "Iteration 1601, Loss: 0.033497631549835205\n",
            "Iteration 1602, Loss: 0.03555966541171074\n",
            "Iteration 1603, Loss: 0.031159203499555588\n",
            "Iteration 1604, Loss: 0.0331621915102005\n",
            "Iteration 1605, Loss: 0.033566851168870926\n",
            "Iteration 1606, Loss: 0.03685961663722992\n",
            "Iteration 1607, Loss: 0.02842804789543152\n",
            "Iteration 1608, Loss: 0.026523161679506302\n",
            "Iteration 1609, Loss: 0.02384858950972557\n",
            "Iteration 1610, Loss: 0.03702046722173691\n",
            "Iteration 1611, Loss: 0.022398706525564194\n",
            "Iteration 1612, Loss: 0.02613825909793377\n",
            "Iteration 1613, Loss: 0.03964322805404663\n",
            "Iteration 1614, Loss: 0.027447041124105453\n",
            "Iteration 1615, Loss: 0.039104949682950974\n",
            "Iteration 1616, Loss: 0.021325120702385902\n",
            "Iteration 1617, Loss: 0.0312713086605072\n",
            "Iteration 1618, Loss: 0.04497460275888443\n",
            "Iteration 1619, Loss: 0.03903093934059143\n",
            "Iteration 1620, Loss: 0.034758035093545914\n",
            "Iteration 1621, Loss: 0.02789703942835331\n",
            "Iteration 1622, Loss: 0.028055474162101746\n",
            "Iteration 1623, Loss: 0.03004402108490467\n",
            "Iteration 1624, Loss: 0.03536735475063324\n",
            "Iteration 1625, Loss: 0.03661806136369705\n",
            "Iteration 1626, Loss: 0.039721518754959106\n",
            "Iteration 1627, Loss: 0.03445972874760628\n",
            "Iteration 1628, Loss: 0.04229871183633804\n",
            "Iteration 1629, Loss: 0.03230694681406021\n",
            "Iteration 1630, Loss: 0.03377784043550491\n",
            "Iteration 1631, Loss: 0.04410355165600777\n",
            "Iteration 1632, Loss: 0.02860627882182598\n",
            "Iteration 1633, Loss: 0.040071796625852585\n",
            "Iteration 1634, Loss: 0.03309796378016472\n",
            "Iteration 1635, Loss: 0.03936260566115379\n",
            "Iteration 1636, Loss: 0.02299029938876629\n",
            "Iteration 1637, Loss: 0.03342438489198685\n",
            "Iteration 1638, Loss: 0.04149104654788971\n",
            "Iteration 1639, Loss: 0.019978169351816177\n",
            "Iteration 1640, Loss: 0.01613652892410755\n",
            "Iteration 1641, Loss: 0.040948376059532166\n",
            "Iteration 1642, Loss: 0.028155766427516937\n",
            "Iteration 1643, Loss: 0.03178120404481888\n",
            "Iteration 1644, Loss: 0.0225971732288599\n",
            "Iteration 1645, Loss: 0.032566823065280914\n",
            "Iteration 1646, Loss: 0.027500109747052193\n",
            "Iteration 1647, Loss: 0.0226620901376009\n",
            "Iteration 1648, Loss: 0.03586260974407196\n",
            "Iteration 1649, Loss: 0.026910880580544472\n",
            "Iteration 1650, Loss: 0.035265084356069565\n",
            "Iteration 1651, Loss: 0.030624354258179665\n",
            "Iteration 1652, Loss: 0.030797354876995087\n",
            "Iteration 1653, Loss: 0.02893073856830597\n",
            "Iteration 1654, Loss: 0.04517295956611633\n",
            "Iteration 1655, Loss: 0.036588404327631\n",
            "Iteration 1656, Loss: 0.026359032839536667\n",
            "Iteration 1657, Loss: 0.025373367592692375\n",
            "Iteration 1658, Loss: 0.024656400084495544\n",
            "Iteration 1659, Loss: 0.04077491909265518\n",
            "Iteration 1660, Loss: 0.03493990749120712\n",
            "Iteration 1661, Loss: 0.024349873885512352\n",
            "Iteration 1662, Loss: 0.03021932579576969\n",
            "Iteration 1663, Loss: 0.03490380942821503\n",
            "Iteration 1664, Loss: 0.028028205037117004\n",
            "Iteration 1665, Loss: 0.0272456593811512\n",
            "Iteration 1666, Loss: 0.03108283318579197\n",
            "Iteration 1667, Loss: 0.02646794356405735\n",
            "Iteration 1668, Loss: 0.03566640987992287\n",
            "Iteration 1669, Loss: 0.04632626846432686\n",
            "Iteration 1670, Loss: 0.04511610418558121\n",
            "Iteration 1671, Loss: 0.026676587760448456\n",
            "Iteration 1672, Loss: 0.031160185113549232\n",
            "Iteration 1673, Loss: 0.026404233649373055\n",
            "Iteration 1674, Loss: 0.03888995572924614\n",
            "Iteration 1675, Loss: 0.034940898418426514\n",
            "Iteration 1676, Loss: 0.044600289314985275\n",
            "Iteration 1677, Loss: 0.03184528648853302\n",
            "Iteration 1678, Loss: 0.03730269521474838\n",
            "Iteration 1679, Loss: 0.03356137499213219\n",
            "Iteration 1680, Loss: 0.025967219844460487\n",
            "Iteration 1681, Loss: 0.0222749225795269\n",
            "Iteration 1682, Loss: 0.03966176509857178\n",
            "Iteration 1683, Loss: 0.026288343593478203\n",
            "Iteration 1684, Loss: 0.0405704528093338\n",
            "Iteration 1685, Loss: 0.019042298197746277\n",
            "Iteration 1686, Loss: 0.03182261437177658\n",
            "Iteration 1687, Loss: 0.027728648856282234\n",
            "Iteration 1688, Loss: 0.0297883078455925\n",
            "Iteration 1689, Loss: 0.025537976995110512\n",
            "Iteration 1690, Loss: 0.029941683635115623\n",
            "Iteration 1691, Loss: 0.0224290918558836\n",
            "Iteration 1692, Loss: 0.02839542366564274\n",
            "Iteration 1693, Loss: 0.03352298215031624\n",
            "Iteration 1694, Loss: 0.027190349996089935\n",
            "Iteration 1695, Loss: 0.0280170701444149\n",
            "Iteration 1696, Loss: 0.04883651062846184\n",
            "Iteration 1697, Loss: 0.03344552218914032\n",
            "Iteration 1698, Loss: 0.03746550530195236\n",
            "Iteration 1699, Loss: 0.03067675232887268\n",
            "Iteration 1700, Loss: 0.03157385066151619\n",
            "Iteration 1701, Loss: 0.02520100399851799\n",
            "Iteration 1702, Loss: 0.03616183251142502\n",
            "Iteration 1703, Loss: 0.024119315668940544\n",
            "Iteration 1704, Loss: 0.05501216650009155\n",
            "Iteration 1705, Loss: 0.026564646512269974\n",
            "Iteration 1706, Loss: 0.02714003622531891\n",
            "Iteration 1707, Loss: 0.0318475104868412\n",
            "Iteration 1708, Loss: 0.03100922703742981\n",
            "Iteration 1709, Loss: 0.045741915702819824\n",
            "Iteration 1710, Loss: 0.03106948919594288\n",
            "Iteration 1711, Loss: 0.02894553542137146\n",
            "Iteration 1712, Loss: 0.029101340100169182\n",
            "Iteration 1713, Loss: 0.03290823474526405\n",
            "Iteration 1714, Loss: 0.031364597380161285\n",
            "Iteration 1715, Loss: 0.045976780354976654\n",
            "Iteration 1716, Loss: 0.03135405480861664\n",
            "Iteration 1717, Loss: 0.04646996781229973\n",
            "Iteration 1718, Loss: 0.02794678509235382\n",
            "Iteration 1719, Loss: 0.03180256858468056\n",
            "Iteration 1720, Loss: 0.029317308217287064\n",
            "Iteration 1721, Loss: 0.03101106360554695\n",
            "Iteration 1722, Loss: 0.03608785942196846\n",
            "Iteration 1723, Loss: 0.020240534096956253\n",
            "Iteration 1724, Loss: 0.02492494136095047\n",
            "Iteration 1725, Loss: 0.03693286329507828\n",
            "Iteration 1726, Loss: 0.03024216927587986\n",
            "Iteration 1727, Loss: 0.050704024732112885\n",
            "Iteration 1728, Loss: 0.028554802760481834\n",
            "Iteration 1729, Loss: 0.035131748765707016\n",
            "Iteration 1730, Loss: 0.03083907812833786\n",
            "Iteration 1731, Loss: 0.02733268402516842\n",
            "Iteration 1732, Loss: 0.03294882923364639\n",
            "Iteration 1733, Loss: 0.02885182946920395\n",
            "Iteration 1734, Loss: 0.021182771772146225\n",
            "Iteration 1735, Loss: 0.03937438875436783\n",
            "Iteration 1736, Loss: 0.024555323645472527\n",
            "Iteration 1737, Loss: 0.026041502133011818\n",
            "Iteration 1738, Loss: 0.035311438143253326\n",
            "Iteration 1739, Loss: 0.049796584993600845\n",
            "Iteration 1740, Loss: 0.03368857875466347\n",
            "Iteration 1741, Loss: 0.029695091769099236\n",
            "Iteration 1742, Loss: 0.02992784045636654\n",
            "Iteration 1743, Loss: 0.04090269282460213\n",
            "Iteration 1744, Loss: 0.049770161509513855\n",
            "Iteration 1745, Loss: 0.03617478162050247\n",
            "Iteration 1746, Loss: 0.03522852435708046\n",
            "Iteration 1747, Loss: 0.03640928864479065\n",
            "Iteration 1748, Loss: 0.047689441591501236\n",
            "Iteration 1749, Loss: 0.023683227598667145\n",
            "Iteration 1750, Loss: 0.04072379320859909\n",
            "Iteration 1751, Loss: 0.03916401043534279\n",
            "Iteration 1752, Loss: 0.02017252892255783\n",
            "Iteration 1753, Loss: 0.04380890354514122\n",
            "Iteration 1754, Loss: 0.022615499794483185\n",
            "Iteration 1755, Loss: 0.04072174057364464\n",
            "Iteration 1756, Loss: 0.04592658206820488\n",
            "Iteration 1757, Loss: 0.021816717460751534\n",
            "Iteration 1758, Loss: 0.020837854593992233\n",
            "Iteration 1759, Loss: 0.03384935110807419\n",
            "Iteration 1760, Loss: 0.029497941955924034\n",
            "Iteration 1761, Loss: 0.03736308962106705\n",
            "Iteration 1762, Loss: 0.03593503311276436\n",
            "Iteration 1763, Loss: 0.02721458487212658\n",
            "Iteration 1764, Loss: 0.03137984871864319\n",
            "Iteration 1765, Loss: 0.023328673094511032\n",
            "Iteration 1766, Loss: 0.026936912909150124\n",
            "Iteration 1767, Loss: 0.028457798063755035\n",
            "Iteration 1768, Loss: 0.028668075799942017\n",
            "Iteration 1769, Loss: 0.0410732738673687\n",
            "Iteration 1770, Loss: 0.04619821161031723\n",
            "Iteration 1771, Loss: 0.03448117896914482\n",
            "Iteration 1772, Loss: 0.03315535932779312\n",
            "Iteration 1773, Loss: 0.02309419773519039\n",
            "Iteration 1774, Loss: 0.027277717366814613\n",
            "Iteration 1775, Loss: 0.030870916321873665\n",
            "Iteration 1776, Loss: 0.02712228335440159\n",
            "Iteration 1777, Loss: 0.02327466756105423\n",
            "Iteration 1778, Loss: 0.0297801923006773\n",
            "Iteration 1779, Loss: 0.03482692688703537\n",
            "Iteration 1780, Loss: 0.023307647556066513\n",
            "Iteration 1781, Loss: 0.027924794703722\n",
            "Iteration 1782, Loss: 0.031270433217287064\n",
            "Iteration 1783, Loss: 0.021444611251354218\n",
            "Iteration 1784, Loss: 0.028941920027136803\n",
            "Iteration 1785, Loss: 0.03071155957877636\n",
            "Iteration 1786, Loss: 0.02873585931956768\n",
            "Iteration 1787, Loss: 0.022074397653341293\n",
            "Iteration 1788, Loss: 0.04102711006999016\n",
            "Iteration 1789, Loss: 0.023443171754479408\n",
            "Iteration 1790, Loss: 0.030368342995643616\n",
            "Iteration 1791, Loss: 0.043668534606695175\n",
            "Iteration 1792, Loss: 0.03343619033694267\n",
            "Iteration 1793, Loss: 0.035683974623680115\n",
            "Iteration 1794, Loss: 0.036548975855112076\n",
            "Iteration 1795, Loss: 0.033190298825502396\n",
            "Iteration 1796, Loss: 0.0395796000957489\n",
            "Iteration 1797, Loss: 0.03248470649123192\n",
            "Iteration 1798, Loss: 0.029618380591273308\n",
            "Iteration 1799, Loss: 0.02340526133775711\n",
            "Iteration 1800, Loss: 0.029807593673467636\n",
            "Iteration 1801, Loss: 0.022509805858135223\n",
            "Iteration 1802, Loss: 0.018727583810687065\n",
            "Iteration 1803, Loss: 0.024743007495999336\n",
            "Iteration 1804, Loss: 0.033198993653059006\n",
            "Iteration 1805, Loss: 0.040531400591135025\n",
            "Iteration 1806, Loss: 0.03847344219684601\n",
            "Iteration 1807, Loss: 0.03451473265886307\n",
            "Iteration 1808, Loss: 0.036215487867593765\n",
            "Iteration 1809, Loss: 0.019784094765782356\n",
            "Iteration 1810, Loss: 0.04489723965525627\n",
            "Iteration 1811, Loss: 0.027660289779305458\n",
            "Iteration 1812, Loss: 0.03697942569851875\n",
            "Iteration 1813, Loss: 0.02767854556441307\n",
            "Iteration 1814, Loss: 0.030375216156244278\n",
            "Iteration 1815, Loss: 0.03316187486052513\n",
            "Iteration 1816, Loss: 0.029748674482107162\n",
            "Iteration 1817, Loss: 0.021688757464289665\n",
            "Iteration 1818, Loss: 0.020604705438017845\n",
            "Iteration 1819, Loss: 0.022986536845564842\n",
            "Iteration 1820, Loss: 0.03027915582060814\n",
            "Iteration 1821, Loss: 0.0245297122746706\n",
            "Iteration 1822, Loss: 0.027808139100670815\n",
            "Iteration 1823, Loss: 0.038741584867239\n",
            "Iteration 1824, Loss: 0.027104180306196213\n",
            "Iteration 1825, Loss: 0.028869377449154854\n",
            "Iteration 1826, Loss: 0.029203571379184723\n",
            "Iteration 1827, Loss: 0.03057260811328888\n",
            "Iteration 1828, Loss: 0.03267743065953255\n",
            "Iteration 1829, Loss: 0.03917758911848068\n",
            "Iteration 1830, Loss: 0.0342099592089653\n",
            "Iteration 1831, Loss: 0.028821012005209923\n",
            "Iteration 1832, Loss: 0.0287936981767416\n",
            "Iteration 1833, Loss: 0.039956189692020416\n",
            "Iteration 1834, Loss: 0.030916526913642883\n",
            "Iteration 1835, Loss: 0.0218990258872509\n",
            "Iteration 1836, Loss: 0.029059384018182755\n",
            "Iteration 1837, Loss: 0.034896932542324066\n",
            "Iteration 1838, Loss: 0.031037120148539543\n",
            "Iteration 1839, Loss: 0.034977465867996216\n",
            "Iteration 1840, Loss: 0.02939978614449501\n",
            "Iteration 1841, Loss: 0.03045840933918953\n",
            "Iteration 1842, Loss: 0.03606712445616722\n",
            "Iteration 1843, Loss: 0.030052676796913147\n",
            "Iteration 1844, Loss: 0.032358959317207336\n",
            "Iteration 1845, Loss: 0.025163684040308\n",
            "Iteration 1846, Loss: 0.030514396727085114\n",
            "Iteration 1847, Loss: 0.025344599038362503\n",
            "Iteration 1848, Loss: 0.023044848814606667\n",
            "Iteration 1849, Loss: 0.022337399423122406\n",
            "Iteration 1850, Loss: 0.025373846292495728\n",
            "Iteration 1851, Loss: 0.036759745329618454\n",
            "Iteration 1852, Loss: 0.02876422367990017\n",
            "Iteration 1853, Loss: 0.028053326532244682\n",
            "Iteration 1854, Loss: 0.01664883829653263\n",
            "Iteration 1855, Loss: 0.02315393276512623\n",
            "Iteration 1856, Loss: 0.03504345938563347\n",
            "Iteration 1857, Loss: 0.03571323677897453\n",
            "Iteration 1858, Loss: 0.03981899842619896\n",
            "Iteration 1859, Loss: 0.03147688880562782\n",
            "Iteration 1860, Loss: 0.029510583728551865\n",
            "Iteration 1861, Loss: 0.04214424267411232\n",
            "Iteration 1862, Loss: 0.03335733339190483\n",
            "Iteration 1863, Loss: 0.03462572395801544\n",
            "Iteration 1864, Loss: 0.037270110100507736\n",
            "Iteration 1865, Loss: 0.028454497456550598\n",
            "Iteration 1866, Loss: 0.03202059492468834\n",
            "Iteration 1867, Loss: 0.03513764962553978\n",
            "Iteration 1868, Loss: 0.030529344454407692\n",
            "Iteration 1869, Loss: 0.031388942152261734\n",
            "Iteration 1870, Loss: 0.0382472388446331\n",
            "Iteration 1871, Loss: 0.034227147698402405\n",
            "Iteration 1872, Loss: 0.021079644560813904\n",
            "Iteration 1873, Loss: 0.045702602714300156\n",
            "Iteration 1874, Loss: 0.02692507393658161\n",
            "Iteration 1875, Loss: 0.02457326650619507\n",
            "Iteration 1876, Loss: 0.04577212408185005\n",
            "Iteration 1877, Loss: 0.02047535963356495\n",
            "Iteration 1878, Loss: 0.03298748657107353\n",
            "Iteration 1879, Loss: 0.03006633371114731\n",
            "Iteration 1880, Loss: 0.026444513350725174\n",
            "Iteration 1881, Loss: 0.03043690137565136\n",
            "Iteration 1882, Loss: 0.022569559514522552\n",
            "Iteration 1883, Loss: 0.017104092985391617\n",
            "Iteration 1884, Loss: 0.03337136283516884\n",
            "Iteration 1885, Loss: 0.028911100700497627\n",
            "Iteration 1886, Loss: 0.02991688810288906\n",
            "Iteration 1887, Loss: 0.016543563455343246\n",
            "Iteration 1888, Loss: 0.04578687995672226\n",
            "Iteration 1889, Loss: 0.030300939455628395\n",
            "Iteration 1890, Loss: 0.025905337184667587\n",
            "Iteration 1891, Loss: 0.023450741544365883\n",
            "Iteration 1892, Loss: 0.020683472976088524\n",
            "Iteration 1893, Loss: 0.029286658391356468\n",
            "Iteration 1894, Loss: 0.02816164307296276\n",
            "Iteration 1895, Loss: 0.029283631592988968\n",
            "Iteration 1896, Loss: 0.026678942143917084\n",
            "Iteration 1897, Loss: 0.02774396352469921\n",
            "Iteration 1898, Loss: 0.0227956622838974\n",
            "Iteration 1899, Loss: 0.047667574137449265\n",
            "Iteration 1900, Loss: 0.03542362526059151\n",
            "Iteration 1901, Loss: 0.043168626725673676\n",
            "Iteration 1902, Loss: 0.03271244093775749\n",
            "Iteration 1903, Loss: 0.03608055040240288\n",
            "Iteration 1904, Loss: 0.029333658516407013\n",
            "Iteration 1905, Loss: 0.026498951017856598\n",
            "Iteration 1906, Loss: 0.03384922072291374\n",
            "Iteration 1907, Loss: 0.03272341564297676\n",
            "Iteration 1908, Loss: 0.029372168704867363\n",
            "Iteration 1909, Loss: 0.030305558815598488\n",
            "Iteration 1910, Loss: 0.027964787557721138\n",
            "Iteration 1911, Loss: 0.023473359644412994\n",
            "Iteration 1912, Loss: 0.039101630449295044\n",
            "Iteration 1913, Loss: 0.033623937517404556\n",
            "Iteration 1914, Loss: 0.04084174335002899\n",
            "Iteration 1915, Loss: 0.028876041993498802\n",
            "Iteration 1916, Loss: 0.024129042401909828\n",
            "Iteration 1917, Loss: 0.03940294310450554\n",
            "Iteration 1918, Loss: 0.03631613403558731\n",
            "Iteration 1919, Loss: 0.03073189966380596\n",
            "Iteration 1920, Loss: 0.023876529186964035\n",
            "Iteration 1921, Loss: 0.03510948270559311\n",
            "Iteration 1922, Loss: 0.033584170043468475\n",
            "Iteration 1923, Loss: 0.02496815286576748\n",
            "Iteration 1924, Loss: 0.04224556311964989\n",
            "Iteration 1925, Loss: 0.0338914692401886\n",
            "Iteration 1926, Loss: 0.04131613299250603\n",
            "Iteration 1927, Loss: 0.03926950320601463\n",
            "Iteration 1928, Loss: 0.022629426792263985\n",
            "Iteration 1929, Loss: 0.03325478732585907\n",
            "Iteration 1930, Loss: 0.037679050117731094\n",
            "Iteration 1931, Loss: 0.03431803733110428\n",
            "Iteration 1932, Loss: 0.03550790250301361\n",
            "Iteration 1933, Loss: 0.032195672392845154\n",
            "Iteration 1934, Loss: 0.026016928255558014\n",
            "Iteration 1935, Loss: 0.03241321071982384\n",
            "Iteration 1936, Loss: 0.03676944226026535\n",
            "Iteration 1937, Loss: 0.025577865540981293\n",
            "Iteration 1938, Loss: 0.033478591591119766\n",
            "Iteration 1939, Loss: 0.02814757637679577\n",
            "Iteration 1940, Loss: 0.017878307029604912\n",
            "Iteration 1941, Loss: 0.01676560752093792\n",
            "Iteration 1942, Loss: 0.027571646496653557\n",
            "Iteration 1943, Loss: 0.034201860427856445\n",
            "Iteration 1944, Loss: 0.032231610268354416\n",
            "Iteration 1945, Loss: 0.026832344010472298\n",
            "Iteration 1946, Loss: 0.028398066759109497\n",
            "Iteration 1947, Loss: 0.018688982352614403\n",
            "Iteration 1948, Loss: 0.026881413534283638\n",
            "Iteration 1949, Loss: 0.031106585636734962\n",
            "Iteration 1950, Loss: 0.030028002336621284\n",
            "Iteration 1951, Loss: 0.034415919333696365\n",
            "Iteration 1952, Loss: 0.019903074949979782\n",
            "Iteration 1953, Loss: 0.022743485867977142\n",
            "Iteration 1954, Loss: 0.03455781191587448\n",
            "Iteration 1955, Loss: 0.021946491673588753\n",
            "Iteration 1956, Loss: 0.038636915385723114\n",
            "Iteration 1957, Loss: 0.038133248686790466\n",
            "Iteration 1958, Loss: 0.022730600088834763\n",
            "Iteration 1959, Loss: 0.02421691082417965\n",
            "Iteration 1960, Loss: 0.029360393062233925\n",
            "Iteration 1961, Loss: 0.024502601474523544\n",
            "Iteration 1962, Loss: 0.027755968272686005\n",
            "Iteration 1963, Loss: 0.03419053554534912\n",
            "Iteration 1964, Loss: 0.02188713289797306\n",
            "Iteration 1965, Loss: 0.022814534604549408\n",
            "Iteration 1966, Loss: 0.024242233484983444\n",
            "Iteration 1967, Loss: 0.03208551183342934\n",
            "Iteration 1968, Loss: 0.029504001140594482\n",
            "Iteration 1969, Loss: 0.02834162302315235\n",
            "Iteration 1970, Loss: 0.03319635987281799\n",
            "Iteration 1971, Loss: 0.02698405086994171\n",
            "Iteration 1972, Loss: 0.02506338804960251\n",
            "Iteration 1973, Loss: 0.02452232502400875\n",
            "Iteration 1974, Loss: 0.039167195558547974\n",
            "Iteration 1975, Loss: 0.03136441856622696\n",
            "Iteration 1976, Loss: 0.03210010379552841\n",
            "Iteration 1977, Loss: 0.03443680331110954\n",
            "Iteration 1978, Loss: 0.03482508286833763\n",
            "Iteration 1979, Loss: 0.02843700908124447\n",
            "Iteration 1980, Loss: 0.024329736828804016\n",
            "Iteration 1981, Loss: 0.02448994480073452\n",
            "Iteration 1982, Loss: 0.02972320467233658\n",
            "Iteration 1983, Loss: 0.02653363160789013\n",
            "Iteration 1984, Loss: 0.040105294436216354\n",
            "Iteration 1985, Loss: 0.02832632325589657\n",
            "Iteration 1986, Loss: 0.03458578884601593\n",
            "Iteration 1987, Loss: 0.03307374194264412\n",
            "Iteration 1988, Loss: 0.02130040153861046\n",
            "Iteration 1989, Loss: 0.033193208277225494\n",
            "Iteration 1990, Loss: 0.029769444838166237\n",
            "Iteration 1991, Loss: 0.034272972494363785\n",
            "Iteration 1992, Loss: 0.028555840253829956\n",
            "Iteration 1993, Loss: 0.033522509038448334\n",
            "Iteration 1994, Loss: 0.021885380148887634\n",
            "Iteration 1995, Loss: 0.027820315212011337\n",
            "Iteration 1996, Loss: 0.02207469753921032\n",
            "Iteration 1997, Loss: 0.017742561176419258\n",
            "Iteration 1998, Loss: 0.028822118416428566\n",
            "Iteration 1999, Loss: 0.021414296701550484\n",
            "Iteration 2000, Loss: 0.03529014810919762\n",
            "Test Loss: 0.039950452744960785\n",
            "Iteration 2001, Loss: 0.029802877455949783\n",
            "Iteration 2002, Loss: 0.029515421018004417\n",
            "Iteration 2003, Loss: 0.03311263024806976\n",
            "Iteration 2004, Loss: 0.026013512164354324\n",
            "Iteration 2005, Loss: 0.02802465297281742\n",
            "Iteration 2006, Loss: 0.026770083233714104\n",
            "Iteration 2007, Loss: 0.028148697689175606\n",
            "Iteration 2008, Loss: 0.026484200730919838\n",
            "Iteration 2009, Loss: 0.027652017772197723\n",
            "Iteration 2010, Loss: 0.029483137652277946\n",
            "Iteration 2011, Loss: 0.01261734776198864\n",
            "Iteration 2012, Loss: 0.025767091661691666\n",
            "Iteration 2013, Loss: 0.04964013770222664\n",
            "Iteration 2014, Loss: 0.022487526759505272\n",
            "Iteration 2015, Loss: 0.02557697333395481\n",
            "Iteration 2016, Loss: 0.03153347596526146\n",
            "Iteration 2017, Loss: 0.02447989583015442\n",
            "Iteration 2018, Loss: 0.028232892975211143\n",
            "Iteration 2019, Loss: 0.026776600629091263\n",
            "Iteration 2020, Loss: 0.025584423914551735\n",
            "Iteration 2021, Loss: 0.022441253066062927\n",
            "Iteration 2022, Loss: 0.023013729602098465\n",
            "Iteration 2023, Loss: 0.027712972834706306\n",
            "Iteration 2024, Loss: 0.02992996573448181\n",
            "Iteration 2025, Loss: 0.023677421733736992\n",
            "Iteration 2026, Loss: 0.02260151505470276\n",
            "Iteration 2027, Loss: 0.027548998594284058\n",
            "Iteration 2028, Loss: 0.026373011991381645\n",
            "Iteration 2029, Loss: 0.021137213334441185\n",
            "Iteration 2030, Loss: 0.017919400706887245\n",
            "Iteration 2031, Loss: 0.029326196759939194\n",
            "Iteration 2032, Loss: 0.04452706128358841\n",
            "Iteration 2033, Loss: 0.031640663743019104\n",
            "Iteration 2034, Loss: 0.020000847056508064\n",
            "Iteration 2035, Loss: 0.027284400537610054\n",
            "Iteration 2036, Loss: 0.024662451818585396\n",
            "Iteration 2037, Loss: 0.022299885749816895\n",
            "Iteration 2038, Loss: 0.038727790117263794\n",
            "Iteration 2039, Loss: 0.031067728996276855\n",
            "Iteration 2040, Loss: 0.020369501784443855\n",
            "Iteration 2041, Loss: 0.020638281479477882\n",
            "Iteration 2042, Loss: 0.015044636093080044\n",
            "Iteration 2043, Loss: 0.022468237206339836\n",
            "Iteration 2044, Loss: 0.025183429941534996\n",
            "Iteration 2045, Loss: 0.02801148220896721\n",
            "Iteration 2046, Loss: 0.030920034274458885\n",
            "Iteration 2047, Loss: 0.022978441789746284\n",
            "Iteration 2048, Loss: 0.02680370770394802\n",
            "Iteration 2049, Loss: 0.02509576454758644\n",
            "Iteration 2050, Loss: 0.03995439037680626\n",
            "Iteration 2051, Loss: 0.024760328233242035\n",
            "Iteration 2052, Loss: 0.03012022376060486\n",
            "Iteration 2053, Loss: 0.037380315363407135\n",
            "Iteration 2054, Loss: 0.02820064313709736\n",
            "Iteration 2055, Loss: 0.022956250235438347\n",
            "Iteration 2056, Loss: 0.02449595369398594\n",
            "Iteration 2057, Loss: 0.022439852356910706\n",
            "Iteration 2058, Loss: 0.04315439611673355\n",
            "Iteration 2059, Loss: 0.024630434811115265\n",
            "Iteration 2060, Loss: 0.03909352049231529\n",
            "Iteration 2061, Loss: 0.01649405248463154\n",
            "Iteration 2062, Loss: 0.03416339308023453\n",
            "Iteration 2063, Loss: 0.0378415547311306\n",
            "Iteration 2064, Loss: 0.02494550123810768\n",
            "Iteration 2065, Loss: 0.024792121723294258\n",
            "Iteration 2066, Loss: 0.03780880570411682\n",
            "Iteration 2067, Loss: 0.0369415320456028\n",
            "Iteration 2068, Loss: 0.03435008227825165\n",
            "Iteration 2069, Loss: 0.03622264042496681\n",
            "Iteration 2070, Loss: 0.029270252212882042\n",
            "Iteration 2071, Loss: 0.04958714544773102\n",
            "Iteration 2072, Loss: 0.02425735630095005\n",
            "Iteration 2073, Loss: 0.018712909892201424\n",
            "Iteration 2074, Loss: 0.03244514763355255\n",
            "Iteration 2075, Loss: 0.03644693270325661\n",
            "Iteration 2076, Loss: 0.03567614406347275\n",
            "Iteration 2077, Loss: 0.02492387220263481\n",
            "Iteration 2078, Loss: 0.02538045309484005\n",
            "Iteration 2079, Loss: 0.030589977279305458\n",
            "Iteration 2080, Loss: 0.027598947286605835\n",
            "Iteration 2081, Loss: 0.02324827015399933\n",
            "Iteration 2082, Loss: 0.025866927579045296\n",
            "Iteration 2083, Loss: 0.04079090803861618\n",
            "Iteration 2084, Loss: 0.020321859046816826\n",
            "Iteration 2085, Loss: 0.02773115411400795\n",
            "Iteration 2086, Loss: 0.04384428635239601\n",
            "Iteration 2087, Loss: 0.03205317631363869\n",
            "Iteration 2088, Loss: 0.026126904413104057\n",
            "Iteration 2089, Loss: 0.04452424496412277\n",
            "Iteration 2090, Loss: 0.026256348937749863\n",
            "Iteration 2091, Loss: 0.03649801015853882\n",
            "Iteration 2092, Loss: 0.02605133317410946\n",
            "Iteration 2093, Loss: 0.031630851328372955\n",
            "Iteration 2094, Loss: 0.036951445043087006\n",
            "Iteration 2095, Loss: 0.030490370467305183\n",
            "Iteration 2096, Loss: 0.022737130522727966\n",
            "Iteration 2097, Loss: 0.021764587610960007\n",
            "Iteration 2098, Loss: 0.032318077981472015\n",
            "Iteration 2099, Loss: 0.0226639024913311\n",
            "Iteration 2100, Loss: 0.03948667272925377\n",
            "Iteration 2101, Loss: 0.030231546610593796\n",
            "Iteration 2102, Loss: 0.028940249234437943\n",
            "Iteration 2103, Loss: 0.02467719092965126\n",
            "Iteration 2104, Loss: 0.04155934602022171\n",
            "Iteration 2105, Loss: 0.03226517140865326\n",
            "Iteration 2106, Loss: 0.020362624898552895\n",
            "Iteration 2107, Loss: 0.02137094736099243\n",
            "Iteration 2108, Loss: 0.030347976833581924\n",
            "Iteration 2109, Loss: 0.03296869248151779\n",
            "Iteration 2110, Loss: 0.023853037506341934\n",
            "Iteration 2111, Loss: 0.03782882168889046\n",
            "Iteration 2112, Loss: 0.02444634772837162\n",
            "Iteration 2113, Loss: 0.027667732909321785\n",
            "Iteration 2114, Loss: 0.025176728144288063\n",
            "Iteration 2115, Loss: 0.037399038672447205\n",
            "Iteration 2116, Loss: 0.028226474300026894\n",
            "Iteration 2117, Loss: 0.02979249134659767\n",
            "Iteration 2118, Loss: 0.03217959403991699\n",
            "Iteration 2119, Loss: 0.032512735575437546\n",
            "Iteration 2120, Loss: 0.02602601982653141\n",
            "Iteration 2121, Loss: 0.02520512230694294\n",
            "Iteration 2122, Loss: 0.028070516884326935\n",
            "Iteration 2123, Loss: 0.02090204693377018\n",
            "Iteration 2124, Loss: 0.022610798478126526\n",
            "Iteration 2125, Loss: 0.03449796885251999\n",
            "Iteration 2126, Loss: 0.05135459452867508\n",
            "Iteration 2127, Loss: 0.026705417782068253\n",
            "Iteration 2128, Loss: 0.03525238484144211\n",
            "Iteration 2129, Loss: 0.026285242289304733\n",
            "Iteration 2130, Loss: 0.03292065113782883\n",
            "Iteration 2131, Loss: 0.030964327976107597\n",
            "Iteration 2132, Loss: 0.02420775033533573\n",
            "Iteration 2133, Loss: 0.03404613584280014\n",
            "Iteration 2134, Loss: 0.022449787706136703\n",
            "Iteration 2135, Loss: 0.029910167679190636\n",
            "Iteration 2136, Loss: 0.02083124965429306\n",
            "Iteration 2137, Loss: 0.026060163974761963\n",
            "Iteration 2138, Loss: 0.031161045655608177\n",
            "Iteration 2139, Loss: 0.022464264184236526\n",
            "Iteration 2140, Loss: 0.021878071129322052\n",
            "Iteration 2141, Loss: 0.029096269980072975\n",
            "Iteration 2142, Loss: 0.026605214923620224\n",
            "Iteration 2143, Loss: 0.03752117604017258\n",
            "Iteration 2144, Loss: 0.02772504836320877\n",
            "Iteration 2145, Loss: 0.029919978231191635\n",
            "Iteration 2146, Loss: 0.027321603149175644\n",
            "Iteration 2147, Loss: 0.03503884747624397\n",
            "Iteration 2148, Loss: 0.028383374214172363\n",
            "Iteration 2149, Loss: 0.024318384006619453\n",
            "Iteration 2150, Loss: 0.027440231293439865\n",
            "Iteration 2151, Loss: 0.028249571099877357\n",
            "Iteration 2152, Loss: 0.026380330324172974\n",
            "Iteration 2153, Loss: 0.032144881784915924\n",
            "Iteration 2154, Loss: 0.02809433452785015\n",
            "Iteration 2155, Loss: 0.0354272834956646\n",
            "Iteration 2156, Loss: 0.021984687075018883\n",
            "Iteration 2157, Loss: 0.03527585044503212\n",
            "Iteration 2158, Loss: 0.029275428503751755\n",
            "Iteration 2159, Loss: 0.029383042827248573\n",
            "Iteration 2160, Loss: 0.02824951522052288\n",
            "Iteration 2161, Loss: 0.02468908578157425\n",
            "Iteration 2162, Loss: 0.035917628556489944\n",
            "Iteration 2163, Loss: 0.0254011582583189\n",
            "Iteration 2164, Loss: 0.029804382473230362\n",
            "Iteration 2165, Loss: 0.04322834312915802\n",
            "Iteration 2166, Loss: 0.021761776879429817\n",
            "Iteration 2167, Loss: 0.026707125827670097\n",
            "Iteration 2168, Loss: 0.02566944807767868\n",
            "Iteration 2169, Loss: 0.03335266932845116\n",
            "Iteration 2170, Loss: 0.022303469479084015\n",
            "Iteration 2171, Loss: 0.028573665767908096\n",
            "Iteration 2172, Loss: 0.02214786410331726\n",
            "Iteration 2173, Loss: 0.02826162800192833\n",
            "Iteration 2174, Loss: 0.024738069623708725\n",
            "Iteration 2175, Loss: 0.01614793948829174\n",
            "Iteration 2176, Loss: 0.02363431267440319\n",
            "Iteration 2177, Loss: 0.028541963547468185\n",
            "Iteration 2178, Loss: 0.030182067304849625\n",
            "Iteration 2179, Loss: 0.026784203946590424\n",
            "Iteration 2180, Loss: 0.030581125989556313\n",
            "Iteration 2181, Loss: 0.02004549838602543\n",
            "Iteration 2182, Loss: 0.02295975759625435\n",
            "Iteration 2183, Loss: 0.023280100896954536\n",
            "Iteration 2184, Loss: 0.02470296062529087\n",
            "Iteration 2185, Loss: 0.015992889180779457\n",
            "Iteration 2186, Loss: 0.032426703721284866\n",
            "Iteration 2187, Loss: 0.036332547664642334\n",
            "Iteration 2188, Loss: 0.02139653079211712\n",
            "Iteration 2189, Loss: 0.028698043897747993\n",
            "Iteration 2190, Loss: 0.031408846378326416\n",
            "Iteration 2191, Loss: 0.0252582598477602\n",
            "Iteration 2192, Loss: 0.029122086241841316\n",
            "Iteration 2193, Loss: 0.03504549339413643\n",
            "Iteration 2194, Loss: 0.03447037562727928\n",
            "Iteration 2195, Loss: 0.02133062854409218\n",
            "Iteration 2196, Loss: 0.02198358252644539\n",
            "Iteration 2197, Loss: 0.025704270228743553\n",
            "Iteration 2198, Loss: 0.03312386944890022\n",
            "Iteration 2199, Loss: 0.02962663024663925\n",
            "Iteration 2200, Loss: 0.029897945001721382\n",
            "Iteration 2201, Loss: 0.025407951325178146\n",
            "Iteration 2202, Loss: 0.02864704094827175\n",
            "Iteration 2203, Loss: 0.03594478592276573\n",
            "Iteration 2204, Loss: 0.02488582767546177\n",
            "Iteration 2205, Loss: 0.04958965629339218\n",
            "Iteration 2206, Loss: 0.024828793480992317\n",
            "Iteration 2207, Loss: 0.022997327148914337\n",
            "Iteration 2208, Loss: 0.024667445570230484\n",
            "Iteration 2209, Loss: 0.023651432245969772\n",
            "Iteration 2210, Loss: 0.0359346829354763\n",
            "Iteration 2211, Loss: 0.023864930495619774\n",
            "Iteration 2212, Loss: 0.02751872129738331\n",
            "Iteration 2213, Loss: 0.038635049015283585\n",
            "Iteration 2214, Loss: 0.03099459782242775\n",
            "Iteration 2215, Loss: 0.022610055282711983\n",
            "Iteration 2216, Loss: 0.027462175115942955\n",
            "Iteration 2217, Loss: 0.02068166807293892\n",
            "Iteration 2218, Loss: 0.022763343527913094\n",
            "Iteration 2219, Loss: 0.027186594903469086\n",
            "Iteration 2220, Loss: 0.031189333647489548\n",
            "Iteration 2221, Loss: 0.025259962305426598\n",
            "Iteration 2222, Loss: 0.028174981474876404\n",
            "Iteration 2223, Loss: 0.027418991550803185\n",
            "Iteration 2224, Loss: 0.0349213182926178\n",
            "Iteration 2225, Loss: 0.018560508266091347\n",
            "Iteration 2226, Loss: 0.02251587063074112\n",
            "Iteration 2227, Loss: 0.030430089682340622\n",
            "Iteration 2228, Loss: 0.03803952410817146\n",
            "Iteration 2229, Loss: 0.027384137734770775\n",
            "Iteration 2230, Loss: 0.027796605601906776\n",
            "Iteration 2231, Loss: 0.029793770983815193\n",
            "Iteration 2232, Loss: 0.03134694695472717\n",
            "Iteration 2233, Loss: 0.02801535464823246\n",
            "Iteration 2234, Loss: 0.01515705231577158\n",
            "Iteration 2235, Loss: 0.02245044894516468\n",
            "Iteration 2236, Loss: 0.03189444914460182\n",
            "Iteration 2237, Loss: 0.03211376443505287\n",
            "Iteration 2238, Loss: 0.020747825503349304\n",
            "Iteration 2239, Loss: 0.02895432710647583\n",
            "Iteration 2240, Loss: 0.029071833938360214\n",
            "Iteration 2241, Loss: 0.024335281923413277\n",
            "Iteration 2242, Loss: 0.028741484507918358\n",
            "Iteration 2243, Loss: 0.028580036014318466\n",
            "Iteration 2244, Loss: 0.019318200647830963\n",
            "Iteration 2245, Loss: 0.027741804718971252\n",
            "Iteration 2246, Loss: 0.0306859128177166\n",
            "Iteration 2247, Loss: 0.025445185601711273\n",
            "Iteration 2248, Loss: 0.028324352577328682\n",
            "Iteration 2249, Loss: 0.022110428661108017\n",
            "Iteration 2250, Loss: 0.022920457646250725\n",
            "Iteration 2251, Loss: 0.020698394626379013\n",
            "Iteration 2252, Loss: 0.03328996151685715\n",
            "Iteration 2253, Loss: 0.026955198496580124\n",
            "Iteration 2254, Loss: 0.034344758838415146\n",
            "Iteration 2255, Loss: 0.030291739851236343\n",
            "Iteration 2256, Loss: 0.030151424929499626\n",
            "Iteration 2257, Loss: 0.02479635924100876\n",
            "Iteration 2258, Loss: 0.02750585600733757\n",
            "Iteration 2259, Loss: 0.031299762427806854\n",
            "Iteration 2260, Loss: 0.015267705544829369\n",
            "Iteration 2261, Loss: 0.020148513838648796\n",
            "Iteration 2262, Loss: 0.03393426910042763\n",
            "Iteration 2263, Loss: 0.01599104143679142\n",
            "Iteration 2264, Loss: 0.03061414137482643\n",
            "Iteration 2265, Loss: 0.02718154340982437\n",
            "Iteration 2266, Loss: 0.02864951267838478\n",
            "Iteration 2267, Loss: 0.029588427394628525\n",
            "Iteration 2268, Loss: 0.022948473691940308\n",
            "Iteration 2269, Loss: 0.02267201989889145\n",
            "Iteration 2270, Loss: 0.020889557898044586\n",
            "Iteration 2271, Loss: 0.02836005575954914\n",
            "Iteration 2272, Loss: 0.025423599407076836\n",
            "Iteration 2273, Loss: 0.02495092898607254\n",
            "Iteration 2274, Loss: 0.02643771842122078\n",
            "Iteration 2275, Loss: 0.027061687782406807\n",
            "Iteration 2276, Loss: 0.027099182829260826\n",
            "Iteration 2277, Loss: 0.01754325069487095\n",
            "Iteration 2278, Loss: 0.021987952291965485\n",
            "Iteration 2279, Loss: 0.022554954513907433\n",
            "Iteration 2280, Loss: 0.02344552055001259\n",
            "Iteration 2281, Loss: 0.02408563904464245\n",
            "Iteration 2282, Loss: 0.02666899934411049\n",
            "Iteration 2283, Loss: 0.019917361438274384\n",
            "Iteration 2284, Loss: 0.023504363372921944\n",
            "Iteration 2285, Loss: 0.03353375196456909\n",
            "Iteration 2286, Loss: 0.03366243466734886\n",
            "Iteration 2287, Loss: 0.024460695683956146\n",
            "Iteration 2288, Loss: 0.021598832681775093\n",
            "Iteration 2289, Loss: 0.032160889357328415\n",
            "Iteration 2290, Loss: 0.02207878790795803\n",
            "Iteration 2291, Loss: 0.025952456519007683\n",
            "Iteration 2292, Loss: 0.024215538054704666\n",
            "Iteration 2293, Loss: 0.021281398832798004\n",
            "Iteration 2294, Loss: 0.03508831188082695\n",
            "Iteration 2295, Loss: 0.0237380713224411\n",
            "Iteration 2296, Loss: 0.0212485920637846\n",
            "Iteration 2297, Loss: 0.04191788658499718\n",
            "Iteration 2298, Loss: 0.0365758091211319\n",
            "Iteration 2299, Loss: 0.026965290307998657\n",
            "Iteration 2300, Loss: 0.028794853016734123\n",
            "Iteration 2301, Loss: 0.024669326841831207\n",
            "Iteration 2302, Loss: 0.02528567984700203\n",
            "Iteration 2303, Loss: 0.022662241011857986\n",
            "Iteration 2304, Loss: 0.025766128674149513\n",
            "Iteration 2305, Loss: 0.02739551290869713\n",
            "Iteration 2306, Loss: 0.021241309121251106\n",
            "Iteration 2307, Loss: 0.020429283380508423\n",
            "Iteration 2308, Loss: 0.019653700292110443\n",
            "Iteration 2309, Loss: 0.026777144521474838\n",
            "Iteration 2310, Loss: 0.026230398565530777\n",
            "Iteration 2311, Loss: 0.021827159449458122\n",
            "Iteration 2312, Loss: 0.03534049913287163\n",
            "Iteration 2313, Loss: 0.023111360147595406\n",
            "Iteration 2314, Loss: 0.024845004081726074\n",
            "Iteration 2315, Loss: 0.0251593180000782\n",
            "Iteration 2316, Loss: 0.02259973995387554\n",
            "Iteration 2317, Loss: 0.03132996708154678\n",
            "Iteration 2318, Loss: 0.027573781087994576\n",
            "Iteration 2319, Loss: 0.022749412804841995\n",
            "Iteration 2320, Loss: 0.019877983257174492\n",
            "Iteration 2321, Loss: 0.017582319676876068\n",
            "Iteration 2322, Loss: 0.015920326113700867\n",
            "Iteration 2323, Loss: 0.023927271366119385\n",
            "Iteration 2324, Loss: 0.0378531813621521\n",
            "Iteration 2325, Loss: 0.03568649664521217\n",
            "Iteration 2326, Loss: 0.03364022821187973\n",
            "Iteration 2327, Loss: 0.04001845046877861\n",
            "Iteration 2328, Loss: 0.028222069144248962\n",
            "Iteration 2329, Loss: 0.027678385376930237\n",
            "Iteration 2330, Loss: 0.02771075628697872\n",
            "Iteration 2331, Loss: 0.024804657325148582\n",
            "Iteration 2332, Loss: 0.019865084439516068\n",
            "Iteration 2333, Loss: 0.031038206070661545\n",
            "Iteration 2334, Loss: 0.02436564676463604\n",
            "Iteration 2335, Loss: 0.01477912999689579\n",
            "Iteration 2336, Loss: 0.032618939876556396\n",
            "Iteration 2337, Loss: 0.029224596917629242\n",
            "Iteration 2338, Loss: 0.024251313880085945\n",
            "Iteration 2339, Loss: 0.0395042710006237\n",
            "Iteration 2340, Loss: 0.030234307050704956\n",
            "Iteration 2341, Loss: 0.022291002795100212\n",
            "Iteration 2342, Loss: 0.02729973755776882\n",
            "Iteration 2343, Loss: 0.021239954978227615\n",
            "Iteration 2344, Loss: 0.036790285259485245\n",
            "Iteration 2345, Loss: 0.030134737491607666\n",
            "Iteration 2346, Loss: 0.019616374745965004\n",
            "Iteration 2347, Loss: 0.0208186823874712\n",
            "Iteration 2348, Loss: 0.030109699815511703\n",
            "Iteration 2349, Loss: 0.022310158237814903\n",
            "Iteration 2350, Loss: 0.025530682876706123\n",
            "Iteration 2351, Loss: 0.030571969226002693\n",
            "Iteration 2352, Loss: 0.042802680283784866\n",
            "Iteration 2353, Loss: 0.02230195142328739\n",
            "Iteration 2354, Loss: 0.03256189823150635\n",
            "Iteration 2355, Loss: 0.027789384126663208\n",
            "Iteration 2356, Loss: 0.02553962543606758\n",
            "Iteration 2357, Loss: 0.028676630929112434\n",
            "Iteration 2358, Loss: 0.03877580165863037\n",
            "Iteration 2359, Loss: 0.03224993497133255\n",
            "Iteration 2360, Loss: 0.026024462655186653\n",
            "Iteration 2361, Loss: 0.026722293347120285\n",
            "Iteration 2362, Loss: 0.02609691396355629\n",
            "Iteration 2363, Loss: 0.019735902547836304\n",
            "Iteration 2364, Loss: 0.024922149255871773\n",
            "Iteration 2365, Loss: 0.02714940346777439\n",
            "Iteration 2366, Loss: 0.02465570904314518\n",
            "Iteration 2367, Loss: 0.018260274082422256\n",
            "Iteration 2368, Loss: 0.022206123918294907\n",
            "Iteration 2369, Loss: 0.019334014505147934\n",
            "Iteration 2370, Loss: 0.034667547792196274\n",
            "Iteration 2371, Loss: 0.031906161457300186\n",
            "Iteration 2372, Loss: 0.023476779460906982\n",
            "Iteration 2373, Loss: 0.04080408066511154\n",
            "Iteration 2374, Loss: 0.024677010253071785\n",
            "Iteration 2375, Loss: 0.02425309456884861\n",
            "Iteration 2376, Loss: 0.02323225699365139\n",
            "Iteration 2377, Loss: 0.027506640180945396\n",
            "Iteration 2378, Loss: 0.01865517906844616\n",
            "Iteration 2379, Loss: 0.027462303638458252\n",
            "Iteration 2380, Loss: 0.02041422761976719\n",
            "Iteration 2381, Loss: 0.02865700051188469\n",
            "Iteration 2382, Loss: 0.03197126090526581\n",
            "Iteration 2383, Loss: 0.027264276519417763\n",
            "Iteration 2384, Loss: 0.024736374616622925\n",
            "Iteration 2385, Loss: 0.020662812516093254\n",
            "Iteration 2386, Loss: 0.03194764629006386\n",
            "Iteration 2387, Loss: 0.02385793626308441\n",
            "Iteration 2388, Loss: 0.01954147219657898\n",
            "Iteration 2389, Loss: 0.03228812664747238\n",
            "Iteration 2390, Loss: 0.014539877884089947\n",
            "Iteration 2391, Loss: 0.026476453989744186\n",
            "Iteration 2392, Loss: 0.028802484273910522\n",
            "Iteration 2393, Loss: 0.03160150721669197\n",
            "Iteration 2394, Loss: 0.03166710212826729\n",
            "Iteration 2395, Loss: 0.03313972428441048\n",
            "Iteration 2396, Loss: 0.020995711907744408\n",
            "Iteration 2397, Loss: 0.02138853445649147\n",
            "Iteration 2398, Loss: 0.027110371738672256\n",
            "Iteration 2399, Loss: 0.03360823541879654\n",
            "Iteration 2400, Loss: 0.018888277933001518\n",
            "Iteration 2401, Loss: 0.027477353811264038\n",
            "Iteration 2402, Loss: 0.029059674590826035\n",
            "Iteration 2403, Loss: 0.023667722940444946\n",
            "Iteration 2404, Loss: 0.01636004075407982\n",
            "Iteration 2405, Loss: 0.0294348057359457\n",
            "Iteration 2406, Loss: 0.02844798006117344\n",
            "Iteration 2407, Loss: 0.0236347708851099\n",
            "Iteration 2408, Loss: 0.020153885707259178\n",
            "Iteration 2409, Loss: 0.03489946201443672\n",
            "Iteration 2410, Loss: 0.035102736204862595\n",
            "Iteration 2411, Loss: 0.032736796885728836\n",
            "Iteration 2412, Loss: 0.022112198173999786\n",
            "Iteration 2413, Loss: 0.03135025501251221\n",
            "Iteration 2414, Loss: 0.028564009815454483\n",
            "Iteration 2415, Loss: 0.023379437625408173\n",
            "Iteration 2416, Loss: 0.026068193838000298\n",
            "Iteration 2417, Loss: 0.024309948086738586\n",
            "Iteration 2418, Loss: 0.017718004062771797\n",
            "Iteration 2419, Loss: 0.02527605928480625\n",
            "Iteration 2420, Loss: 0.03084188513457775\n",
            "Iteration 2421, Loss: 0.020881975069642067\n",
            "Iteration 2422, Loss: 0.014592764899134636\n",
            "Iteration 2423, Loss: 0.02326992154121399\n",
            "Iteration 2424, Loss: 0.020078454166650772\n",
            "Iteration 2425, Loss: 0.025256259366869926\n",
            "Iteration 2426, Loss: 0.027244362980127335\n",
            "Iteration 2427, Loss: 0.017008842900395393\n",
            "Iteration 2428, Loss: 0.03674031421542168\n",
            "Iteration 2429, Loss: 0.018844129517674446\n",
            "Iteration 2430, Loss: 0.017930185422301292\n",
            "Iteration 2431, Loss: 0.03066248446702957\n",
            "Iteration 2432, Loss: 0.03415707126259804\n",
            "Iteration 2433, Loss: 0.028963932767510414\n",
            "Iteration 2434, Loss: 0.029462184756994247\n",
            "Iteration 2435, Loss: 0.02293182536959648\n",
            "Iteration 2436, Loss: 0.04014620557427406\n",
            "Iteration 2437, Loss: 0.042017657309770584\n",
            "Iteration 2438, Loss: 0.024478871375322342\n",
            "Iteration 2439, Loss: 0.02812383882701397\n",
            "Iteration 2440, Loss: 0.030532849952578545\n",
            "Iteration 2441, Loss: 0.01869306154549122\n",
            "Iteration 2442, Loss: 0.0209378469735384\n",
            "Iteration 2443, Loss: 0.029055843129754066\n",
            "Iteration 2444, Loss: 0.02517629601061344\n",
            "Iteration 2445, Loss: 0.01753765158355236\n",
            "Iteration 2446, Loss: 0.016540413722395897\n",
            "Iteration 2447, Loss: 0.041325654834508896\n",
            "Iteration 2448, Loss: 0.025663072243332863\n",
            "Iteration 2449, Loss: 0.029081452637910843\n",
            "Iteration 2450, Loss: 0.025565285235643387\n",
            "Iteration 2451, Loss: 0.02429131232202053\n",
            "Iteration 2452, Loss: 0.025346269831061363\n",
            "Iteration 2453, Loss: 0.017524514347314835\n",
            "Iteration 2454, Loss: 0.02401508204638958\n",
            "Iteration 2455, Loss: 0.011339318007230759\n",
            "Iteration 2456, Loss: 0.015760188922286034\n",
            "Iteration 2457, Loss: 0.02748396433889866\n",
            "Iteration 2458, Loss: 0.01630251295864582\n",
            "Iteration 2459, Loss: 0.024846689775586128\n",
            "Iteration 2460, Loss: 0.022619053721427917\n",
            "Iteration 2461, Loss: 0.019354039803147316\n",
            "Iteration 2462, Loss: 0.018309881910681725\n",
            "Iteration 2463, Loss: 0.02423904277384281\n",
            "Iteration 2464, Loss: 0.02118336223065853\n",
            "Iteration 2465, Loss: 0.031531114131212234\n",
            "Iteration 2466, Loss: 0.020895369350910187\n",
            "Iteration 2467, Loss: 0.026021050289273262\n",
            "Iteration 2468, Loss: 0.03334770351648331\n",
            "Iteration 2469, Loss: 0.023061346262693405\n",
            "Iteration 2470, Loss: 0.026406344026327133\n",
            "Iteration 2471, Loss: 0.026672618463635445\n",
            "Iteration 2472, Loss: 0.027824869379401207\n",
            "Iteration 2473, Loss: 0.015495245344936848\n",
            "Iteration 2474, Loss: 0.040679920464754105\n",
            "Iteration 2475, Loss: 0.03254760801792145\n",
            "Iteration 2476, Loss: 0.022637218236923218\n",
            "Iteration 2477, Loss: 0.020328253507614136\n",
            "Iteration 2478, Loss: 0.028568757697939873\n",
            "Iteration 2479, Loss: 0.03107719123363495\n",
            "Iteration 2480, Loss: 0.019740469753742218\n",
            "Iteration 2481, Loss: 0.03284545615315437\n",
            "Iteration 2482, Loss: 0.03127806633710861\n",
            "Iteration 2483, Loss: 0.020962050184607506\n",
            "Iteration 2484, Loss: 0.027593711391091347\n",
            "Iteration 2485, Loss: 0.020732326433062553\n",
            "Iteration 2486, Loss: 0.0313846729695797\n",
            "Iteration 2487, Loss: 0.020016822963953018\n",
            "Iteration 2488, Loss: 0.034863900393247604\n",
            "Iteration 2489, Loss: 0.023380329832434654\n",
            "Iteration 2490, Loss: 0.019744841381907463\n",
            "Iteration 2491, Loss: 0.01529440563172102\n",
            "Iteration 2492, Loss: 0.021269511431455612\n",
            "Iteration 2493, Loss: 0.02793131209909916\n",
            "Iteration 2494, Loss: 0.030408427119255066\n",
            "Iteration 2495, Loss: 0.03824687376618385\n",
            "Iteration 2496, Loss: 0.022866245359182358\n",
            "Iteration 2497, Loss: 0.014258453622460365\n",
            "Iteration 2498, Loss: 0.02738470584154129\n",
            "Iteration 2499, Loss: 0.026269271969795227\n",
            "Iteration 2500, Loss: 0.021110467612743378\n",
            "Iteration 2501, Loss: 0.023647990077733994\n",
            "Iteration 2502, Loss: 0.03111429512500763\n",
            "Iteration 2503, Loss: 0.0311785526573658\n",
            "Iteration 2504, Loss: 0.024154838174581528\n",
            "Iteration 2505, Loss: 0.023197947070002556\n",
            "Iteration 2506, Loss: 0.029411418363451958\n",
            "Iteration 2507, Loss: 0.019234875217080116\n",
            "Iteration 2508, Loss: 0.02928781323134899\n",
            "Iteration 2509, Loss: 0.025938156992197037\n",
            "Iteration 2510, Loss: 0.01601489633321762\n",
            "Iteration 2511, Loss: 0.025486698374152184\n",
            "Iteration 2512, Loss: 0.01925080269575119\n",
            "Iteration 2513, Loss: 0.029991310089826584\n",
            "Iteration 2514, Loss: 0.0304986871778965\n",
            "Iteration 2515, Loss: 0.04046639800071716\n",
            "Iteration 2516, Loss: 0.02352265827357769\n",
            "Iteration 2517, Loss: 0.02362906001508236\n",
            "Iteration 2518, Loss: 0.031610243022441864\n",
            "Iteration 2519, Loss: 0.016538428142666817\n",
            "Iteration 2520, Loss: 0.017976220697164536\n",
            "Iteration 2521, Loss: 0.023794803768396378\n",
            "Iteration 2522, Loss: 0.016089508309960365\n",
            "Iteration 2523, Loss: 0.016682401299476624\n",
            "Iteration 2524, Loss: 0.021381907165050507\n",
            "Iteration 2525, Loss: 0.017933739349246025\n",
            "Iteration 2526, Loss: 0.026000667363405228\n",
            "Iteration 2527, Loss: 0.030091211199760437\n",
            "Iteration 2528, Loss: 0.01546065229922533\n",
            "Iteration 2529, Loss: 0.023810692131519318\n",
            "Iteration 2530, Loss: 0.022334052249789238\n",
            "Iteration 2531, Loss: 0.026458976790308952\n",
            "Iteration 2532, Loss: 0.0195588581264019\n",
            "Iteration 2533, Loss: 0.034008827060461044\n",
            "Iteration 2534, Loss: 0.017876921221613884\n",
            "Iteration 2535, Loss: 0.03810982033610344\n",
            "Iteration 2536, Loss: 0.028120875358581543\n",
            "Iteration 2537, Loss: 0.02802026830613613\n",
            "Iteration 2538, Loss: 0.030513489618897438\n",
            "Iteration 2539, Loss: 0.01762324571609497\n",
            "Iteration 2540, Loss: 0.016651926562190056\n",
            "Iteration 2541, Loss: 0.025316642597317696\n",
            "Iteration 2542, Loss: 0.030917707830667496\n",
            "Iteration 2543, Loss: 0.018805982545018196\n",
            "Iteration 2544, Loss: 0.024440903216600418\n",
            "Iteration 2545, Loss: 0.018941540271043777\n",
            "Iteration 2546, Loss: 0.02450263313949108\n",
            "Iteration 2547, Loss: 0.02163677290081978\n",
            "Iteration 2548, Loss: 0.02202346734702587\n",
            "Iteration 2549, Loss: 0.04246075078845024\n",
            "Iteration 2550, Loss: 0.025946777313947678\n",
            "Iteration 2551, Loss: 0.025896793231368065\n",
            "Iteration 2552, Loss: 0.022350408136844635\n",
            "Iteration 2553, Loss: 0.019917702302336693\n",
            "Iteration 2554, Loss: 0.027456631883978844\n",
            "Iteration 2555, Loss: 0.020148398354649544\n",
            "Iteration 2556, Loss: 0.02174939215183258\n",
            "Iteration 2557, Loss: 0.024103377014398575\n",
            "Iteration 2558, Loss: 0.025104857981204987\n",
            "Iteration 2559, Loss: 0.022332236170768738\n",
            "Iteration 2560, Loss: 0.02899194322526455\n",
            "Iteration 2561, Loss: 0.022024819627404213\n",
            "Iteration 2562, Loss: 0.020832091569900513\n",
            "Iteration 2563, Loss: 0.030637642368674278\n",
            "Iteration 2564, Loss: 0.020308183506131172\n",
            "Iteration 2565, Loss: 0.015701746568083763\n",
            "Iteration 2566, Loss: 0.017443403601646423\n",
            "Iteration 2567, Loss: 0.03299424797296524\n",
            "Iteration 2568, Loss: 0.029592065140604973\n",
            "Iteration 2569, Loss: 0.03053080290555954\n",
            "Iteration 2570, Loss: 0.03089418075978756\n",
            "Iteration 2571, Loss: 0.017885928973555565\n",
            "Iteration 2572, Loss: 0.026732372120022774\n",
            "Iteration 2573, Loss: 0.024197598919272423\n",
            "Iteration 2574, Loss: 0.015079938806593418\n",
            "Iteration 2575, Loss: 0.0241803377866745\n",
            "Iteration 2576, Loss: 0.027785053476691246\n",
            "Iteration 2577, Loss: 0.026889832690358162\n",
            "Iteration 2578, Loss: 0.016631165519356728\n",
            "Iteration 2579, Loss: 0.023077454417943954\n",
            "Iteration 2580, Loss: 0.02177603542804718\n",
            "Iteration 2581, Loss: 0.023606596514582634\n",
            "Iteration 2582, Loss: 0.019267592579126358\n",
            "Iteration 2583, Loss: 0.0182331595569849\n",
            "Iteration 2584, Loss: 0.0248269010335207\n",
            "Iteration 2585, Loss: 0.014819514006376266\n",
            "Iteration 2586, Loss: 0.026067696511745453\n",
            "Iteration 2587, Loss: 0.021834593266248703\n",
            "Iteration 2588, Loss: 0.01953645795583725\n",
            "Iteration 2589, Loss: 0.02172033302485943\n",
            "Iteration 2590, Loss: 0.02385677583515644\n",
            "Iteration 2591, Loss: 0.01829609088599682\n",
            "Iteration 2592, Loss: 0.024681493639945984\n",
            "Iteration 2593, Loss: 0.0240252036601305\n",
            "Iteration 2594, Loss: 0.02668004296720028\n",
            "Iteration 2595, Loss: 0.020340753719210625\n",
            "Iteration 2596, Loss: 0.01697198674082756\n",
            "Iteration 2597, Loss: 0.014940413646399975\n",
            "Iteration 2598, Loss: 0.019735462963581085\n",
            "Iteration 2599, Loss: 0.02262895368039608\n",
            "Iteration 2600, Loss: 0.025278866291046143\n",
            "Iteration 2601, Loss: 0.019788620993494987\n",
            "Iteration 2602, Loss: 0.022027799859642982\n",
            "Iteration 2603, Loss: 0.02133549004793167\n",
            "Iteration 2604, Loss: 0.024316702038049698\n",
            "Iteration 2605, Loss: 0.016515744850039482\n",
            "Iteration 2606, Loss: 0.032454054802656174\n",
            "Iteration 2607, Loss: 0.01599842682480812\n",
            "Iteration 2608, Loss: 0.01968243531882763\n",
            "Iteration 2609, Loss: 0.02500329352915287\n",
            "Iteration 2610, Loss: 0.02611096203327179\n",
            "Iteration 2611, Loss: 0.031547971069812775\n",
            "Iteration 2612, Loss: 0.01994732953608036\n",
            "Iteration 2613, Loss: 0.02441195771098137\n",
            "Iteration 2614, Loss: 0.02937670610845089\n",
            "Iteration 2615, Loss: 0.03036932274699211\n",
            "Iteration 2616, Loss: 0.027053620666265488\n",
            "Iteration 2617, Loss: 0.022426635026931763\n",
            "Iteration 2618, Loss: 0.03067474439740181\n",
            "Iteration 2619, Loss: 0.02686699852347374\n",
            "Iteration 2620, Loss: 0.018418453633785248\n",
            "Iteration 2621, Loss: 0.0292006004601717\n",
            "Iteration 2622, Loss: 0.015622762963175774\n",
            "Iteration 2623, Loss: 0.023560915142297745\n",
            "Iteration 2624, Loss: 0.03455411642789841\n",
            "Iteration 2625, Loss: 0.03408926725387573\n",
            "Iteration 2626, Loss: 0.01896929368376732\n",
            "Iteration 2627, Loss: 0.022383442148566246\n",
            "Iteration 2628, Loss: 0.025932002812623978\n",
            "Iteration 2629, Loss: 0.01946621760725975\n",
            "Iteration 2630, Loss: 0.02470099739730358\n",
            "Iteration 2631, Loss: 0.02976386249065399\n",
            "Iteration 2632, Loss: 0.037151120603084564\n",
            "Iteration 2633, Loss: 0.026148658245801926\n",
            "Iteration 2634, Loss: 0.02541756071150303\n",
            "Iteration 2635, Loss: 0.013452859595417976\n",
            "Iteration 2636, Loss: 0.0324087031185627\n",
            "Iteration 2637, Loss: 0.031152168288826942\n",
            "Iteration 2638, Loss: 0.022961048409342766\n",
            "Iteration 2639, Loss: 0.0344589538872242\n",
            "Iteration 2640, Loss: 0.03220028802752495\n",
            "Iteration 2641, Loss: 0.016723759472370148\n",
            "Iteration 2642, Loss: 0.021029755473136902\n",
            "Iteration 2643, Loss: 0.026403963565826416\n",
            "Iteration 2644, Loss: 0.026183102279901505\n",
            "Iteration 2645, Loss: 0.018320418894290924\n",
            "Iteration 2646, Loss: 0.013061560690402985\n",
            "Iteration 2647, Loss: 0.0162569060921669\n",
            "Iteration 2648, Loss: 0.018337884917855263\n",
            "Iteration 2649, Loss: 0.01653583161532879\n",
            "Iteration 2650, Loss: 0.029979674145579338\n",
            "Iteration 2651, Loss: 0.023253796622157097\n",
            "Iteration 2652, Loss: 0.023546332493424416\n",
            "Iteration 2653, Loss: 0.02523568831384182\n",
            "Iteration 2654, Loss: 0.0235731340944767\n",
            "Iteration 2655, Loss: 0.01910046860575676\n",
            "Iteration 2656, Loss: 0.02030584216117859\n",
            "Iteration 2657, Loss: 0.022817937657237053\n",
            "Iteration 2658, Loss: 0.02717139571905136\n",
            "Iteration 2659, Loss: 0.01793503947556019\n",
            "Iteration 2660, Loss: 0.016534414142370224\n",
            "Iteration 2661, Loss: 0.016407832503318787\n",
            "Iteration 2662, Loss: 0.02226867340505123\n",
            "Iteration 2663, Loss: 0.03225032240152359\n",
            "Iteration 2664, Loss: 0.020677456632256508\n",
            "Iteration 2665, Loss: 0.022571949288249016\n",
            "Iteration 2666, Loss: 0.02462145872414112\n",
            "Iteration 2667, Loss: 0.018039043992757797\n",
            "Iteration 2668, Loss: 0.025917941704392433\n",
            "Iteration 2669, Loss: 0.016927365213632584\n",
            "Iteration 2670, Loss: 0.014535894617438316\n",
            "Iteration 2671, Loss: 0.01790080964565277\n",
            "Iteration 2672, Loss: 0.018938753753900528\n",
            "Iteration 2673, Loss: 0.01709742285311222\n",
            "Iteration 2674, Loss: 0.04424386844038963\n",
            "Iteration 2675, Loss: 0.023745939135551453\n",
            "Iteration 2676, Loss: 0.02266480214893818\n",
            "Iteration 2677, Loss: 0.02574712410569191\n",
            "Iteration 2678, Loss: 0.027478789910674095\n",
            "Iteration 2679, Loss: 0.0257134810090065\n",
            "Iteration 2680, Loss: 0.023098589852452278\n",
            "Iteration 2681, Loss: 0.0248884204775095\n",
            "Iteration 2682, Loss: 0.021816140040755272\n",
            "Iteration 2683, Loss: 0.0218630563467741\n",
            "Iteration 2684, Loss: 0.031504835933446884\n",
            "Iteration 2685, Loss: 0.020453836768865585\n",
            "Iteration 2686, Loss: 0.02042907476425171\n",
            "Iteration 2687, Loss: 0.0375707671046257\n",
            "Iteration 2688, Loss: 0.017132122069597244\n",
            "Iteration 2689, Loss: 0.023894719779491425\n",
            "Iteration 2690, Loss: 0.02861977554857731\n",
            "Iteration 2691, Loss: 0.018012629821896553\n",
            "Iteration 2692, Loss: 0.02067553997039795\n",
            "Iteration 2693, Loss: 0.016996663063764572\n",
            "Iteration 2694, Loss: 0.027798326686024666\n",
            "Iteration 2695, Loss: 0.012489556334912777\n",
            "Iteration 2696, Loss: 0.01862258091568947\n",
            "Iteration 2697, Loss: 0.0217306986451149\n",
            "Iteration 2698, Loss: 0.030593248084187508\n",
            "Iteration 2699, Loss: 0.025805305689573288\n",
            "Iteration 2700, Loss: 0.029081912711262703\n",
            "Iteration 2701, Loss: 0.021868184208869934\n",
            "Iteration 2702, Loss: 0.019300665706396103\n",
            "Iteration 2703, Loss: 0.020263271406292915\n",
            "Iteration 2704, Loss: 0.021699214354157448\n",
            "Iteration 2705, Loss: 0.02799377590417862\n",
            "Iteration 2706, Loss: 0.01617572270333767\n",
            "Iteration 2707, Loss: 0.022799702361226082\n",
            "Iteration 2708, Loss: 0.01277143694460392\n",
            "Iteration 2709, Loss: 0.016469284892082214\n",
            "Iteration 2710, Loss: 0.020587889477610588\n",
            "Iteration 2711, Loss: 0.014643782749772072\n",
            "Iteration 2712, Loss: 0.027004921808838844\n",
            "Iteration 2713, Loss: 0.016813814640045166\n",
            "Iteration 2714, Loss: 0.041383467614650726\n",
            "Iteration 2715, Loss: 0.016109684482216835\n",
            "Iteration 2716, Loss: 0.020327318459749222\n",
            "Iteration 2717, Loss: 0.019700951874256134\n",
            "Iteration 2718, Loss: 0.024421846494078636\n",
            "Iteration 2719, Loss: 0.023014644160866737\n",
            "Iteration 2720, Loss: 0.030606457963585854\n",
            "Iteration 2721, Loss: 0.022740835323929787\n",
            "Iteration 2722, Loss: 0.022365441545844078\n",
            "Iteration 2723, Loss: 0.02311040833592415\n",
            "Iteration 2724, Loss: 0.020195050165057182\n",
            "Iteration 2725, Loss: 0.03274150937795639\n",
            "Iteration 2726, Loss: 0.01538932416588068\n",
            "Iteration 2727, Loss: 0.03243798762559891\n",
            "Iteration 2728, Loss: 0.022837648168206215\n",
            "Iteration 2729, Loss: 0.01946658082306385\n",
            "Iteration 2730, Loss: 0.01893683895468712\n",
            "Iteration 2731, Loss: 0.02585049904882908\n",
            "Iteration 2732, Loss: 0.02190682850778103\n",
            "Iteration 2733, Loss: 0.022189728915691376\n",
            "Iteration 2734, Loss: 0.01896914839744568\n",
            "Iteration 2735, Loss: 0.021996047347784042\n",
            "Iteration 2736, Loss: 0.019113030284643173\n",
            "Iteration 2737, Loss: 0.020565524697303772\n",
            "Iteration 2738, Loss: 0.013775171712040901\n",
            "Iteration 2739, Loss: 0.030206559225916862\n",
            "Iteration 2740, Loss: 0.021540291607379913\n",
            "Iteration 2741, Loss: 0.029337409883737564\n",
            "Iteration 2742, Loss: 0.030002135783433914\n",
            "Iteration 2743, Loss: 0.022615166381001472\n",
            "Iteration 2744, Loss: 0.018046241253614426\n",
            "Iteration 2745, Loss: 0.018675146624445915\n",
            "Iteration 2746, Loss: 0.01678551733493805\n",
            "Iteration 2747, Loss: 0.030405692756175995\n",
            "Iteration 2748, Loss: 0.02548142895102501\n",
            "Iteration 2749, Loss: 0.027443984523415565\n",
            "Iteration 2750, Loss: 0.023278092965483665\n",
            "Iteration 2751, Loss: 0.018368612974882126\n",
            "Iteration 2752, Loss: 0.01699988730251789\n",
            "Iteration 2753, Loss: 0.026435108855366707\n",
            "Iteration 2754, Loss: 0.020738618448376656\n",
            "Iteration 2755, Loss: 0.030372964218258858\n",
            "Iteration 2756, Loss: 0.012494741939008236\n",
            "Iteration 2757, Loss: 0.018609464168548584\n",
            "Iteration 2758, Loss: 0.024724159389734268\n",
            "Iteration 2759, Loss: 0.0217099841684103\n",
            "Iteration 2760, Loss: 0.02921467460691929\n",
            "Iteration 2761, Loss: 0.024610508233308792\n",
            "Iteration 2762, Loss: 0.0260422732681036\n",
            "Iteration 2763, Loss: 0.020473124459385872\n",
            "Iteration 2764, Loss: 0.01789049431681633\n",
            "Iteration 2765, Loss: 0.012478752061724663\n",
            "Iteration 2766, Loss: 0.022439520806074142\n",
            "Iteration 2767, Loss: 0.017449699342250824\n",
            "Iteration 2768, Loss: 0.016883859410881996\n",
            "Iteration 2769, Loss: 0.03148914873600006\n",
            "Iteration 2770, Loss: 0.018586764112114906\n",
            "Iteration 2771, Loss: 0.03333338350057602\n",
            "Iteration 2772, Loss: 0.020786777138710022\n",
            "Iteration 2773, Loss: 0.028200414031744003\n",
            "Iteration 2774, Loss: 0.028497371822595596\n",
            "Iteration 2775, Loss: 0.01967138983309269\n",
            "Iteration 2776, Loss: 0.022665714845061302\n",
            "Iteration 2777, Loss: 0.016525451093912125\n",
            "Iteration 2778, Loss: 0.016385741531848907\n",
            "Iteration 2779, Loss: 0.0169918704777956\n",
            "Iteration 2780, Loss: 0.024738609790802002\n",
            "Iteration 2781, Loss: 0.03131282329559326\n",
            "Iteration 2782, Loss: 0.021841028705239296\n",
            "Iteration 2783, Loss: 0.01624617911875248\n",
            "Iteration 2784, Loss: 0.02054831013083458\n",
            "Iteration 2785, Loss: 0.021083174273371696\n",
            "Iteration 2786, Loss: 0.035038821399211884\n",
            "Iteration 2787, Loss: 0.022982994094491005\n",
            "Iteration 2788, Loss: 0.025742286816239357\n",
            "Iteration 2789, Loss: 0.042595162987709045\n",
            "Iteration 2790, Loss: 0.025152720510959625\n",
            "Iteration 2791, Loss: 0.02276783436536789\n",
            "Iteration 2792, Loss: 0.019570663571357727\n",
            "Iteration 2793, Loss: 0.020802278071641922\n",
            "Iteration 2794, Loss: 0.023301754146814346\n",
            "Iteration 2795, Loss: 0.02063675969839096\n",
            "Iteration 2796, Loss: 0.017290350049734116\n",
            "Iteration 2797, Loss: 0.030080942437052727\n",
            "Iteration 2798, Loss: 0.022432545199990273\n",
            "Iteration 2799, Loss: 0.013384037651121616\n",
            "Iteration 2800, Loss: 0.022190077230334282\n",
            "Iteration 2801, Loss: 0.01553429663181305\n",
            "Iteration 2802, Loss: 0.020182562991976738\n",
            "Iteration 2803, Loss: 0.019710419699549675\n",
            "Iteration 2804, Loss: 0.020971572026610374\n",
            "Iteration 2805, Loss: 0.019201137125492096\n",
            "Iteration 2806, Loss: 0.019877726212143898\n",
            "Iteration 2807, Loss: 0.02551787905395031\n",
            "Iteration 2808, Loss: 0.0220306646078825\n",
            "Iteration 2809, Loss: 0.020990146324038506\n",
            "Iteration 2810, Loss: 0.016877464950084686\n",
            "Iteration 2811, Loss: 0.020831678062677383\n",
            "Iteration 2812, Loss: 0.023611191660165787\n",
            "Iteration 2813, Loss: 0.02274862676858902\n",
            "Iteration 2814, Loss: 0.026742642745375633\n",
            "Iteration 2815, Loss: 0.023132504895329475\n",
            "Iteration 2816, Loss: 0.025622151792049408\n",
            "Iteration 2817, Loss: 0.019267188385128975\n",
            "Iteration 2818, Loss: 0.02138657122850418\n",
            "Iteration 2819, Loss: 0.02234400436282158\n",
            "Iteration 2820, Loss: 0.02055382914841175\n",
            "Iteration 2821, Loss: 0.025265615433454514\n",
            "Iteration 2822, Loss: 0.023150386288762093\n",
            "Iteration 2823, Loss: 0.01651928946375847\n",
            "Iteration 2824, Loss: 0.0232081301510334\n",
            "Iteration 2825, Loss: 0.01730530336499214\n",
            "Iteration 2826, Loss: 0.02428705431520939\n",
            "Iteration 2827, Loss: 0.020452432334423065\n",
            "Iteration 2828, Loss: 0.019115867093205452\n",
            "Iteration 2829, Loss: 0.015698440372943878\n",
            "Iteration 2830, Loss: 0.016697637736797333\n",
            "Iteration 2831, Loss: 0.022474845871329308\n",
            "Iteration 2832, Loss: 0.02573278360068798\n",
            "Iteration 2833, Loss: 0.024561511352658272\n",
            "Iteration 2834, Loss: 0.029296398162841797\n",
            "Iteration 2835, Loss: 0.018187666311860085\n",
            "Iteration 2836, Loss: 0.01866331323981285\n",
            "Iteration 2837, Loss: 0.023799536749720573\n",
            "Iteration 2838, Loss: 0.023007310926914215\n",
            "Iteration 2839, Loss: 0.015859220176935196\n",
            "Iteration 2840, Loss: 0.02632148750126362\n",
            "Iteration 2841, Loss: 0.016503874212503433\n",
            "Iteration 2842, Loss: 0.02299722284078598\n",
            "Iteration 2843, Loss: 0.01657336950302124\n",
            "Iteration 2844, Loss: 0.01608245074748993\n",
            "Iteration 2845, Loss: 0.014233468100428581\n",
            "Iteration 2846, Loss: 0.020625494420528412\n",
            "Iteration 2847, Loss: 0.017534643411636353\n",
            "Iteration 2848, Loss: 0.01973499357700348\n",
            "Iteration 2849, Loss: 0.015965258702635765\n",
            "Iteration 2850, Loss: 0.010771090164780617\n",
            "Iteration 2851, Loss: 0.020430848002433777\n",
            "Iteration 2852, Loss: 0.0106100644916296\n",
            "Iteration 2853, Loss: 0.01416156068444252\n",
            "Iteration 2854, Loss: 0.02687474712729454\n",
            "Iteration 2855, Loss: 0.014984370209276676\n",
            "Iteration 2856, Loss: 0.018558111041784286\n",
            "Iteration 2857, Loss: 0.0264417864382267\n",
            "Iteration 2858, Loss: 0.016058197245001793\n",
            "Iteration 2859, Loss: 0.018110059201717377\n",
            "Iteration 2860, Loss: 0.015576180070638657\n",
            "Iteration 2861, Loss: 0.024930067360401154\n",
            "Iteration 2862, Loss: 0.018087198957800865\n",
            "Iteration 2863, Loss: 0.01897219568490982\n",
            "Iteration 2864, Loss: 0.017572546377778053\n",
            "Iteration 2865, Loss: 0.018951863050460815\n",
            "Iteration 2866, Loss: 0.017839035019278526\n",
            "Iteration 2867, Loss: 0.02152816578745842\n",
            "Iteration 2868, Loss: 0.016267335042357445\n",
            "Iteration 2869, Loss: 0.01499887928366661\n",
            "Iteration 2870, Loss: 0.026353437453508377\n",
            "Iteration 2871, Loss: 0.018079422414302826\n",
            "Iteration 2872, Loss: 0.018558243289589882\n",
            "Iteration 2873, Loss: 0.02362724579870701\n",
            "Iteration 2874, Loss: 0.023386063054203987\n",
            "Iteration 2875, Loss: 0.03249156102538109\n",
            "Iteration 2876, Loss: 0.024500787258148193\n",
            "Iteration 2877, Loss: 0.021774614229798317\n",
            "Iteration 2878, Loss: 0.022510845214128494\n",
            "Iteration 2879, Loss: 0.022153740748763084\n",
            "Iteration 2880, Loss: 0.028220554813742638\n",
            "Iteration 2881, Loss: 0.016038084402680397\n",
            "Iteration 2882, Loss: 0.01850755326449871\n",
            "Iteration 2883, Loss: 0.021963827311992645\n",
            "Iteration 2884, Loss: 0.030042624101042747\n",
            "Iteration 2885, Loss: 0.028780583292245865\n",
            "Iteration 2886, Loss: 0.019617769867181778\n",
            "Iteration 2887, Loss: 0.01818331889808178\n",
            "Iteration 2888, Loss: 0.024098966270685196\n",
            "Iteration 2889, Loss: 0.018345138058066368\n",
            "Iteration 2890, Loss: 0.021501705050468445\n",
            "Iteration 2891, Loss: 0.02668479084968567\n",
            "Iteration 2892, Loss: 0.02996203303337097\n",
            "Iteration 2893, Loss: 0.029179327189922333\n",
            "Iteration 2894, Loss: 0.02122456021606922\n",
            "Iteration 2895, Loss: 0.015343809500336647\n",
            "Iteration 2896, Loss: 0.02803546003997326\n",
            "Iteration 2897, Loss: 0.021431120112538338\n",
            "Iteration 2898, Loss: 0.016977401450276375\n",
            "Iteration 2899, Loss: 0.023090889677405357\n",
            "Iteration 2900, Loss: 0.02553754858672619\n",
            "Iteration 2901, Loss: 0.022196268662810326\n",
            "Iteration 2902, Loss: 0.01836484670639038\n",
            "Iteration 2903, Loss: 0.017833039164543152\n",
            "Iteration 2904, Loss: 0.013716061599552631\n",
            "Iteration 2905, Loss: 0.016589511185884476\n",
            "Iteration 2906, Loss: 0.014860782772302628\n",
            "Iteration 2907, Loss: 0.026738015934824944\n",
            "Iteration 2908, Loss: 0.016473297029733658\n",
            "Iteration 2909, Loss: 0.027025843039155006\n",
            "Iteration 2910, Loss: 0.016176698729395866\n",
            "Iteration 2911, Loss: 0.019172221422195435\n",
            "Iteration 2912, Loss: 0.021977467462420464\n",
            "Iteration 2913, Loss: 0.024521544575691223\n",
            "Iteration 2914, Loss: 0.019257543608546257\n",
            "Iteration 2915, Loss: 0.01653137058019638\n",
            "Iteration 2916, Loss: 0.016578586772084236\n",
            "Iteration 2917, Loss: 0.014873663894832134\n",
            "Iteration 2918, Loss: 0.01977619342505932\n",
            "Iteration 2919, Loss: 0.01847071573138237\n",
            "Iteration 2920, Loss: 0.014463596977293491\n",
            "Iteration 2921, Loss: 0.015575271099805832\n",
            "Iteration 2922, Loss: 0.028377313166856766\n",
            "Iteration 2923, Loss: 0.02620798908174038\n",
            "Iteration 2924, Loss: 0.023409506306052208\n",
            "Iteration 2925, Loss: 0.023923248052597046\n",
            "Iteration 2926, Loss: 0.021420173346996307\n",
            "Iteration 2927, Loss: 0.0221833698451519\n",
            "Iteration 2928, Loss: 0.01943344809114933\n",
            "Iteration 2929, Loss: 0.022436775267124176\n",
            "Iteration 2930, Loss: 0.019723137840628624\n",
            "Iteration 2931, Loss: 0.021179284900426865\n",
            "Iteration 2932, Loss: 0.020516961812973022\n",
            "Iteration 2933, Loss: 0.01946405880153179\n",
            "Iteration 2934, Loss: 0.020556241273880005\n",
            "Iteration 2935, Loss: 0.021268900483846664\n",
            "Iteration 2936, Loss: 0.024914199486374855\n",
            "Iteration 2937, Loss: 0.020049700513482094\n",
            "Iteration 2938, Loss: 0.019776087254285812\n",
            "Iteration 2939, Loss: 0.028032604604959488\n",
            "Iteration 2940, Loss: 0.01924331858754158\n",
            "Iteration 2941, Loss: 0.023563824594020844\n",
            "Iteration 2942, Loss: 0.021077878773212433\n",
            "Iteration 2943, Loss: 0.023691914975643158\n",
            "Iteration 2944, Loss: 0.034032274037599564\n",
            "Iteration 2945, Loss: 0.015767887234687805\n",
            "Iteration 2946, Loss: 0.021792057901620865\n",
            "Iteration 2947, Loss: 0.0360909067094326\n",
            "Iteration 2948, Loss: 0.013962147757411003\n",
            "Iteration 2949, Loss: 0.019558610394597054\n",
            "Iteration 2950, Loss: 0.018504347652196884\n",
            "Iteration 2951, Loss: 0.024164127185940742\n",
            "Iteration 2952, Loss: 0.02072017639875412\n",
            "Iteration 2953, Loss: 0.024667492136359215\n",
            "Iteration 2954, Loss: 0.01590712182223797\n",
            "Iteration 2955, Loss: 0.023360077291727066\n",
            "Iteration 2956, Loss: 0.016601575538516045\n",
            "Iteration 2957, Loss: 0.015828153118491173\n",
            "Iteration 2958, Loss: 0.032651931047439575\n",
            "Iteration 2959, Loss: 0.025691235437989235\n",
            "Iteration 2960, Loss: 0.027794841676950455\n",
            "Iteration 2961, Loss: 0.022382592782378197\n",
            "Iteration 2962, Loss: 0.02262002043426037\n",
            "Iteration 2963, Loss: 0.02473265677690506\n",
            "Iteration 2964, Loss: 0.025774654000997543\n",
            "Iteration 2965, Loss: 0.02188384160399437\n",
            "Iteration 2966, Loss: 0.023899169638752937\n",
            "Iteration 2967, Loss: 0.020132606849074364\n",
            "Iteration 2968, Loss: 0.02372611127793789\n",
            "Iteration 2969, Loss: 0.022832969203591347\n",
            "Iteration 2970, Loss: 0.023024311289191246\n",
            "Iteration 2971, Loss: 0.022320598363876343\n",
            "Iteration 2972, Loss: 0.01535257138311863\n",
            "Iteration 2973, Loss: 0.016449226066470146\n",
            "Iteration 2974, Loss: 0.01952475495636463\n",
            "Iteration 2975, Loss: 0.023146796971559525\n",
            "Iteration 2976, Loss: 0.01868736930191517\n",
            "Iteration 2977, Loss: 0.015261603519320488\n",
            "Iteration 2978, Loss: 0.01769094169139862\n",
            "Iteration 2979, Loss: 0.028895283117890358\n",
            "Iteration 2980, Loss: 0.012129704467952251\n",
            "Iteration 2981, Loss: 0.018227506428956985\n",
            "Iteration 2982, Loss: 0.016995005309581757\n",
            "Iteration 2983, Loss: 0.016501212492585182\n",
            "Iteration 2984, Loss: 0.02801906131207943\n",
            "Iteration 2985, Loss: 0.020051203668117523\n",
            "Iteration 2986, Loss: 0.022398127242922783\n",
            "Iteration 2987, Loss: 0.0204109326004982\n",
            "Iteration 2988, Loss: 0.013229632750153542\n",
            "Iteration 2989, Loss: 0.021935243159532547\n",
            "Iteration 2990, Loss: 0.027455326169729233\n",
            "Iteration 2991, Loss: 0.023182449862360954\n",
            "Iteration 2992, Loss: 0.025303548201918602\n",
            "Iteration 2993, Loss: 0.018959537148475647\n",
            "Iteration 2994, Loss: 0.022013260051608086\n",
            "Iteration 2995, Loss: 0.02021283470094204\n",
            "Iteration 2996, Loss: 0.015796227380633354\n",
            "Iteration 2997, Loss: 0.020968463271856308\n",
            "Iteration 2998, Loss: 0.021426014602184296\n",
            "Iteration 2999, Loss: 0.017194492742419243\n",
            "Iteration 3000, Loss: 0.026015782728791237\n",
            "Test Loss: 0.04501238092780113\n",
            "Iteration 3001, Loss: 0.02150019071996212\n",
            "Iteration 3002, Loss: 0.017343129962682724\n",
            "Iteration 3003, Loss: 0.01856527477502823\n",
            "Iteration 3004, Loss: 0.0172464270144701\n",
            "Iteration 3005, Loss: 0.015600320883095264\n",
            "Iteration 3006, Loss: 0.017545856535434723\n",
            "Iteration 3007, Loss: 0.01910579949617386\n",
            "Iteration 3008, Loss: 0.02115975320339203\n",
            "Iteration 3009, Loss: 0.030445853248238564\n",
            "Iteration 3010, Loss: 0.029320497065782547\n",
            "Iteration 3011, Loss: 0.02146890200674534\n",
            "Iteration 3012, Loss: 0.03067641146481037\n",
            "Iteration 3013, Loss: 0.024769136682152748\n",
            "Iteration 3014, Loss: 0.03295576944947243\n",
            "Iteration 3015, Loss: 0.012748132459819317\n",
            "Iteration 3016, Loss: 0.029343226924538612\n",
            "Iteration 3017, Loss: 0.0180557481944561\n",
            "Iteration 3018, Loss: 0.02201654016971588\n",
            "Iteration 3019, Loss: 0.027316678315401077\n",
            "Iteration 3020, Loss: 0.027002805843949318\n",
            "Iteration 3021, Loss: 0.019088203087449074\n",
            "Iteration 3022, Loss: 0.015172407031059265\n",
            "Iteration 3023, Loss: 0.024253666400909424\n",
            "Iteration 3024, Loss: 0.023163236677646637\n",
            "Iteration 3025, Loss: 0.026296013966202736\n",
            "Iteration 3026, Loss: 0.020056497305631638\n",
            "Iteration 3027, Loss: 0.025016043335199356\n",
            "Iteration 3028, Loss: 0.018429458141326904\n",
            "Iteration 3029, Loss: 0.025251204147934914\n",
            "Iteration 3030, Loss: 0.015492998994886875\n",
            "Iteration 3031, Loss: 0.020561018958687782\n",
            "Iteration 3032, Loss: 0.015450802631676197\n",
            "Iteration 3033, Loss: 0.02513505332171917\n",
            "Iteration 3034, Loss: 0.02519623376429081\n",
            "Iteration 3035, Loss: 0.03949476405978203\n",
            "Iteration 3036, Loss: 0.02707236260175705\n",
            "Iteration 3037, Loss: 0.015529006719589233\n",
            "Iteration 3038, Loss: 0.024813765659928322\n",
            "Iteration 3039, Loss: 0.029005415737628937\n",
            "Iteration 3040, Loss: 0.020726224407553673\n",
            "Iteration 3041, Loss: 0.024213124066591263\n",
            "Iteration 3042, Loss: 0.021970823407173157\n",
            "Iteration 3043, Loss: 0.03065563552081585\n",
            "Iteration 3044, Loss: 0.010052826255559921\n",
            "Iteration 3045, Loss: 0.015566016547381878\n",
            "Iteration 3046, Loss: 0.02567122131586075\n",
            "Iteration 3047, Loss: 0.03129706159234047\n",
            "Iteration 3048, Loss: 0.020036306232213974\n",
            "Iteration 3049, Loss: 0.0173444002866745\n",
            "Iteration 3050, Loss: 0.01689966395497322\n",
            "Iteration 3051, Loss: 0.01935981959104538\n",
            "Iteration 3052, Loss: 0.025414546951651573\n",
            "Iteration 3053, Loss: 0.016009438782930374\n",
            "Iteration 3054, Loss: 0.02252175472676754\n",
            "Iteration 3055, Loss: 0.02691999450325966\n",
            "Iteration 3056, Loss: 0.015714963898062706\n",
            "Iteration 3057, Loss: 0.022466635331511497\n",
            "Iteration 3058, Loss: 0.01829385757446289\n",
            "Iteration 3059, Loss: 0.024630999192595482\n",
            "Iteration 3060, Loss: 0.026185862720012665\n",
            "Iteration 3061, Loss: 0.029957108199596405\n",
            "Iteration 3062, Loss: 0.017749983817338943\n",
            "Iteration 3063, Loss: 0.0221672672778368\n",
            "Iteration 3064, Loss: 0.024222049862146378\n",
            "Iteration 3065, Loss: 0.015640318393707275\n",
            "Iteration 3066, Loss: 0.02711137942969799\n",
            "Iteration 3067, Loss: 0.022231129929423332\n",
            "Iteration 3068, Loss: 0.022999722510576248\n",
            "Iteration 3069, Loss: 0.021906254813075066\n",
            "Iteration 3070, Loss: 0.021609783172607422\n",
            "Iteration 3071, Loss: 0.016407357528805733\n",
            "Iteration 3072, Loss: 0.02459871582686901\n",
            "Iteration 3073, Loss: 0.02027874067425728\n",
            "Iteration 3074, Loss: 0.027158323675394058\n",
            "Iteration 3075, Loss: 0.020847467705607414\n",
            "Iteration 3076, Loss: 0.020383097231388092\n",
            "Iteration 3077, Loss: 0.020868925377726555\n",
            "Iteration 3078, Loss: 0.021369321271777153\n",
            "Iteration 3079, Loss: 0.027474509552121162\n",
            "Iteration 3080, Loss: 0.020971396937966347\n",
            "Iteration 3081, Loss: 0.013543645851314068\n",
            "Iteration 3082, Loss: 0.025752415880560875\n",
            "Iteration 3083, Loss: 0.028901269659399986\n",
            "Iteration 3084, Loss: 0.01910080760717392\n",
            "Iteration 3085, Loss: 0.02292504720389843\n",
            "Iteration 3086, Loss: 0.02110452950000763\n",
            "Iteration 3087, Loss: 0.027667708694934845\n",
            "Iteration 3088, Loss: 0.018192939460277557\n",
            "Iteration 3089, Loss: 0.02378196455538273\n",
            "Iteration 3090, Loss: 0.014917287044227123\n",
            "Iteration 3091, Loss: 0.02772989124059677\n",
            "Iteration 3092, Loss: 0.02183879353106022\n",
            "Iteration 3093, Loss: 0.028014469891786575\n",
            "Iteration 3094, Loss: 0.020702704787254333\n",
            "Iteration 3095, Loss: 0.013311588205397129\n",
            "Iteration 3096, Loss: 0.016293561086058617\n",
            "Iteration 3097, Loss: 0.018233396112918854\n",
            "Iteration 3098, Loss: 0.02089974470436573\n",
            "Iteration 3099, Loss: 0.032831281423568726\n",
            "Iteration 3100, Loss: 0.020940491929650307\n",
            "Iteration 3101, Loss: 0.02492316998541355\n",
            "Iteration 3102, Loss: 0.019473018124699593\n",
            "Iteration 3103, Loss: 0.01610327698290348\n",
            "Iteration 3104, Loss: 0.029843684285879135\n",
            "Iteration 3105, Loss: 0.016833115369081497\n",
            "Iteration 3106, Loss: 0.020156260579824448\n",
            "Iteration 3107, Loss: 0.02165183238685131\n",
            "Iteration 3108, Loss: 0.014153589494526386\n",
            "Iteration 3109, Loss: 0.025421421974897385\n",
            "Iteration 3110, Loss: 0.023585423827171326\n",
            "Iteration 3111, Loss: 0.024734167382121086\n",
            "Iteration 3112, Loss: 0.025924796238541603\n",
            "Iteration 3113, Loss: 0.021610485389828682\n",
            "Iteration 3114, Loss: 0.022233374416828156\n",
            "Iteration 3115, Loss: 0.021168429404497147\n",
            "Iteration 3116, Loss: 0.015094248577952385\n",
            "Iteration 3117, Loss: 0.0149499187245965\n",
            "Iteration 3118, Loss: 0.02355106733739376\n",
            "Iteration 3119, Loss: 0.01167670264840126\n",
            "Iteration 3120, Loss: 0.0194658525288105\n",
            "Iteration 3121, Loss: 0.017351491376757622\n",
            "Iteration 3122, Loss: 0.021597661077976227\n",
            "Iteration 3123, Loss: 0.02270803414285183\n",
            "Iteration 3124, Loss: 0.018459537997841835\n",
            "Iteration 3125, Loss: 0.015379835851490498\n",
            "Iteration 3126, Loss: 0.01285943016409874\n",
            "Iteration 3127, Loss: 0.019859543070197105\n",
            "Iteration 3128, Loss: 0.030688777565956116\n",
            "Iteration 3129, Loss: 0.020272810012102127\n",
            "Iteration 3130, Loss: 0.011898069642484188\n",
            "Iteration 3131, Loss: 0.021719761192798615\n",
            "Iteration 3132, Loss: 0.023557834327220917\n",
            "Iteration 3133, Loss: 0.02703670784831047\n",
            "Iteration 3134, Loss: 0.015434354543685913\n",
            "Iteration 3135, Loss: 0.01910354010760784\n",
            "Iteration 3136, Loss: 0.028109898790717125\n",
            "Iteration 3137, Loss: 0.02229985222220421\n",
            "Iteration 3138, Loss: 0.019011860713362694\n",
            "Iteration 3139, Loss: 0.028709247708320618\n",
            "Iteration 3140, Loss: 0.02169605903327465\n",
            "Iteration 3141, Loss: 0.017029747366905212\n",
            "Iteration 3142, Loss: 0.020264040678739548\n",
            "Iteration 3143, Loss: 0.0255842674523592\n",
            "Iteration 3144, Loss: 0.03196803480386734\n",
            "Iteration 3145, Loss: 0.02954580821096897\n",
            "Iteration 3146, Loss: 0.0213579460978508\n",
            "Iteration 3147, Loss: 0.02304736152291298\n",
            "Iteration 3148, Loss: 0.02301785536110401\n",
            "Iteration 3149, Loss: 0.02221372351050377\n",
            "Iteration 3150, Loss: 0.03223453834652901\n",
            "Iteration 3151, Loss: 0.020790640264749527\n",
            "Iteration 3152, Loss: 0.020304197445511818\n",
            "Iteration 3153, Loss: 0.02033831924200058\n",
            "Iteration 3154, Loss: 0.021074598655104637\n",
            "Iteration 3155, Loss: 0.015872297808527946\n",
            "Iteration 3156, Loss: 0.017890863120555878\n",
            "Iteration 3157, Loss: 0.01836424693465233\n",
            "Iteration 3158, Loss: 0.015381164848804474\n",
            "Iteration 3159, Loss: 0.025844283401966095\n",
            "Iteration 3160, Loss: 0.015737876296043396\n",
            "Iteration 3161, Loss: 0.022993823513388634\n",
            "Iteration 3162, Loss: 0.024327058345079422\n",
            "Iteration 3163, Loss: 0.020045682787895203\n",
            "Iteration 3164, Loss: 0.020711040124297142\n",
            "Iteration 3165, Loss: 0.01537654735147953\n",
            "Iteration 3166, Loss: 0.035439442843198776\n",
            "Iteration 3167, Loss: 0.013944861479103565\n",
            "Iteration 3168, Loss: 0.02008405514061451\n",
            "Iteration 3169, Loss: 0.019679736346006393\n",
            "Iteration 3170, Loss: 0.017021553590893745\n",
            "Iteration 3171, Loss: 0.023545915260910988\n",
            "Iteration 3172, Loss: 0.014618882909417152\n",
            "Iteration 3173, Loss: 0.032854098826646805\n",
            "Iteration 3174, Loss: 0.0166836678981781\n",
            "Iteration 3175, Loss: 0.010410929098725319\n",
            "Iteration 3176, Loss: 0.019075317308306694\n",
            "Iteration 3177, Loss: 0.02672184258699417\n",
            "Iteration 3178, Loss: 0.017413288354873657\n",
            "Iteration 3179, Loss: 0.03516264259815216\n",
            "Iteration 3180, Loss: 0.0178870540112257\n",
            "Iteration 3181, Loss: 0.020112477242946625\n",
            "Iteration 3182, Loss: 0.017312558367848396\n",
            "Iteration 3183, Loss: 0.017506275326013565\n",
            "Iteration 3184, Loss: 0.020659418776631355\n",
            "Iteration 3185, Loss: 0.0211690291762352\n",
            "Iteration 3186, Loss: 0.025265295058488846\n",
            "Iteration 3187, Loss: 0.021114394068717957\n",
            "Iteration 3188, Loss: 0.016961611807346344\n",
            "Iteration 3189, Loss: 0.015206737443804741\n",
            "Iteration 3190, Loss: 0.015668794512748718\n",
            "Iteration 3191, Loss: 0.014758957549929619\n",
            "Iteration 3192, Loss: 0.016101263463497162\n",
            "Iteration 3193, Loss: 0.015282963402569294\n",
            "Iteration 3194, Loss: 0.02781902812421322\n",
            "Iteration 3195, Loss: 0.021371565759181976\n",
            "Iteration 3196, Loss: 0.01504646334797144\n",
            "Iteration 3197, Loss: 0.01938377507030964\n",
            "Iteration 3198, Loss: 0.026945635676383972\n",
            "Iteration 3199, Loss: 0.03227861970663071\n",
            "Iteration 3200, Loss: 0.016734778881072998\n",
            "Iteration 3201, Loss: 0.023639574646949768\n",
            "Iteration 3202, Loss: 0.023596234619617462\n",
            "Iteration 3203, Loss: 0.0167220551520586\n",
            "Iteration 3204, Loss: 0.024244600906968117\n",
            "Iteration 3205, Loss: 0.022337784990668297\n",
            "Iteration 3206, Loss: 0.01916450634598732\n",
            "Iteration 3207, Loss: 0.015884801745414734\n",
            "Iteration 3208, Loss: 0.026313232257962227\n",
            "Iteration 3209, Loss: 0.017008116468787193\n",
            "Iteration 3210, Loss: 0.028595322743058205\n",
            "Iteration 3211, Loss: 0.017047280445694923\n",
            "Iteration 3212, Loss: 0.021486077457666397\n",
            "Iteration 3213, Loss: 0.015664661303162575\n",
            "Iteration 3214, Loss: 0.019740888848900795\n",
            "Iteration 3215, Loss: 0.01430290937423706\n",
            "Iteration 3216, Loss: 0.015334426425397396\n",
            "Iteration 3217, Loss: 0.014485549181699753\n",
            "Iteration 3218, Loss: 0.01469632051885128\n",
            "Iteration 3219, Loss: 0.03241248056292534\n",
            "Iteration 3220, Loss: 0.02708621509373188\n",
            "Iteration 3221, Loss: 0.018779627978801727\n",
            "Iteration 3222, Loss: 0.018293192610144615\n",
            "Iteration 3223, Loss: 0.012730592861771584\n",
            "Iteration 3224, Loss: 0.0176809374243021\n",
            "Iteration 3225, Loss: 0.025641407817602158\n",
            "Iteration 3226, Loss: 0.020278988406062126\n",
            "Iteration 3227, Loss: 0.03418896347284317\n",
            "Iteration 3228, Loss: 0.027169985696673393\n",
            "Iteration 3229, Loss: 0.01020245160907507\n",
            "Iteration 3230, Loss: 0.01756872981786728\n",
            "Iteration 3231, Loss: 0.022210003808140755\n",
            "Iteration 3232, Loss: 0.010181928984820843\n",
            "Iteration 3233, Loss: 0.025066262111067772\n",
            "Iteration 3234, Loss: 0.01961808279156685\n",
            "Iteration 3235, Loss: 0.021382713690400124\n",
            "Iteration 3236, Loss: 0.016296977177262306\n",
            "Iteration 3237, Loss: 0.029600495472550392\n",
            "Iteration 3238, Loss: 0.013783486559987068\n",
            "Iteration 3239, Loss: 0.022769715636968613\n",
            "Iteration 3240, Loss: 0.01714176870882511\n",
            "Iteration 3241, Loss: 0.02183173969388008\n",
            "Iteration 3242, Loss: 0.01142954546958208\n",
            "Iteration 3243, Loss: 0.018952786922454834\n",
            "Iteration 3244, Loss: 0.029214471578598022\n",
            "Iteration 3245, Loss: 0.024067804217338562\n",
            "Iteration 3246, Loss: 0.016960201784968376\n",
            "Iteration 3247, Loss: 0.02728450857102871\n",
            "Iteration 3248, Loss: 0.028454190120100975\n",
            "Iteration 3249, Loss: 0.015602095052599907\n",
            "Iteration 3250, Loss: 0.023301390931010246\n",
            "Iteration 3251, Loss: 0.022015608847141266\n",
            "Iteration 3252, Loss: 0.01834033615887165\n",
            "Iteration 3253, Loss: 0.01953738182783127\n",
            "Iteration 3254, Loss: 0.027191627770662308\n",
            "Iteration 3255, Loss: 0.020949959754943848\n",
            "Iteration 3256, Loss: 0.030972858890891075\n",
            "Iteration 3257, Loss: 0.020998062565922737\n",
            "Iteration 3258, Loss: 0.017767509445548058\n",
            "Iteration 3259, Loss: 0.023841656744480133\n",
            "Iteration 3260, Loss: 0.023160802200436592\n",
            "Iteration 3261, Loss: 0.02272159978747368\n",
            "Iteration 3262, Loss: 0.01636488176882267\n",
            "Iteration 3263, Loss: 0.015602204017341137\n",
            "Iteration 3264, Loss: 0.021285945549607277\n",
            "Iteration 3265, Loss: 0.02594253607094288\n",
            "Iteration 3266, Loss: 0.022759344428777695\n",
            "Iteration 3267, Loss: 0.02268468216061592\n",
            "Iteration 3268, Loss: 0.01916826330125332\n",
            "Iteration 3269, Loss: 0.01569278910756111\n",
            "Iteration 3270, Loss: 0.022704754024744034\n",
            "Iteration 3271, Loss: 0.014855321496725082\n",
            "Iteration 3272, Loss: 0.017651285976171494\n",
            "Iteration 3273, Loss: 0.016572142019867897\n",
            "Iteration 3274, Loss: 0.029376203194260597\n",
            "Iteration 3275, Loss: 0.020396390929818153\n",
            "Iteration 3276, Loss: 0.015288705937564373\n",
            "Iteration 3277, Loss: 0.022161683067679405\n",
            "Iteration 3278, Loss: 0.026127297431230545\n",
            "Iteration 3279, Loss: 0.0172528475522995\n",
            "Iteration 3280, Loss: 0.016462774947285652\n",
            "Iteration 3281, Loss: 0.02122047170996666\n",
            "Iteration 3282, Loss: 0.01652912050485611\n",
            "Iteration 3283, Loss: 0.016033340245485306\n",
            "Iteration 3284, Loss: 0.02739258110523224\n",
            "Iteration 3285, Loss: 0.019458582624793053\n",
            "Iteration 3286, Loss: 0.02173776365816593\n",
            "Iteration 3287, Loss: 0.019016485661268234\n",
            "Iteration 3288, Loss: 0.017698783427476883\n",
            "Iteration 3289, Loss: 0.025234634056687355\n",
            "Iteration 3290, Loss: 0.02446775697171688\n",
            "Iteration 3291, Loss: 0.022475307807326317\n",
            "Iteration 3292, Loss: 0.019888820126652718\n",
            "Iteration 3293, Loss: 0.023999176919460297\n",
            "Iteration 3294, Loss: 0.02241453342139721\n",
            "Iteration 3295, Loss: 0.01998702436685562\n",
            "Iteration 3296, Loss: 0.020928174257278442\n",
            "Iteration 3297, Loss: 0.021143555641174316\n",
            "Iteration 3298, Loss: 0.02861376479268074\n",
            "Iteration 3299, Loss: 0.019674643874168396\n",
            "Iteration 3300, Loss: 0.01749715767800808\n",
            "Iteration 3301, Loss: 0.013052612543106079\n",
            "Iteration 3302, Loss: 0.024962041527032852\n",
            "Iteration 3303, Loss: 0.0151075953617692\n",
            "Iteration 3304, Loss: 0.016711844131350517\n",
            "Iteration 3305, Loss: 0.011154131032526493\n",
            "Iteration 3306, Loss: 0.020958442240953445\n",
            "Iteration 3307, Loss: 0.010255767032504082\n",
            "Iteration 3308, Loss: 0.023685989901423454\n",
            "Iteration 3309, Loss: 0.022707724943757057\n",
            "Iteration 3310, Loss: 0.011345840990543365\n",
            "Iteration 3311, Loss: 0.02447308413684368\n",
            "Iteration 3312, Loss: 0.012548217549920082\n",
            "Iteration 3313, Loss: 0.022066807374358177\n",
            "Iteration 3314, Loss: 0.022740965709090233\n",
            "Iteration 3315, Loss: 0.017074234783649445\n",
            "Iteration 3316, Loss: 0.016681762412190437\n",
            "Iteration 3317, Loss: 0.02144625224173069\n",
            "Iteration 3318, Loss: 0.018216362223029137\n",
            "Iteration 3319, Loss: 0.014062310568988323\n",
            "Iteration 3320, Loss: 0.02474292926490307\n",
            "Iteration 3321, Loss: 0.01669980399310589\n",
            "Iteration 3322, Loss: 0.016546739265322685\n",
            "Iteration 3323, Loss: 0.025923922657966614\n",
            "Iteration 3324, Loss: 0.009701096452772617\n",
            "Iteration 3325, Loss: 0.017686210572719574\n",
            "Iteration 3326, Loss: 0.016919350251555443\n",
            "Iteration 3327, Loss: 0.0201300960034132\n",
            "Iteration 3328, Loss: 0.015381093136966228\n",
            "Iteration 3329, Loss: 0.011122781783342361\n",
            "Iteration 3330, Loss: 0.02030838653445244\n",
            "Iteration 3331, Loss: 0.02348477765917778\n",
            "Iteration 3332, Loss: 0.012189531698822975\n",
            "Iteration 3333, Loss: 0.0137802604585886\n",
            "Iteration 3334, Loss: 0.018267594277858734\n",
            "Iteration 3335, Loss: 0.021804532036185265\n",
            "Iteration 3336, Loss: 0.014698246493935585\n",
            "Iteration 3337, Loss: 0.0189911350607872\n",
            "Iteration 3338, Loss: 0.024388957768678665\n",
            "Iteration 3339, Loss: 0.02901572920382023\n",
            "Iteration 3340, Loss: 0.02163206972181797\n",
            "Iteration 3341, Loss: 0.015649208799004555\n",
            "Iteration 3342, Loss: 0.01747678406536579\n",
            "Iteration 3343, Loss: 0.0271800197660923\n",
            "Iteration 3344, Loss: 0.027806472033262253\n",
            "Iteration 3345, Loss: 0.021076643839478493\n",
            "Iteration 3346, Loss: 0.019686849787831306\n",
            "Iteration 3347, Loss: 0.015381722711026669\n",
            "Iteration 3348, Loss: 0.015222801826894283\n",
            "Iteration 3349, Loss: 0.016779759898781776\n",
            "Iteration 3350, Loss: 0.0239481870085001\n",
            "Iteration 3351, Loss: 0.019784322008490562\n",
            "Iteration 3352, Loss: 0.012346467934548855\n",
            "Iteration 3353, Loss: 0.026649532839655876\n",
            "Iteration 3354, Loss: 0.014511416666209698\n",
            "Iteration 3355, Loss: 0.017174800857901573\n",
            "Iteration 3356, Loss: 0.01472289115190506\n",
            "Iteration 3357, Loss: 0.013682063668966293\n",
            "Iteration 3358, Loss: 0.013079770840704441\n",
            "Iteration 3359, Loss: 0.020315436646342278\n",
            "Iteration 3360, Loss: 0.019955379888415337\n",
            "Iteration 3361, Loss: 0.019317027181386948\n",
            "Iteration 3362, Loss: 0.01218292023986578\n",
            "Iteration 3363, Loss: 0.02832174114882946\n",
            "Iteration 3364, Loss: 0.013831907883286476\n",
            "Iteration 3365, Loss: 0.015207072719931602\n",
            "Iteration 3366, Loss: 0.018009919673204422\n",
            "Iteration 3367, Loss: 0.018555069342255592\n",
            "Iteration 3368, Loss: 0.015427901409566402\n",
            "Iteration 3369, Loss: 0.020372407510876656\n",
            "Iteration 3370, Loss: 0.022461147978901863\n",
            "Iteration 3371, Loss: 0.02047882229089737\n",
            "Iteration 3372, Loss: 0.017848659306764603\n",
            "Iteration 3373, Loss: 0.014694498851895332\n",
            "Iteration 3374, Loss: 0.020244050770998\n",
            "Iteration 3375, Loss: 0.014649709686636925\n",
            "Iteration 3376, Loss: 0.014427185989916325\n",
            "Iteration 3377, Loss: 0.018629131838679314\n",
            "Iteration 3378, Loss: 0.019877983257174492\n",
            "Iteration 3379, Loss: 0.018930451944470406\n",
            "Iteration 3380, Loss: 0.02037065289914608\n",
            "Iteration 3381, Loss: 0.024530945345759392\n",
            "Iteration 3382, Loss: 0.014787002466619015\n",
            "Iteration 3383, Loss: 0.014502373524010181\n",
            "Iteration 3384, Loss: 0.011714666150510311\n",
            "Iteration 3385, Loss: 0.028588956221938133\n",
            "Iteration 3386, Loss: 0.01628946326673031\n",
            "Iteration 3387, Loss: 0.01229062955826521\n",
            "Iteration 3388, Loss: 0.020529896020889282\n",
            "Iteration 3389, Loss: 0.016603421419858932\n",
            "Iteration 3390, Loss: 0.016488689929246902\n",
            "Iteration 3391, Loss: 0.020437484607100487\n",
            "Iteration 3392, Loss: 0.016079137101769447\n",
            "Iteration 3393, Loss: 0.012263629585504532\n",
            "Iteration 3394, Loss: 0.018317406997084618\n",
            "Iteration 3395, Loss: 0.01551254652440548\n",
            "Iteration 3396, Loss: 0.01950920559465885\n",
            "Iteration 3397, Loss: 0.019275324419140816\n",
            "Iteration 3398, Loss: 0.0168601144105196\n",
            "Iteration 3399, Loss: 0.0218973308801651\n",
            "Iteration 3400, Loss: 0.013494889251887798\n",
            "Iteration 3401, Loss: 0.01686992682516575\n",
            "Iteration 3402, Loss: 0.01362582203000784\n",
            "Iteration 3403, Loss: 0.021156243979930878\n",
            "Iteration 3404, Loss: 0.018118182197213173\n",
            "Iteration 3405, Loss: 0.01624983921647072\n",
            "Iteration 3406, Loss: 0.027787940576672554\n",
            "Iteration 3407, Loss: 0.02326478250324726\n",
            "Iteration 3408, Loss: 0.014014013111591339\n",
            "Iteration 3409, Loss: 0.02246374823153019\n",
            "Iteration 3410, Loss: 0.01303483359515667\n",
            "Iteration 3411, Loss: 0.01958494260907173\n",
            "Iteration 3412, Loss: 0.022563643753528595\n",
            "Iteration 3413, Loss: 0.017462031915783882\n",
            "Iteration 3414, Loss: 0.019289229065179825\n",
            "Iteration 3415, Loss: 0.013617846183478832\n",
            "Iteration 3416, Loss: 0.020880190655589104\n",
            "Iteration 3417, Loss: 0.02580144628882408\n",
            "Iteration 3418, Loss: 0.021190548315644264\n",
            "Iteration 3419, Loss: 0.016215113922953606\n",
            "Iteration 3420, Loss: 0.022802075371146202\n",
            "Iteration 3421, Loss: 0.02239413559436798\n",
            "Iteration 3422, Loss: 0.02677556499838829\n",
            "Iteration 3423, Loss: 0.016770025715231895\n",
            "Iteration 3424, Loss: 0.01929713413119316\n",
            "Iteration 3425, Loss: 0.01901889778673649\n",
            "Iteration 3426, Loss: 0.01671021617949009\n",
            "Iteration 3427, Loss: 0.012562940828502178\n",
            "Iteration 3428, Loss: 0.01949068158864975\n",
            "Iteration 3429, Loss: 0.012612811289727688\n",
            "Iteration 3430, Loss: 0.01392656471580267\n",
            "Iteration 3431, Loss: 0.019147256389260292\n",
            "Iteration 3432, Loss: 0.019711937755346298\n",
            "Iteration 3433, Loss: 0.01409999467432499\n",
            "Iteration 3434, Loss: 0.016880907118320465\n",
            "Iteration 3435, Loss: 0.016041349619627\n",
            "Iteration 3436, Loss: 0.0222586952149868\n",
            "Iteration 3437, Loss: 0.017486192286014557\n",
            "Iteration 3438, Loss: 0.023351844400167465\n",
            "Iteration 3439, Loss: 0.024380149319767952\n",
            "Iteration 3440, Loss: 0.022238437086343765\n",
            "Iteration 3441, Loss: 0.021977676078677177\n",
            "Iteration 3442, Loss: 0.02018626034259796\n",
            "Iteration 3443, Loss: 0.01681515946984291\n",
            "Iteration 3444, Loss: 0.020759737119078636\n",
            "Iteration 3445, Loss: 0.01809755153954029\n",
            "Iteration 3446, Loss: 0.0181120652705431\n",
            "Iteration 3447, Loss: 0.02385559491813183\n",
            "Iteration 3448, Loss: 0.02149392105638981\n",
            "Iteration 3449, Loss: 0.022742711007595062\n",
            "Iteration 3450, Loss: 0.016363251954317093\n",
            "Iteration 3451, Loss: 0.02299896627664566\n",
            "Iteration 3452, Loss: 0.019458921626210213\n",
            "Iteration 3453, Loss: 0.015858301892876625\n",
            "Iteration 3454, Loss: 0.021828435361385345\n",
            "Iteration 3455, Loss: 0.018958570435643196\n",
            "Iteration 3456, Loss: 0.01854739710688591\n",
            "Iteration 3457, Loss: 0.020784251391887665\n",
            "Iteration 3458, Loss: 0.01976805552840233\n",
            "Iteration 3459, Loss: 0.014466887339949608\n",
            "Iteration 3460, Loss: 0.017455771565437317\n",
            "Iteration 3461, Loss: 0.02423943392932415\n",
            "Iteration 3462, Loss: 0.0241031926125288\n",
            "Iteration 3463, Loss: 0.02114018425345421\n",
            "Iteration 3464, Loss: 0.012879782356321812\n",
            "Iteration 3465, Loss: 0.02112196572124958\n",
            "Iteration 3466, Loss: 0.026072237640619278\n",
            "Iteration 3467, Loss: 0.01786581426858902\n",
            "Iteration 3468, Loss: 0.016976386308670044\n",
            "Iteration 3469, Loss: 0.022672878578305244\n",
            "Iteration 3470, Loss: 0.02008325420320034\n",
            "Iteration 3471, Loss: 0.020279472693800926\n",
            "Iteration 3472, Loss: 0.019766831770539284\n",
            "Iteration 3473, Loss: 0.019461924210190773\n",
            "Iteration 3474, Loss: 0.01775764673948288\n",
            "Iteration 3475, Loss: 0.025269046425819397\n",
            "Iteration 3476, Loss: 0.01987561769783497\n",
            "Iteration 3477, Loss: 0.021020084619522095\n",
            "Iteration 3478, Loss: 0.01847659796476364\n",
            "Iteration 3479, Loss: 0.01774238608777523\n",
            "Iteration 3480, Loss: 0.02529503032565117\n",
            "Iteration 3481, Loss: 0.017795709893107414\n",
            "Iteration 3482, Loss: 0.012743733823299408\n",
            "Iteration 3483, Loss: 0.016521930694580078\n",
            "Iteration 3484, Loss: 0.023252243176102638\n",
            "Iteration 3485, Loss: 0.025675993412733078\n",
            "Iteration 3486, Loss: 0.012632091529667377\n",
            "Iteration 3487, Loss: 0.01852370612323284\n",
            "Iteration 3488, Loss: 0.02610224485397339\n",
            "Iteration 3489, Loss: 0.01831628940999508\n",
            "Iteration 3490, Loss: 0.01698915660381317\n",
            "Iteration 3491, Loss: 0.015016833320260048\n",
            "Iteration 3492, Loss: 0.016061892732977867\n",
            "Iteration 3493, Loss: 0.014939947985112667\n",
            "Iteration 3494, Loss: 0.015565373934805393\n",
            "Iteration 3495, Loss: 0.013286570087075233\n",
            "Iteration 3496, Loss: 0.021130532026290894\n",
            "Iteration 3497, Loss: 0.011245626024901867\n",
            "Iteration 3498, Loss: 0.009859951213002205\n",
            "Iteration 3499, Loss: 0.025183508172631264\n",
            "Iteration 3500, Loss: 0.017789453268051147\n",
            "Iteration 3501, Loss: 0.020550716668367386\n",
            "Iteration 3502, Loss: 0.014260581694543362\n",
            "Iteration 3503, Loss: 0.013729074038565159\n",
            "Iteration 3504, Loss: 0.0258397925645113\n",
            "Iteration 3505, Loss: 0.012639394961297512\n",
            "Iteration 3506, Loss: 0.015515255741775036\n",
            "Iteration 3507, Loss: 0.017131194472312927\n",
            "Iteration 3508, Loss: 0.016180042177438736\n",
            "Iteration 3509, Loss: 0.020007435232400894\n",
            "Iteration 3510, Loss: 0.023831218481063843\n",
            "Iteration 3511, Loss: 0.022027673199772835\n",
            "Iteration 3512, Loss: 0.01585700362920761\n",
            "Iteration 3513, Loss: 0.021973704919219017\n",
            "Iteration 3514, Loss: 0.0171427670866251\n",
            "Iteration 3515, Loss: 0.012798254378139973\n",
            "Iteration 3516, Loss: 0.02485998347401619\n",
            "Iteration 3517, Loss: 0.02415086328983307\n",
            "Iteration 3518, Loss: 0.014790120534598827\n",
            "Iteration 3519, Loss: 0.022472595795989037\n",
            "Iteration 3520, Loss: 0.013135779649019241\n",
            "Iteration 3521, Loss: 0.018972080200910568\n",
            "Iteration 3522, Loss: 0.018547168001532555\n",
            "Iteration 3523, Loss: 0.013678801245987415\n",
            "Iteration 3524, Loss: 0.020444991067051888\n",
            "Iteration 3525, Loss: 0.01781177707016468\n",
            "Iteration 3526, Loss: 0.01958162523806095\n",
            "Iteration 3527, Loss: 0.018085414543747902\n",
            "Iteration 3528, Loss: 0.02005835808813572\n",
            "Iteration 3529, Loss: 0.017258385196328163\n",
            "Iteration 3530, Loss: 0.01615460216999054\n",
            "Iteration 3531, Loss: 0.022467250004410744\n",
            "Iteration 3532, Loss: 0.014355235733091831\n",
            "Iteration 3533, Loss: 0.019132429733872414\n",
            "Iteration 3534, Loss: 0.021999051794409752\n",
            "Iteration 3535, Loss: 0.03149592876434326\n",
            "Iteration 3536, Loss: 0.014219663105905056\n",
            "Iteration 3537, Loss: 0.01651621237397194\n",
            "Iteration 3538, Loss: 0.01677248626947403\n",
            "Iteration 3539, Loss: 0.01739143393933773\n",
            "Iteration 3540, Loss: 0.020079053938388824\n",
            "Iteration 3541, Loss: 0.020672570914030075\n",
            "Iteration 3542, Loss: 0.011262173764407635\n",
            "Iteration 3543, Loss: 0.017721889540553093\n",
            "Iteration 3544, Loss: 0.018118662759661674\n",
            "Iteration 3545, Loss: 0.02320566028356552\n",
            "Iteration 3546, Loss: 0.01387297548353672\n",
            "Iteration 3547, Loss: 0.017840594053268433\n",
            "Iteration 3548, Loss: 0.015114359557628632\n",
            "Iteration 3549, Loss: 0.024315988644957542\n",
            "Iteration 3550, Loss: 0.018847668543457985\n",
            "Iteration 3551, Loss: 0.02089119143784046\n",
            "Iteration 3552, Loss: 0.018322309479117393\n",
            "Iteration 3553, Loss: 0.014619593508541584\n",
            "Iteration 3554, Loss: 0.012493002228438854\n",
            "Iteration 3555, Loss: 0.016350213438272476\n",
            "Iteration 3556, Loss: 0.024121560156345367\n",
            "Iteration 3557, Loss: 0.011679916642606258\n",
            "Iteration 3558, Loss: 0.01915365643799305\n",
            "Iteration 3559, Loss: 0.023614317178726196\n",
            "Iteration 3560, Loss: 0.020104503259062767\n",
            "Iteration 3561, Loss: 0.01671319082379341\n",
            "Iteration 3562, Loss: 0.0208405964076519\n",
            "Iteration 3563, Loss: 0.0143594266846776\n",
            "Iteration 3564, Loss: 0.016084536910057068\n",
            "Iteration 3565, Loss: 0.023378482088446617\n",
            "Iteration 3566, Loss: 0.01501674298197031\n",
            "Iteration 3567, Loss: 0.014660073444247246\n",
            "Iteration 3568, Loss: 0.025442352518439293\n",
            "Iteration 3569, Loss: 0.01614011637866497\n",
            "Iteration 3570, Loss: 0.02166755311191082\n",
            "Iteration 3571, Loss: 0.014908844605088234\n",
            "Iteration 3572, Loss: 0.018621675670146942\n",
            "Iteration 3573, Loss: 0.011114522814750671\n",
            "Iteration 3574, Loss: 0.014612054452300072\n",
            "Iteration 3575, Loss: 0.014977550134062767\n",
            "Iteration 3576, Loss: 0.012111802585422993\n",
            "Iteration 3577, Loss: 0.02271353453397751\n",
            "Iteration 3578, Loss: 0.015903523191809654\n",
            "Iteration 3579, Loss: 0.01305310521274805\n",
            "Iteration 3580, Loss: 0.0108648044988513\n",
            "Iteration 3581, Loss: 0.01572856865823269\n",
            "Iteration 3582, Loss: 0.015402977354824543\n",
            "Iteration 3583, Loss: 0.017724642530083656\n",
            "Iteration 3584, Loss: 0.013440601527690887\n",
            "Iteration 3585, Loss: 0.020041413605213165\n",
            "Iteration 3586, Loss: 0.01899932697415352\n",
            "Iteration 3587, Loss: 0.02483668364584446\n",
            "Iteration 3588, Loss: 0.015834519639611244\n",
            "Iteration 3589, Loss: 0.021398790180683136\n",
            "Iteration 3590, Loss: 0.01636081002652645\n",
            "Iteration 3591, Loss: 0.015404663048684597\n",
            "Iteration 3592, Loss: 0.02455245517194271\n",
            "Iteration 3593, Loss: 0.01747984066605568\n",
            "Iteration 3594, Loss: 0.014747551642358303\n",
            "Iteration 3595, Loss: 0.021421587094664574\n",
            "Iteration 3596, Loss: 0.013252862729132175\n",
            "Iteration 3597, Loss: 0.014167948625981808\n",
            "Iteration 3598, Loss: 0.016930582001805305\n",
            "Iteration 3599, Loss: 0.01663506217300892\n",
            "Iteration 3600, Loss: 0.01165130827575922\n",
            "Iteration 3601, Loss: 0.017998836934566498\n",
            "Iteration 3602, Loss: 0.018031101673841476\n",
            "Iteration 3603, Loss: 0.017258230596780777\n",
            "Iteration 3604, Loss: 0.014200250618159771\n",
            "Iteration 3605, Loss: 0.019063055515289307\n",
            "Iteration 3606, Loss: 0.02233019657433033\n",
            "Iteration 3607, Loss: 0.019388126209378242\n",
            "Iteration 3608, Loss: 0.016203487291932106\n",
            "Iteration 3609, Loss: 0.013479997403919697\n",
            "Iteration 3610, Loss: 0.017163053154945374\n",
            "Iteration 3611, Loss: 0.02020278386771679\n",
            "Iteration 3612, Loss: 0.018169652670621872\n",
            "Iteration 3613, Loss: 0.010984734632074833\n",
            "Iteration 3614, Loss: 0.017791127786040306\n",
            "Iteration 3615, Loss: 0.015761278569698334\n",
            "Iteration 3616, Loss: 0.017044488340616226\n",
            "Iteration 3617, Loss: 0.01247122511267662\n",
            "Iteration 3618, Loss: 0.01676281914114952\n",
            "Iteration 3619, Loss: 0.01739431545138359\n",
            "Iteration 3620, Loss: 0.025946613401174545\n",
            "Iteration 3621, Loss: 0.0181892029941082\n",
            "Iteration 3622, Loss: 0.0190143883228302\n",
            "Iteration 3623, Loss: 0.017197664827108383\n",
            "Iteration 3624, Loss: 0.016030816361308098\n",
            "Iteration 3625, Loss: 0.016018379479646683\n",
            "Iteration 3626, Loss: 0.018715323880314827\n",
            "Iteration 3627, Loss: 0.02087215706706047\n",
            "Iteration 3628, Loss: 0.01768387109041214\n",
            "Iteration 3629, Loss: 0.01439092867076397\n",
            "Iteration 3630, Loss: 0.011713329702615738\n",
            "Iteration 3631, Loss: 0.024012377485632896\n",
            "Iteration 3632, Loss: 0.01394382119178772\n",
            "Iteration 3633, Loss: 0.020377833396196365\n",
            "Iteration 3634, Loss: 0.012133418582379818\n",
            "Iteration 3635, Loss: 0.0211366955190897\n",
            "Iteration 3636, Loss: 0.018865695223212242\n",
            "Iteration 3637, Loss: 0.02169051766395569\n",
            "Iteration 3638, Loss: 0.024683848023414612\n",
            "Iteration 3639, Loss: 0.01654127985239029\n",
            "Iteration 3640, Loss: 0.020893532782793045\n",
            "Iteration 3641, Loss: 0.025717904791235924\n",
            "Iteration 3642, Loss: 0.014825518243014812\n",
            "Iteration 3643, Loss: 0.01701909676194191\n",
            "Iteration 3644, Loss: 0.025978906080126762\n",
            "Iteration 3645, Loss: 0.015233637765049934\n",
            "Iteration 3646, Loss: 0.01966673508286476\n",
            "Iteration 3647, Loss: 0.012074158526957035\n",
            "Iteration 3648, Loss: 0.015787960961461067\n",
            "Iteration 3649, Loss: 0.013722206465899944\n",
            "Iteration 3650, Loss: 0.022973211482167244\n",
            "Iteration 3651, Loss: 0.01698761060833931\n",
            "Iteration 3652, Loss: 0.010546807199716568\n",
            "Iteration 3653, Loss: 0.01703690178692341\n",
            "Iteration 3654, Loss: 0.015781516209244728\n",
            "Iteration 3655, Loss: 0.02264910377562046\n",
            "Iteration 3656, Loss: 0.025113195180892944\n",
            "Iteration 3657, Loss: 0.012289771810173988\n",
            "Iteration 3658, Loss: 0.023480581119656563\n",
            "Iteration 3659, Loss: 0.012637274339795113\n",
            "Iteration 3660, Loss: 0.0129020931199193\n",
            "Iteration 3661, Loss: 0.01496630534529686\n",
            "Iteration 3662, Loss: 0.019887588918209076\n",
            "Iteration 3663, Loss: 0.025002507492899895\n",
            "Iteration 3664, Loss: 0.014086984097957611\n",
            "Iteration 3665, Loss: 0.021038496866822243\n",
            "Iteration 3666, Loss: 0.016988364979624748\n",
            "Iteration 3667, Loss: 0.019497055560350418\n",
            "Iteration 3668, Loss: 0.02098957449197769\n",
            "Iteration 3669, Loss: 0.022799760103225708\n",
            "Iteration 3670, Loss: 0.024810731410980225\n",
            "Iteration 3671, Loss: 0.018220223486423492\n",
            "Iteration 3672, Loss: 0.02157754637300968\n",
            "Iteration 3673, Loss: 0.011983951553702354\n",
            "Iteration 3674, Loss: 0.020668763667345047\n",
            "Iteration 3675, Loss: 0.017595617100596428\n",
            "Iteration 3676, Loss: 0.016637077555060387\n",
            "Iteration 3677, Loss: 0.019161058589816093\n",
            "Iteration 3678, Loss: 0.021202072501182556\n",
            "Iteration 3679, Loss: 0.022751102223992348\n",
            "Iteration 3680, Loss: 0.02083074115216732\n",
            "Iteration 3681, Loss: 0.017313066869974136\n",
            "Iteration 3682, Loss: 0.008861744776368141\n",
            "Iteration 3683, Loss: 0.02009534277021885\n",
            "Iteration 3684, Loss: 0.01282819826155901\n",
            "Iteration 3685, Loss: 0.009566337801516056\n",
            "Iteration 3686, Loss: 0.017871327698230743\n",
            "Iteration 3687, Loss: 0.023536264896392822\n",
            "Iteration 3688, Loss: 0.014280405826866627\n",
            "Iteration 3689, Loss: 0.01885313168168068\n",
            "Iteration 3690, Loss: 0.013094695284962654\n",
            "Iteration 3691, Loss: 0.025953643023967743\n",
            "Iteration 3692, Loss: 0.020205477252602577\n",
            "Iteration 3693, Loss: 0.017107097432017326\n",
            "Iteration 3694, Loss: 0.02051086165010929\n",
            "Iteration 3695, Loss: 0.01897008903324604\n",
            "Iteration 3696, Loss: 0.01636761613190174\n",
            "Iteration 3697, Loss: 0.022351477295160294\n",
            "Iteration 3698, Loss: 0.017737839370965958\n",
            "Iteration 3699, Loss: 0.013845006003975868\n",
            "Iteration 3700, Loss: 0.01791350543498993\n",
            "Iteration 3701, Loss: 0.02361188270151615\n",
            "Iteration 3702, Loss: 0.016457080841064453\n",
            "Iteration 3703, Loss: 0.031031761318445206\n",
            "Iteration 3704, Loss: 0.02083921805024147\n",
            "Iteration 3705, Loss: 0.023063521832227707\n",
            "Iteration 3706, Loss: 0.015847500413656235\n",
            "Iteration 3707, Loss: 0.01805737242102623\n",
            "Iteration 3708, Loss: 0.017606554552912712\n",
            "Iteration 3709, Loss: 0.011385593563318253\n",
            "Iteration 3710, Loss: 0.02485552802681923\n",
            "Iteration 3711, Loss: 0.014882457442581654\n",
            "Iteration 3712, Loss: 0.016526056453585625\n",
            "Iteration 3713, Loss: 0.015513882972300053\n",
            "Iteration 3714, Loss: 0.011228958144783974\n",
            "Iteration 3715, Loss: 0.018675928935408592\n",
            "Iteration 3716, Loss: 0.0194623414427042\n",
            "Iteration 3717, Loss: 0.016211003065109253\n",
            "Iteration 3718, Loss: 0.015412175096571445\n",
            "Iteration 3719, Loss: 0.010519917123019695\n",
            "Iteration 3720, Loss: 0.0164901465177536\n",
            "Iteration 3721, Loss: 0.013052994385361671\n",
            "Iteration 3722, Loss: 0.0132223479449749\n",
            "Iteration 3723, Loss: 0.01988770253956318\n",
            "Iteration 3724, Loss: 0.016119521111249924\n",
            "Iteration 3725, Loss: 0.016983356326818466\n",
            "Iteration 3726, Loss: 0.013715888373553753\n",
            "Iteration 3727, Loss: 0.010930562391877174\n",
            "Iteration 3728, Loss: 0.020319633185863495\n",
            "Iteration 3729, Loss: 0.022710928693413734\n",
            "Iteration 3730, Loss: 0.015162003226578236\n",
            "Iteration 3731, Loss: 0.016278591006994247\n",
            "Iteration 3732, Loss: 0.01661357283592224\n",
            "Iteration 3733, Loss: 0.028209442272782326\n",
            "Iteration 3734, Loss: 0.01202023308724165\n",
            "Iteration 3735, Loss: 0.0251003485172987\n",
            "Iteration 3736, Loss: 0.014861183241009712\n",
            "Iteration 3737, Loss: 0.014178361743688583\n",
            "Iteration 3738, Loss: 0.01680288277566433\n",
            "Iteration 3739, Loss: 0.019974488765001297\n",
            "Iteration 3740, Loss: 0.02006380259990692\n",
            "Iteration 3741, Loss: 0.015399148687720299\n",
            "Iteration 3742, Loss: 0.014946051873266697\n",
            "Iteration 3743, Loss: 0.02597203105688095\n",
            "Iteration 3744, Loss: 0.01473997626453638\n",
            "Iteration 3745, Loss: 0.010215413756668568\n",
            "Iteration 3746, Loss: 0.013341492973268032\n",
            "Iteration 3747, Loss: 0.021948963403701782\n",
            "Iteration 3748, Loss: 0.015058019198477268\n",
            "Iteration 3749, Loss: 0.013930737972259521\n",
            "Iteration 3750, Loss: 0.01484975777566433\n",
            "Iteration 3751, Loss: 0.014067808166146278\n",
            "Iteration 3752, Loss: 0.009508263319730759\n",
            "Iteration 3753, Loss: 0.013054111041128635\n",
            "Iteration 3754, Loss: 0.01735992357134819\n",
            "Iteration 3755, Loss: 0.01652565598487854\n",
            "Iteration 3756, Loss: 0.01617451384663582\n",
            "Iteration 3757, Loss: 0.02007388509809971\n",
            "Iteration 3758, Loss: 0.02426867187023163\n",
            "Iteration 3759, Loss: 0.013106562197208405\n",
            "Iteration 3760, Loss: 0.014833065681159496\n",
            "Iteration 3761, Loss: 0.018874170258641243\n",
            "Iteration 3762, Loss: 0.01760929450392723\n",
            "Iteration 3763, Loss: 0.02028976008296013\n",
            "Iteration 3764, Loss: 0.018567072227597237\n",
            "Iteration 3765, Loss: 0.012550760991871357\n",
            "Iteration 3766, Loss: 0.014086339622735977\n",
            "Iteration 3767, Loss: 0.013751719146966934\n",
            "Iteration 3768, Loss: 0.017484895884990692\n",
            "Iteration 3769, Loss: 0.023042436689138412\n",
            "Iteration 3770, Loss: 0.01491259690374136\n",
            "Iteration 3771, Loss: 0.028954993933439255\n",
            "Iteration 3772, Loss: 0.02021496370434761\n",
            "Iteration 3773, Loss: 0.01377426739782095\n",
            "Iteration 3774, Loss: 0.01913495548069477\n",
            "Iteration 3775, Loss: 0.015444434247910976\n",
            "Iteration 3776, Loss: 0.012271280400454998\n",
            "Iteration 3777, Loss: 0.013393023051321507\n",
            "Iteration 3778, Loss: 0.021847130730748177\n",
            "Iteration 3779, Loss: 0.015880608931183815\n",
            "Iteration 3780, Loss: 0.01977609470486641\n",
            "Iteration 3781, Loss: 0.025154436007142067\n",
            "Iteration 3782, Loss: 0.0212391410022974\n",
            "Iteration 3783, Loss: 0.020060811191797256\n",
            "Iteration 3784, Loss: 0.01782897301018238\n",
            "Iteration 3785, Loss: 0.0166671983897686\n",
            "Iteration 3786, Loss: 0.016560347750782967\n",
            "Iteration 3787, Loss: 0.02185344696044922\n",
            "Iteration 3788, Loss: 0.01817185990512371\n",
            "Iteration 3789, Loss: 0.021730653941631317\n",
            "Iteration 3790, Loss: 0.024350279942154884\n",
            "Iteration 3791, Loss: 0.015513247810304165\n",
            "Iteration 3792, Loss: 0.014309678226709366\n",
            "Iteration 3793, Loss: 0.02146032452583313\n",
            "Iteration 3794, Loss: 0.025619274005293846\n",
            "Iteration 3795, Loss: 0.016875555738806725\n",
            "Iteration 3796, Loss: 0.01671484485268593\n",
            "Iteration 3797, Loss: 0.01799672469496727\n",
            "Iteration 3798, Loss: 0.017154861241579056\n",
            "Iteration 3799, Loss: 0.013411344960331917\n",
            "Iteration 3800, Loss: 0.02019181288778782\n",
            "Iteration 3801, Loss: 0.013044975697994232\n",
            "Iteration 3802, Loss: 0.019070927053689957\n",
            "Iteration 3803, Loss: 0.01082477904856205\n",
            "Iteration 3804, Loss: 0.024257849901914597\n",
            "Iteration 3805, Loss: 0.025138167664408684\n",
            "Iteration 3806, Loss: 0.01031651720404625\n",
            "Iteration 3807, Loss: 0.02089640498161316\n",
            "Iteration 3808, Loss: 0.020240088924765587\n",
            "Iteration 3809, Loss: 0.0197479035705328\n",
            "Iteration 3810, Loss: 0.029280275106430054\n",
            "Iteration 3811, Loss: 0.01834087073802948\n",
            "Iteration 3812, Loss: 0.01094804983586073\n",
            "Iteration 3813, Loss: 0.008755420334637165\n",
            "Iteration 3814, Loss: 0.02445993572473526\n",
            "Iteration 3815, Loss: 0.0233394093811512\n",
            "Iteration 3816, Loss: 0.026558497920632362\n",
            "Iteration 3817, Loss: 0.03150523826479912\n",
            "Iteration 3818, Loss: 0.020472606644034386\n",
            "Iteration 3819, Loss: 0.027449890971183777\n",
            "Iteration 3820, Loss: 0.017739424481987953\n",
            "Iteration 3821, Loss: 0.021704725921154022\n",
            "Iteration 3822, Loss: 0.01757531240582466\n",
            "Iteration 3823, Loss: 0.01621006242930889\n",
            "Iteration 3824, Loss: 0.016699330881237984\n",
            "Iteration 3825, Loss: 0.019075894728302956\n",
            "Iteration 3826, Loss: 0.019142571836709976\n",
            "Iteration 3827, Loss: 0.031384121626615524\n",
            "Iteration 3828, Loss: 0.010136357508599758\n",
            "Iteration 3829, Loss: 0.01779928430914879\n",
            "Iteration 3830, Loss: 0.02864668145775795\n",
            "Iteration 3831, Loss: 0.02468692511320114\n",
            "Iteration 3832, Loss: 0.017885033041238785\n",
            "Iteration 3833, Loss: 0.01764138601720333\n",
            "Iteration 3834, Loss: 0.021919041872024536\n",
            "Iteration 3835, Loss: 0.013388327322900295\n",
            "Iteration 3836, Loss: 0.018487894907593727\n",
            "Iteration 3837, Loss: 0.029295146465301514\n",
            "Iteration 3838, Loss: 0.026252850890159607\n",
            "Iteration 3839, Loss: 0.017672298476099968\n",
            "Iteration 3840, Loss: 0.015301809646189213\n",
            "Iteration 3841, Loss: 0.011859570629894733\n",
            "Iteration 3842, Loss: 0.016609953716397285\n",
            "Iteration 3843, Loss: 0.014088154770433903\n",
            "Iteration 3844, Loss: 0.019774120301008224\n",
            "Iteration 3845, Loss: 0.023251069709658623\n",
            "Iteration 3846, Loss: 0.0169062539935112\n",
            "Iteration 3847, Loss: 0.015412972308695316\n",
            "Iteration 3848, Loss: 0.020528435707092285\n",
            "Iteration 3849, Loss: 0.018497472628951073\n",
            "Iteration 3850, Loss: 0.016066981479525566\n",
            "Iteration 3851, Loss: 0.018220670521259308\n",
            "Iteration 3852, Loss: 0.014336053282022476\n",
            "Iteration 3853, Loss: 0.023455694317817688\n",
            "Iteration 3854, Loss: 0.013600509613752365\n",
            "Iteration 3855, Loss: 0.023287300020456314\n",
            "Iteration 3856, Loss: 0.018504464998841286\n",
            "Iteration 3857, Loss: 0.02030261792242527\n",
            "Iteration 3858, Loss: 0.026167144998908043\n",
            "Iteration 3859, Loss: 0.013045666739344597\n",
            "Iteration 3860, Loss: 0.016694718971848488\n",
            "Iteration 3861, Loss: 0.021317467093467712\n",
            "Iteration 3862, Loss: 0.016023946925997734\n",
            "Iteration 3863, Loss: 0.014960229396820068\n",
            "Iteration 3864, Loss: 0.015436581335961819\n",
            "Iteration 3865, Loss: 0.01316428929567337\n",
            "Iteration 3866, Loss: 0.022939343005418777\n",
            "Iteration 3867, Loss: 0.013294667936861515\n",
            "Iteration 3868, Loss: 0.015150623396039009\n",
            "Iteration 3869, Loss: 0.0063289981335401535\n",
            "Iteration 3870, Loss: 0.017014147713780403\n",
            "Iteration 3871, Loss: 0.017776967957615852\n",
            "Iteration 3872, Loss: 0.009715170599520206\n",
            "Iteration 3873, Loss: 0.011098841205239296\n",
            "Iteration 3874, Loss: 0.010782340541481972\n",
            "Iteration 3875, Loss: 0.0216043870896101\n",
            "Iteration 3876, Loss: 0.016615750268101692\n",
            "Iteration 3877, Loss: 0.014701947569847107\n",
            "Iteration 3878, Loss: 0.018140992149710655\n",
            "Iteration 3879, Loss: 0.02484566532075405\n",
            "Iteration 3880, Loss: 0.01340227760374546\n",
            "Iteration 3881, Loss: 0.01797572150826454\n",
            "Iteration 3882, Loss: 0.013773251324892044\n",
            "Iteration 3883, Loss: 0.027882853522896767\n",
            "Iteration 3884, Loss: 0.02163488231599331\n",
            "Iteration 3885, Loss: 0.01077394001185894\n",
            "Iteration 3886, Loss: 0.017908405512571335\n",
            "Iteration 3887, Loss: 0.012577744200825691\n",
            "Iteration 3888, Loss: 0.01244012825191021\n",
            "Iteration 3889, Loss: 0.014787253923714161\n",
            "Iteration 3890, Loss: 0.017820369452238083\n",
            "Iteration 3891, Loss: 0.01792706362903118\n",
            "Iteration 3892, Loss: 0.010685665532946587\n",
            "Iteration 3893, Loss: 0.014649013988673687\n",
            "Iteration 3894, Loss: 0.01572537049651146\n",
            "Iteration 3895, Loss: 0.01850116439163685\n",
            "Iteration 3896, Loss: 0.019539544358849525\n",
            "Iteration 3897, Loss: 0.016978546977043152\n",
            "Iteration 3898, Loss: 0.020152995362877846\n",
            "Iteration 3899, Loss: 0.019499490037560463\n",
            "Iteration 3900, Loss: 0.017108161002397537\n",
            "Iteration 3901, Loss: 0.028288768604397774\n",
            "Iteration 3902, Loss: 0.020412664860486984\n",
            "Iteration 3903, Loss: 0.016528654843568802\n",
            "Iteration 3904, Loss: 0.015233831480145454\n",
            "Iteration 3905, Loss: 0.022782057523727417\n",
            "Iteration 3906, Loss: 0.0176123958081007\n",
            "Iteration 3907, Loss: 0.013574978336691856\n",
            "Iteration 3908, Loss: 0.015944810584187508\n",
            "Iteration 3909, Loss: 0.01683785580098629\n",
            "Iteration 3910, Loss: 0.01570787839591503\n",
            "Iteration 3911, Loss: 0.017932018265128136\n",
            "Iteration 3912, Loss: 0.018283745273947716\n",
            "Iteration 3913, Loss: 0.020245520398020744\n",
            "Iteration 3914, Loss: 0.019267994910478592\n",
            "Iteration 3915, Loss: 0.010453524999320507\n",
            "Iteration 3916, Loss: 0.024995869025588036\n",
            "Iteration 3917, Loss: 0.016233161091804504\n",
            "Iteration 3918, Loss: 0.015106840059161186\n",
            "Iteration 3919, Loss: 0.013492192141711712\n",
            "Iteration 3920, Loss: 0.013526909984648228\n",
            "Iteration 3921, Loss: 0.011143741197884083\n",
            "Iteration 3922, Loss: 0.017478931695222855\n",
            "Iteration 3923, Loss: 0.018003743141889572\n",
            "Iteration 3924, Loss: 0.013902408070862293\n",
            "Iteration 3925, Loss: 0.017749227583408356\n",
            "Iteration 3926, Loss: 0.024792667478322983\n",
            "Iteration 3927, Loss: 0.0212571918964386\n",
            "Iteration 3928, Loss: 0.012208003550767899\n",
            "Iteration 3929, Loss: 0.014418595470488071\n",
            "Iteration 3930, Loss: 0.017770469188690186\n",
            "Iteration 3931, Loss: 0.014890429563820362\n",
            "Iteration 3932, Loss: 0.013551478274166584\n",
            "Iteration 3933, Loss: 0.016235655173659325\n",
            "Iteration 3934, Loss: 0.019835611805319786\n",
            "Iteration 3935, Loss: 0.024662921205163002\n",
            "Iteration 3936, Loss: 0.024873385205864906\n",
            "Iteration 3937, Loss: 0.024940431118011475\n",
            "Iteration 3938, Loss: 0.018117867410182953\n",
            "Iteration 3939, Loss: 0.014704948291182518\n",
            "Iteration 3940, Loss: 0.012188788503408432\n",
            "Iteration 3941, Loss: 0.022860532626509666\n",
            "Iteration 3942, Loss: 0.012603087350726128\n",
            "Iteration 3943, Loss: 0.018196536228060722\n",
            "Iteration 3944, Loss: 0.014152629300951958\n",
            "Iteration 3945, Loss: 0.02713421732187271\n",
            "Iteration 3946, Loss: 0.0183677077293396\n",
            "Iteration 3947, Loss: 0.01616770401597023\n",
            "Iteration 3948, Loss: 0.01664069853723049\n",
            "Iteration 3949, Loss: 0.0252928975969553\n",
            "Iteration 3950, Loss: 0.017393875867128372\n",
            "Iteration 3951, Loss: 0.020364142954349518\n",
            "Iteration 3952, Loss: 0.018498821184039116\n",
            "Iteration 3953, Loss: 0.016272472217679024\n",
            "Iteration 3954, Loss: 0.008391001261770725\n",
            "Iteration 3955, Loss: 0.028610030189156532\n",
            "Iteration 3956, Loss: 0.013634312897920609\n",
            "Iteration 3957, Loss: 0.01679360307753086\n",
            "Iteration 3958, Loss: 0.012871834449470043\n",
            "Iteration 3959, Loss: 0.016829777508974075\n",
            "Iteration 3960, Loss: 0.015707513317465782\n",
            "Iteration 3961, Loss: 0.019935278221964836\n",
            "Iteration 3962, Loss: 0.01952665112912655\n",
            "Iteration 3963, Loss: 0.018775515258312225\n",
            "Iteration 3964, Loss: 0.0177634097635746\n",
            "Iteration 3965, Loss: 0.020017895847558975\n",
            "Iteration 3966, Loss: 0.017404455691576004\n",
            "Iteration 3967, Loss: 0.022000836208462715\n",
            "Iteration 3968, Loss: 0.02366793341934681\n",
            "Iteration 3969, Loss: 0.015048039145767689\n",
            "Iteration 3970, Loss: 0.019248316064476967\n",
            "Iteration 3971, Loss: 0.013906791806221008\n",
            "Iteration 3972, Loss: 0.015942685306072235\n",
            "Iteration 3973, Loss: 0.010578148998320103\n",
            "Iteration 3974, Loss: 0.010988188907504082\n",
            "Iteration 3975, Loss: 0.019622988998889923\n",
            "Iteration 3976, Loss: 0.014917668886482716\n",
            "Iteration 3977, Loss: 0.018624480813741684\n",
            "Iteration 3978, Loss: 0.0204280074685812\n",
            "Iteration 3979, Loss: 0.03304075822234154\n",
            "Iteration 3980, Loss: 0.014850644394755363\n",
            "Iteration 3981, Loss: 0.021754590794444084\n",
            "Iteration 3982, Loss: 0.017611922696232796\n",
            "Iteration 3983, Loss: 0.01595022901892662\n",
            "Iteration 3984, Loss: 0.015852326527237892\n",
            "Iteration 3985, Loss: 0.011433811858296394\n",
            "Iteration 3986, Loss: 0.018066709861159325\n",
            "Iteration 3987, Loss: 0.01901167817413807\n",
            "Iteration 3988, Loss: 0.025765197351574898\n",
            "Iteration 3989, Loss: 0.024579405784606934\n",
            "Iteration 3990, Loss: 0.016850588843226433\n",
            "Iteration 3991, Loss: 0.01688477210700512\n",
            "Iteration 3992, Loss: 0.012847249396145344\n",
            "Iteration 3993, Loss: 0.02017945423722267\n",
            "Iteration 3994, Loss: 0.014842912554740906\n",
            "Iteration 3995, Loss: 0.014890193939208984\n",
            "Iteration 3996, Loss: 0.015168415382504463\n",
            "Iteration 3997, Loss: 0.026557164266705513\n",
            "Iteration 3998, Loss: 0.02217809110879898\n",
            "Iteration 3999, Loss: 0.014602536335587502\n",
            "Iteration 4000, Loss: 0.01073546428233385\n",
            "Test Loss: 0.03696804866194725\n",
            "Iteration 4001, Loss: 0.015921125188469887\n",
            "Iteration 4002, Loss: 0.016606826335191727\n",
            "Iteration 4003, Loss: 0.016880134120583534\n",
            "Iteration 4004, Loss: 0.01959805190563202\n",
            "Iteration 4005, Loss: 0.015809211879968643\n",
            "Iteration 4006, Loss: 0.02163936197757721\n",
            "Iteration 4007, Loss: 0.01235717162489891\n",
            "Iteration 4008, Loss: 0.0175457876175642\n",
            "Iteration 4009, Loss: 0.018570631742477417\n",
            "Iteration 4010, Loss: 0.01226707175374031\n",
            "Iteration 4011, Loss: 0.014688793569803238\n",
            "Iteration 4012, Loss: 0.013560094870626926\n",
            "Iteration 4013, Loss: 0.021501053124666214\n",
            "Iteration 4014, Loss: 0.012942127883434296\n",
            "Iteration 4015, Loss: 0.02318292111158371\n",
            "Iteration 4016, Loss: 0.027325371280312538\n",
            "Iteration 4017, Loss: 0.0174418818205595\n",
            "Iteration 4018, Loss: 0.015422665514051914\n",
            "Iteration 4019, Loss: 0.012511354871094227\n",
            "Iteration 4020, Loss: 0.016365788877010345\n",
            "Iteration 4021, Loss: 0.017934421077370644\n",
            "Iteration 4022, Loss: 0.020951591432094574\n",
            "Iteration 4023, Loss: 0.019328687340021133\n",
            "Iteration 4024, Loss: 0.018018262460827827\n",
            "Iteration 4025, Loss: 0.01882990449666977\n",
            "Iteration 4026, Loss: 0.012500138022005558\n",
            "Iteration 4027, Loss: 0.012014247477054596\n",
            "Iteration 4028, Loss: 0.012579568661749363\n",
            "Iteration 4029, Loss: 0.022401444613933563\n",
            "Iteration 4030, Loss: 0.023316511884331703\n",
            "Iteration 4031, Loss: 0.022665178403258324\n",
            "Iteration 4032, Loss: 0.01839953102171421\n",
            "Iteration 4033, Loss: 0.021550672128796577\n",
            "Iteration 4034, Loss: 0.02066022902727127\n",
            "Iteration 4035, Loss: 0.013806995935738087\n",
            "Iteration 4036, Loss: 0.01338342484086752\n",
            "Iteration 4037, Loss: 0.009800774045288563\n",
            "Iteration 4038, Loss: 0.021634891629219055\n",
            "Iteration 4039, Loss: 0.018158111721277237\n",
            "Iteration 4040, Loss: 0.026622705161571503\n",
            "Iteration 4041, Loss: 0.014898020774126053\n",
            "Iteration 4042, Loss: 0.012285225093364716\n",
            "Iteration 4043, Loss: 0.016825810074806213\n",
            "Iteration 4044, Loss: 0.01615983247756958\n",
            "Iteration 4045, Loss: 0.01979975774884224\n",
            "Iteration 4046, Loss: 0.016710346564650536\n",
            "Iteration 4047, Loss: 0.016160311177372932\n",
            "Iteration 4048, Loss: 0.026115769520401955\n",
            "Iteration 4049, Loss: 0.015078500844538212\n",
            "Iteration 4050, Loss: 0.013840707950294018\n",
            "Iteration 4051, Loss: 0.010751321911811829\n",
            "Iteration 4052, Loss: 0.012445236556231976\n",
            "Iteration 4053, Loss: 0.022393252700567245\n",
            "Iteration 4054, Loss: 0.021696647629141808\n",
            "Iteration 4055, Loss: 0.01481571514159441\n",
            "Iteration 4056, Loss: 0.018356095999479294\n",
            "Iteration 4057, Loss: 0.021285688504576683\n",
            "Iteration 4058, Loss: 0.021582623943686485\n",
            "Iteration 4059, Loss: 0.02333734557032585\n",
            "Iteration 4060, Loss: 0.021604124456644058\n",
            "Iteration 4061, Loss: 0.022479940205812454\n",
            "Iteration 4062, Loss: 0.016363181173801422\n",
            "Iteration 4063, Loss: 0.012191454879939556\n",
            "Iteration 4064, Loss: 0.027000924572348595\n",
            "Iteration 4065, Loss: 0.0179603174328804\n",
            "Iteration 4066, Loss: 0.016360294073820114\n",
            "Iteration 4067, Loss: 0.01036148052662611\n",
            "Iteration 4068, Loss: 0.017835091799497604\n",
            "Iteration 4069, Loss: 0.017804011702537537\n",
            "Iteration 4070, Loss: 0.014315597712993622\n",
            "Iteration 4071, Loss: 0.018823960795998573\n",
            "Iteration 4072, Loss: 0.023427963256835938\n",
            "Iteration 4073, Loss: 0.02132093906402588\n",
            "Iteration 4074, Loss: 0.012473169714212418\n",
            "Iteration 4075, Loss: 0.01773339882493019\n",
            "Iteration 4076, Loss: 0.018603559583425522\n",
            "Iteration 4077, Loss: 0.018372533842921257\n",
            "Iteration 4078, Loss: 0.019261082634329796\n",
            "Iteration 4079, Loss: 0.013462533243000507\n",
            "Iteration 4080, Loss: 0.017396977171301842\n",
            "Iteration 4081, Loss: 0.01799122430384159\n",
            "Iteration 4082, Loss: 0.012433084659278393\n",
            "Iteration 4083, Loss: 0.01697706989943981\n",
            "Iteration 4084, Loss: 0.016896812245249748\n",
            "Iteration 4085, Loss: 0.02908024750649929\n",
            "Iteration 4086, Loss: 0.014387926086783409\n",
            "Iteration 4087, Loss: 0.014990312047302723\n",
            "Iteration 4088, Loss: 0.011943668127059937\n",
            "Iteration 4089, Loss: 0.012625131756067276\n",
            "Iteration 4090, Loss: 0.012606695294380188\n",
            "Iteration 4091, Loss: 0.013962400145828724\n",
            "Iteration 4092, Loss: 0.016005011275410652\n",
            "Iteration 4093, Loss: 0.009826426394283772\n",
            "Iteration 4094, Loss: 0.02449892833828926\n",
            "Iteration 4095, Loss: 0.021549036726355553\n",
            "Iteration 4096, Loss: 0.012961081229150295\n",
            "Iteration 4097, Loss: 0.014363853260874748\n",
            "Iteration 4098, Loss: 0.010146177373826504\n",
            "Iteration 4099, Loss: 0.01396886259317398\n",
            "Iteration 4100, Loss: 0.011862402781844139\n",
            "Iteration 4101, Loss: 0.015781931579113007\n",
            "Iteration 4102, Loss: 0.02224540151655674\n",
            "Iteration 4103, Loss: 0.010043872520327568\n",
            "Iteration 4104, Loss: 0.012614932842552662\n",
            "Iteration 4105, Loss: 0.018443787470459938\n",
            "Iteration 4106, Loss: 0.007968617603182793\n",
            "Iteration 4107, Loss: 0.023367639631032944\n",
            "Iteration 4108, Loss: 0.018504982814192772\n",
            "Iteration 4109, Loss: 0.01007612980902195\n",
            "Iteration 4110, Loss: 0.018689505755901337\n",
            "Iteration 4111, Loss: 0.01818358153104782\n",
            "Iteration 4112, Loss: 0.020716246217489243\n",
            "Iteration 4113, Loss: 0.013234789483249187\n",
            "Iteration 4114, Loss: 0.020119773223996162\n",
            "Iteration 4115, Loss: 0.015500087291002274\n",
            "Iteration 4116, Loss: 0.013632351532578468\n",
            "Iteration 4117, Loss: 0.01950814016163349\n",
            "Iteration 4118, Loss: 0.01820589415729046\n",
            "Iteration 4119, Loss: 0.01923004537820816\n",
            "Iteration 4120, Loss: 0.008896100334823132\n",
            "Iteration 4121, Loss: 0.01572421006858349\n",
            "Iteration 4122, Loss: 0.012648890726268291\n",
            "Iteration 4123, Loss: 0.01834217645227909\n",
            "Iteration 4124, Loss: 0.015663471072912216\n",
            "Iteration 4125, Loss: 0.016936009749770164\n",
            "Iteration 4126, Loss: 0.014938839711248875\n",
            "Iteration 4127, Loss: 0.020663006231188774\n",
            "Iteration 4128, Loss: 0.01366239134222269\n",
            "Iteration 4129, Loss: 0.020962810143828392\n",
            "Iteration 4130, Loss: 0.016809552907943726\n",
            "Iteration 4131, Loss: 0.014144412241876125\n",
            "Iteration 4132, Loss: 0.010413117706775665\n",
            "Iteration 4133, Loss: 0.010147049091756344\n",
            "Iteration 4134, Loss: 0.019941862672567368\n",
            "Iteration 4135, Loss: 0.015660181641578674\n",
            "Iteration 4136, Loss: 0.01819145679473877\n",
            "Iteration 4137, Loss: 0.0162361953407526\n",
            "Iteration 4138, Loss: 0.009433118626475334\n",
            "Iteration 4139, Loss: 0.011732364073395729\n",
            "Iteration 4140, Loss: 0.017050951719284058\n",
            "Iteration 4141, Loss: 0.017923204228281975\n",
            "Iteration 4142, Loss: 0.020952792838215828\n",
            "Iteration 4143, Loss: 0.016615500673651695\n",
            "Iteration 4144, Loss: 0.011803260073065758\n",
            "Iteration 4145, Loss: 0.018021075055003166\n",
            "Iteration 4146, Loss: 0.018938444554805756\n",
            "Iteration 4147, Loss: 0.022480303421616554\n",
            "Iteration 4148, Loss: 0.013546249829232693\n",
            "Iteration 4149, Loss: 0.01738411746919155\n",
            "Iteration 4150, Loss: 0.011884660460054874\n",
            "Iteration 4151, Loss: 0.012905802577733994\n",
            "Iteration 4152, Loss: 0.013972718268632889\n",
            "Iteration 4153, Loss: 0.016471680253744125\n",
            "Iteration 4154, Loss: 0.011558350175619125\n",
            "Iteration 4155, Loss: 0.014450202696025372\n",
            "Iteration 4156, Loss: 0.016770094633102417\n",
            "Iteration 4157, Loss: 0.017732586711645126\n",
            "Iteration 4158, Loss: 0.017425687983632088\n",
            "Iteration 4159, Loss: 0.015101843513548374\n",
            "Iteration 4160, Loss: 0.013900203630328178\n",
            "Iteration 4161, Loss: 0.01679924689233303\n",
            "Iteration 4162, Loss: 0.01610376313328743\n",
            "Iteration 4163, Loss: 0.013556504622101784\n",
            "Iteration 4164, Loss: 0.01478770561516285\n",
            "Iteration 4165, Loss: 0.014563462696969509\n",
            "Iteration 4166, Loss: 0.01111686322838068\n",
            "Iteration 4167, Loss: 0.014947478659451008\n",
            "Iteration 4168, Loss: 0.014492346905171871\n",
            "Iteration 4169, Loss: 0.01355000026524067\n",
            "Iteration 4170, Loss: 0.012388182803988457\n",
            "Iteration 4171, Loss: 0.016369737684726715\n",
            "Iteration 4172, Loss: 0.014224082231521606\n",
            "Iteration 4173, Loss: 0.014228945598006248\n",
            "Iteration 4174, Loss: 0.01763271912932396\n",
            "Iteration 4175, Loss: 0.016789771616458893\n",
            "Iteration 4176, Loss: 0.01695616915822029\n",
            "Iteration 4177, Loss: 0.01985728181898594\n",
            "Iteration 4178, Loss: 0.012020940892398357\n",
            "Iteration 4179, Loss: 0.01641864702105522\n",
            "Iteration 4180, Loss: 0.016399620100855827\n",
            "Iteration 4181, Loss: 0.012090531177818775\n",
            "Iteration 4182, Loss: 0.018189240247011185\n",
            "Iteration 4183, Loss: 0.010240942239761353\n",
            "Iteration 4184, Loss: 0.016166267916560173\n",
            "Iteration 4185, Loss: 0.014087213203310966\n",
            "Iteration 4186, Loss: 0.015256637707352638\n",
            "Iteration 4187, Loss: 0.019949976354837418\n",
            "Iteration 4188, Loss: 0.00998170766979456\n",
            "Iteration 4189, Loss: 0.012620411813259125\n",
            "Iteration 4190, Loss: 0.015957890078425407\n",
            "Iteration 4191, Loss: 0.014471062459051609\n",
            "Iteration 4192, Loss: 0.013013779185712337\n",
            "Iteration 4193, Loss: 0.01877044327557087\n",
            "Iteration 4194, Loss: 0.021298900246620178\n",
            "Iteration 4195, Loss: 0.014488080516457558\n",
            "Iteration 4196, Loss: 0.018534697592258453\n",
            "Iteration 4197, Loss: 0.014546800404787064\n",
            "Iteration 4198, Loss: 0.015685487538576126\n",
            "Iteration 4199, Loss: 0.014868076890707016\n",
            "Iteration 4200, Loss: 0.011858265846967697\n",
            "Iteration 4201, Loss: 0.01364688016474247\n",
            "Iteration 4202, Loss: 0.01803702302277088\n",
            "Iteration 4203, Loss: 0.024072395637631416\n",
            "Iteration 4204, Loss: 0.015296206809580326\n",
            "Iteration 4205, Loss: 0.01874203234910965\n",
            "Iteration 4206, Loss: 0.02039305865764618\n",
            "Iteration 4207, Loss: 0.01579178310930729\n",
            "Iteration 4208, Loss: 0.02079094760119915\n",
            "Iteration 4209, Loss: 0.016489842906594276\n",
            "Iteration 4210, Loss: 0.01939028687775135\n",
            "Iteration 4211, Loss: 0.011562119238078594\n",
            "Iteration 4212, Loss: 0.01368486788123846\n",
            "Iteration 4213, Loss: 0.015291445888578892\n",
            "Iteration 4214, Loss: 0.01622948981821537\n",
            "Iteration 4215, Loss: 0.014340763911604881\n",
            "Iteration 4216, Loss: 0.008903753943741322\n",
            "Iteration 4217, Loss: 0.018478969112038612\n",
            "Iteration 4218, Loss: 0.017461232841014862\n",
            "Iteration 4219, Loss: 0.01849828101694584\n",
            "Iteration 4220, Loss: 0.012514353729784489\n",
            "Iteration 4221, Loss: 0.016480769962072372\n",
            "Iteration 4222, Loss: 0.01788569614291191\n",
            "Iteration 4223, Loss: 0.018462611362338066\n",
            "Iteration 4224, Loss: 0.021005894988775253\n",
            "Iteration 4225, Loss: 0.01615092344582081\n",
            "Iteration 4226, Loss: 0.01910100132226944\n",
            "Iteration 4227, Loss: 0.025104019790887833\n",
            "Iteration 4228, Loss: 0.013766874559223652\n",
            "Iteration 4229, Loss: 0.01801951602101326\n",
            "Iteration 4230, Loss: 0.01785578764975071\n",
            "Iteration 4231, Loss: 0.00874082650989294\n",
            "Iteration 4232, Loss: 0.013812369666993618\n",
            "Iteration 4233, Loss: 0.017865898087620735\n",
            "Iteration 4234, Loss: 0.013133461587131023\n",
            "Iteration 4235, Loss: 0.01880122721195221\n",
            "Iteration 4236, Loss: 0.010863082483410835\n",
            "Iteration 4237, Loss: 0.014287933707237244\n",
            "Iteration 4238, Loss: 0.018501395359635353\n",
            "Iteration 4239, Loss: 0.018304213881492615\n",
            "Iteration 4240, Loss: 0.02322371117770672\n",
            "Iteration 4241, Loss: 0.01602374203503132\n",
            "Iteration 4242, Loss: 0.010423955507576466\n",
            "Iteration 4243, Loss: 0.02018124610185623\n",
            "Iteration 4244, Loss: 0.011429130099713802\n",
            "Iteration 4245, Loss: 0.013400507159531116\n",
            "Iteration 4246, Loss: 0.01516129169613123\n",
            "Iteration 4247, Loss: 0.01616617478430271\n",
            "Iteration 4248, Loss: 0.016251280903816223\n",
            "Iteration 4249, Loss: 0.015460132621228695\n",
            "Iteration 4250, Loss: 0.02370014227926731\n",
            "Iteration 4251, Loss: 0.019856572151184082\n",
            "Iteration 4252, Loss: 0.020287299528717995\n",
            "Iteration 4253, Loss: 0.014476493932306767\n",
            "Iteration 4254, Loss: 0.014831013977527618\n",
            "Iteration 4255, Loss: 0.019348809495568275\n",
            "Iteration 4256, Loss: 0.021781904622912407\n",
            "Iteration 4257, Loss: 0.01435963623225689\n",
            "Iteration 4258, Loss: 0.010134230367839336\n",
            "Iteration 4259, Loss: 0.017676297575235367\n",
            "Iteration 4260, Loss: 0.019747529178857803\n",
            "Iteration 4261, Loss: 0.01636677235364914\n",
            "Iteration 4262, Loss: 0.017421750351786613\n",
            "Iteration 4263, Loss: 0.013679447583854198\n",
            "Iteration 4264, Loss: 0.015352574177086353\n",
            "Iteration 4265, Loss: 0.02068357542157173\n",
            "Iteration 4266, Loss: 0.015625881031155586\n",
            "Iteration 4267, Loss: 0.014488562941551208\n",
            "Iteration 4268, Loss: 0.012521043419837952\n",
            "Iteration 4269, Loss: 0.017109360545873642\n",
            "Iteration 4270, Loss: 0.017341885715723038\n",
            "Iteration 4271, Loss: 0.009788835421204567\n",
            "Iteration 4272, Loss: 0.023059312254190445\n",
            "Iteration 4273, Loss: 0.021758465096354485\n",
            "Iteration 4274, Loss: 0.01934022456407547\n",
            "Iteration 4275, Loss: 0.018953917548060417\n",
            "Iteration 4276, Loss: 0.016476871445775032\n",
            "Iteration 4277, Loss: 0.017950735986232758\n",
            "Iteration 4278, Loss: 0.014891024678945541\n",
            "Iteration 4279, Loss: 0.016133032739162445\n",
            "Iteration 4280, Loss: 0.011695539578795433\n",
            "Iteration 4281, Loss: 0.011499852873384953\n",
            "Iteration 4282, Loss: 0.013909840025007725\n",
            "Iteration 4283, Loss: 0.015688112005591393\n",
            "Iteration 4284, Loss: 0.010123739019036293\n",
            "Iteration 4285, Loss: 0.020826254040002823\n",
            "Iteration 4286, Loss: 0.010643765330314636\n",
            "Iteration 4287, Loss: 0.009556666016578674\n",
            "Iteration 4288, Loss: 0.01770223118364811\n",
            "Iteration 4289, Loss: 0.016339121386408806\n",
            "Iteration 4290, Loss: 0.011137964203953743\n",
            "Iteration 4291, Loss: 0.016400134190917015\n",
            "Iteration 4292, Loss: 0.016788098961114883\n",
            "Iteration 4293, Loss: 0.012154011987149715\n",
            "Iteration 4294, Loss: 0.012934215366840363\n",
            "Iteration 4295, Loss: 0.014971008524298668\n",
            "Iteration 4296, Loss: 0.014534860849380493\n",
            "Iteration 4297, Loss: 0.011775239370763302\n",
            "Iteration 4298, Loss: 0.013210435397922993\n",
            "Iteration 4299, Loss: 0.016911977902054787\n",
            "Iteration 4300, Loss: 0.014856777153909206\n",
            "Iteration 4301, Loss: 0.020035825669765472\n",
            "Iteration 4302, Loss: 0.015931706875562668\n",
            "Iteration 4303, Loss: 0.013415683060884476\n",
            "Iteration 4304, Loss: 0.0178503580391407\n",
            "Iteration 4305, Loss: 0.01935586892068386\n",
            "Iteration 4306, Loss: 0.014410620555281639\n",
            "Iteration 4307, Loss: 0.02236872911453247\n",
            "Iteration 4308, Loss: 0.015458074398338795\n",
            "Iteration 4309, Loss: 0.013735884800553322\n",
            "Iteration 4310, Loss: 0.01884078048169613\n",
            "Iteration 4311, Loss: 0.01617448218166828\n",
            "Iteration 4312, Loss: 0.018535178154706955\n",
            "Iteration 4313, Loss: 0.014596972614526749\n",
            "Iteration 4314, Loss: 0.016085557639598846\n",
            "Iteration 4315, Loss: 0.01935882866382599\n",
            "Iteration 4316, Loss: 0.015932496637105942\n",
            "Iteration 4317, Loss: 0.023102186620235443\n",
            "Iteration 4318, Loss: 0.01369143184274435\n",
            "Iteration 4319, Loss: 0.01404297910630703\n",
            "Iteration 4320, Loss: 0.00973838847130537\n",
            "Iteration 4321, Loss: 0.007614986505359411\n",
            "Iteration 4322, Loss: 0.011549211107194424\n",
            "Iteration 4323, Loss: 0.024949362501502037\n",
            "Iteration 4324, Loss: 0.01642739027738571\n",
            "Iteration 4325, Loss: 0.013133389875292778\n",
            "Iteration 4326, Loss: 0.011280073784291744\n",
            "Iteration 4327, Loss: 0.015338904224336147\n",
            "Iteration 4328, Loss: 0.015150480903685093\n",
            "Iteration 4329, Loss: 0.01647062413394451\n",
            "Iteration 4330, Loss: 0.016734540462493896\n",
            "Iteration 4331, Loss: 0.011047031730413437\n",
            "Iteration 4332, Loss: 0.02021912857890129\n",
            "Iteration 4333, Loss: 0.013317116536200047\n",
            "Iteration 4334, Loss: 0.018517177551984787\n",
            "Iteration 4335, Loss: 0.014667166396975517\n",
            "Iteration 4336, Loss: 0.011139052920043468\n",
            "Iteration 4337, Loss: 0.015713196247816086\n",
            "Iteration 4338, Loss: 0.015404123812913895\n",
            "Iteration 4339, Loss: 0.014855852350592613\n",
            "Iteration 4340, Loss: 0.01639248989522457\n",
            "Iteration 4341, Loss: 0.013764545321464539\n",
            "Iteration 4342, Loss: 0.02406376227736473\n",
            "Iteration 4343, Loss: 0.014698081649839878\n",
            "Iteration 4344, Loss: 0.013425713405013084\n",
            "Iteration 4345, Loss: 0.017924567684531212\n",
            "Iteration 4346, Loss: 0.0080476775765419\n",
            "Iteration 4347, Loss: 0.014802016317844391\n",
            "Iteration 4348, Loss: 0.017871897667646408\n",
            "Iteration 4349, Loss: 0.016706978902220726\n",
            "Iteration 4350, Loss: 0.01135486364364624\n",
            "Iteration 4351, Loss: 0.019561154767870903\n",
            "Iteration 4352, Loss: 0.014044661074876785\n",
            "Iteration 4353, Loss: 0.0076235635206103325\n",
            "Iteration 4354, Loss: 0.01507321372628212\n",
            "Iteration 4355, Loss: 0.020791970193386078\n",
            "Iteration 4356, Loss: 0.015270553529262543\n",
            "Iteration 4357, Loss: 0.01455671526491642\n",
            "Iteration 4358, Loss: 0.013660352677106857\n",
            "Iteration 4359, Loss: 0.017407888546586037\n",
            "Iteration 4360, Loss: 0.0123185645788908\n",
            "Iteration 4361, Loss: 0.017464442178606987\n",
            "Iteration 4362, Loss: 0.009814264252781868\n",
            "Iteration 4363, Loss: 0.02583572454750538\n",
            "Iteration 4364, Loss: 0.00750572606921196\n",
            "Iteration 4365, Loss: 0.01639297977089882\n",
            "Iteration 4366, Loss: 0.013255924917757511\n",
            "Iteration 4367, Loss: 0.01881360076367855\n",
            "Iteration 4368, Loss: 0.014670642092823982\n",
            "Iteration 4369, Loss: 0.014183688908815384\n",
            "Iteration 4370, Loss: 0.013941572047770023\n",
            "Iteration 4371, Loss: 0.015442371368408203\n",
            "Iteration 4372, Loss: 0.013970539905130863\n",
            "Iteration 4373, Loss: 0.015649432316422462\n",
            "Iteration 4374, Loss: 0.01154517475515604\n",
            "Iteration 4375, Loss: 0.019470609724521637\n",
            "Iteration 4376, Loss: 0.016120905056595802\n",
            "Iteration 4377, Loss: 0.014041129499673843\n",
            "Iteration 4378, Loss: 0.013015232048928738\n",
            "Iteration 4379, Loss: 0.01662377640604973\n",
            "Iteration 4380, Loss: 0.011168804951012135\n",
            "Iteration 4381, Loss: 0.016393670812249184\n",
            "Iteration 4382, Loss: 0.010307048447430134\n",
            "Iteration 4383, Loss: 0.011637388728559017\n",
            "Iteration 4384, Loss: 0.015530155971646309\n",
            "Iteration 4385, Loss: 0.021271079778671265\n",
            "Iteration 4386, Loss: 0.011251164600253105\n",
            "Iteration 4387, Loss: 0.009677503257989883\n",
            "Iteration 4388, Loss: 0.016684945672750473\n",
            "Iteration 4389, Loss: 0.014820855110883713\n",
            "Iteration 4390, Loss: 0.01803801767528057\n",
            "Iteration 4391, Loss: 0.017370257526636124\n",
            "Iteration 4392, Loss: 0.010127962566912174\n",
            "Iteration 4393, Loss: 0.01470914389938116\n",
            "Iteration 4394, Loss: 0.021557694301009178\n",
            "Iteration 4395, Loss: 0.010439748875796795\n",
            "Iteration 4396, Loss: 0.019024286419153214\n",
            "Iteration 4397, Loss: 0.009295579977333546\n",
            "Iteration 4398, Loss: 0.01761615462601185\n",
            "Iteration 4399, Loss: 0.01591951586306095\n",
            "Iteration 4400, Loss: 0.01808996871113777\n",
            "Iteration 4401, Loss: 0.018392814323306084\n",
            "Iteration 4402, Loss: 0.01496724784374237\n",
            "Iteration 4403, Loss: 0.016807375475764275\n",
            "Iteration 4404, Loss: 0.015710346400737762\n",
            "Iteration 4405, Loss: 0.018979189917445183\n",
            "Iteration 4406, Loss: 0.013546743430197239\n",
            "Iteration 4407, Loss: 0.012083856388926506\n",
            "Iteration 4408, Loss: 0.02695988304913044\n",
            "Iteration 4409, Loss: 0.01660926826298237\n",
            "Iteration 4410, Loss: 0.01496817171573639\n",
            "Iteration 4411, Loss: 0.015718920156359673\n",
            "Iteration 4412, Loss: 0.01652471534907818\n",
            "Iteration 4413, Loss: 0.012081743218004704\n",
            "Iteration 4414, Loss: 0.015693319961428642\n",
            "Iteration 4415, Loss: 0.017332402989268303\n",
            "Iteration 4416, Loss: 0.0104518448933959\n",
            "Iteration 4417, Loss: 0.015106109902262688\n",
            "Iteration 4418, Loss: 0.0155007503926754\n",
            "Iteration 4419, Loss: 0.012870828621089458\n",
            "Iteration 4420, Loss: 0.015900738537311554\n",
            "Iteration 4421, Loss: 0.015932774171233177\n",
            "Iteration 4422, Loss: 0.014059231616556644\n",
            "Iteration 4423, Loss: 0.013128452934324741\n",
            "Iteration 4424, Loss: 0.015433077700436115\n",
            "Iteration 4425, Loss: 0.01447926927357912\n",
            "Iteration 4426, Loss: 0.01190247107297182\n",
            "Iteration 4427, Loss: 0.016849201172590256\n",
            "Iteration 4428, Loss: 0.02472435124218464\n",
            "Iteration 4429, Loss: 0.017361361533403397\n",
            "Iteration 4430, Loss: 0.021395638585090637\n",
            "Iteration 4431, Loss: 0.010706244967877865\n",
            "Iteration 4432, Loss: 0.01674400269985199\n",
            "Iteration 4433, Loss: 0.008278748020529747\n",
            "Iteration 4434, Loss: 0.017371736466884613\n",
            "Iteration 4435, Loss: 0.015190433710813522\n",
            "Iteration 4436, Loss: 0.014199800789356232\n",
            "Iteration 4437, Loss: 0.017946796491742134\n",
            "Iteration 4438, Loss: 0.011315804906189442\n",
            "Iteration 4439, Loss: 0.01326724886894226\n",
            "Iteration 4440, Loss: 0.02078256569802761\n",
            "Iteration 4441, Loss: 0.015509789809584618\n",
            "Iteration 4442, Loss: 0.010646882466971874\n",
            "Iteration 4443, Loss: 0.017309578135609627\n",
            "Iteration 4444, Loss: 0.02518334984779358\n",
            "Iteration 4445, Loss: 0.016963746398687363\n",
            "Iteration 4446, Loss: 0.014568053185939789\n",
            "Iteration 4447, Loss: 0.012417524121701717\n",
            "Iteration 4448, Loss: 0.008361637592315674\n",
            "Iteration 4449, Loss: 0.015287354588508606\n",
            "Iteration 4450, Loss: 0.009716997854411602\n",
            "Iteration 4451, Loss: 0.017714770510792732\n",
            "Iteration 4452, Loss: 0.017922047525644302\n",
            "Iteration 4453, Loss: 0.01488323975354433\n",
            "Iteration 4454, Loss: 0.021691860631108284\n",
            "Iteration 4455, Loss: 0.017385423183441162\n",
            "Iteration 4456, Loss: 0.011318881995975971\n",
            "Iteration 4457, Loss: 0.020447006449103355\n",
            "Iteration 4458, Loss: 0.017612319439649582\n",
            "Iteration 4459, Loss: 0.011777040548622608\n",
            "Iteration 4460, Loss: 0.015912743285298347\n",
            "Iteration 4461, Loss: 0.011848300695419312\n",
            "Iteration 4462, Loss: 0.018086601048707962\n",
            "Iteration 4463, Loss: 0.02223939076066017\n",
            "Iteration 4464, Loss: 0.009887171909213066\n",
            "Iteration 4465, Loss: 0.009464076720178127\n",
            "Iteration 4466, Loss: 0.01442178338766098\n",
            "Iteration 4467, Loss: 0.007467486895620823\n",
            "Iteration 4468, Loss: 0.01324898935854435\n",
            "Iteration 4469, Loss: 0.007158520631492138\n",
            "Iteration 4470, Loss: 0.015104249119758606\n",
            "Iteration 4471, Loss: 0.01669628731906414\n",
            "Iteration 4472, Loss: 0.020352927967905998\n",
            "Iteration 4473, Loss: 0.02069351077079773\n",
            "Iteration 4474, Loss: 0.008911442942917347\n",
            "Iteration 4475, Loss: 0.012762745842337608\n",
            "Iteration 4476, Loss: 0.011743105947971344\n",
            "Iteration 4477, Loss: 0.013785293325781822\n",
            "Iteration 4478, Loss: 0.0119089400395751\n",
            "Iteration 4479, Loss: 0.017001990228891373\n",
            "Iteration 4480, Loss: 0.017294708639383316\n",
            "Iteration 4481, Loss: 0.02006693370640278\n",
            "Iteration 4482, Loss: 0.016430368646979332\n",
            "Iteration 4483, Loss: 0.015057888813316822\n",
            "Iteration 4484, Loss: 0.01324803289026022\n",
            "Iteration 4485, Loss: 0.017054244875907898\n",
            "Iteration 4486, Loss: 0.017596419900655746\n",
            "Iteration 4487, Loss: 0.012938785366714\n",
            "Iteration 4488, Loss: 0.015176765620708466\n",
            "Iteration 4489, Loss: 0.012266105972230434\n",
            "Iteration 4490, Loss: 0.012502962723374367\n",
            "Iteration 4491, Loss: 0.014652899466454983\n",
            "Iteration 4492, Loss: 0.01772751472890377\n",
            "Iteration 4493, Loss: 0.0167032890021801\n",
            "Iteration 4494, Loss: 0.010056366212666035\n",
            "Iteration 4495, Loss: 0.018126701936125755\n",
            "Iteration 4496, Loss: 0.016665833070874214\n",
            "Iteration 4497, Loss: 0.01365844625979662\n",
            "Iteration 4498, Loss: 0.010387951508164406\n",
            "Iteration 4499, Loss: 0.02119508944451809\n",
            "Iteration 4500, Loss: 0.02281360886991024\n",
            "Iteration 4501, Loss: 0.008127707056701183\n",
            "Iteration 4502, Loss: 0.012215014547109604\n",
            "Iteration 4503, Loss: 0.01563429832458496\n",
            "Iteration 4504, Loss: 0.011060960590839386\n",
            "Iteration 4505, Loss: 0.015573717653751373\n",
            "Iteration 4506, Loss: 0.016131779178977013\n",
            "Iteration 4507, Loss: 0.008457907475531101\n",
            "Iteration 4508, Loss: 0.012291952967643738\n",
            "Iteration 4509, Loss: 0.016348958015441895\n",
            "Iteration 4510, Loss: 0.014923200942575932\n",
            "Iteration 4511, Loss: 0.020510436967015266\n",
            "Iteration 4512, Loss: 0.011689901351928711\n",
            "Iteration 4513, Loss: 0.016069237142801285\n",
            "Iteration 4514, Loss: 0.015923859551548958\n",
            "Iteration 4515, Loss: 0.022969046607613564\n",
            "Iteration 4516, Loss: 0.012669949792325497\n",
            "Iteration 4517, Loss: 0.012594594620168209\n",
            "Iteration 4518, Loss: 0.015455981716513634\n",
            "Iteration 4519, Loss: 0.02041473798453808\n",
            "Iteration 4520, Loss: 0.016842449083924294\n",
            "Iteration 4521, Loss: 0.013321450911462307\n",
            "Iteration 4522, Loss: 0.010568506084382534\n",
            "Iteration 4523, Loss: 0.010257764719426632\n",
            "Iteration 4524, Loss: 0.014790322631597519\n",
            "Iteration 4525, Loss: 0.014147922396659851\n",
            "Iteration 4526, Loss: 0.014467024244368076\n",
            "Iteration 4527, Loss: 0.01305376086384058\n",
            "Iteration 4528, Loss: 0.01770651713013649\n",
            "Iteration 4529, Loss: 0.018090810626745224\n",
            "Iteration 4530, Loss: 0.016927283257246017\n",
            "Iteration 4531, Loss: 0.018180152401328087\n",
            "Iteration 4532, Loss: 0.013940826989710331\n",
            "Iteration 4533, Loss: 0.012085827998816967\n",
            "Iteration 4534, Loss: 0.01753181964159012\n",
            "Iteration 4535, Loss: 0.01355782337486744\n",
            "Iteration 4536, Loss: 0.016588879749178886\n",
            "Iteration 4537, Loss: 0.014426528476178646\n",
            "Iteration 4538, Loss: 0.019461942836642265\n",
            "Iteration 4539, Loss: 0.020817527547478676\n",
            "Iteration 4540, Loss: 0.00818981509655714\n",
            "Iteration 4541, Loss: 0.01432332769036293\n",
            "Iteration 4542, Loss: 0.019428564235568047\n",
            "Iteration 4543, Loss: 0.007457399275153875\n",
            "Iteration 4544, Loss: 0.015626957640051842\n",
            "Iteration 4545, Loss: 0.02048402652144432\n",
            "Iteration 4546, Loss: 0.020224696025252342\n",
            "Iteration 4547, Loss: 0.0178386140614748\n",
            "Iteration 4548, Loss: 0.01536521501839161\n",
            "Iteration 4549, Loss: 0.021678511053323746\n",
            "Iteration 4550, Loss: 0.014884394593536854\n",
            "Iteration 4551, Loss: 0.015277029946446419\n",
            "Iteration 4552, Loss: 0.014815123751759529\n",
            "Iteration 4553, Loss: 0.0113878408446908\n",
            "Iteration 4554, Loss: 0.02113628201186657\n",
            "Iteration 4555, Loss: 0.01891249790787697\n",
            "Iteration 4556, Loss: 0.01433771476149559\n",
            "Iteration 4557, Loss: 0.013346277177333832\n",
            "Iteration 4558, Loss: 0.017191527411341667\n",
            "Iteration 4559, Loss: 0.008097524754703045\n",
            "Iteration 4560, Loss: 0.0153303612023592\n",
            "Iteration 4561, Loss: 0.021371351554989815\n",
            "Iteration 4562, Loss: 0.013078643009066582\n",
            "Iteration 4563, Loss: 0.012821207754313946\n",
            "Iteration 4564, Loss: 0.012030554935336113\n",
            "Iteration 4565, Loss: 0.020759718492627144\n",
            "Iteration 4566, Loss: 0.012469644658267498\n",
            "Iteration 4567, Loss: 0.012650902383029461\n",
            "Iteration 4568, Loss: 0.013943234458565712\n",
            "Iteration 4569, Loss: 0.02134842425584793\n",
            "Iteration 4570, Loss: 0.01967404969036579\n",
            "Iteration 4571, Loss: 0.011414190754294395\n",
            "Iteration 4572, Loss: 0.011410897597670555\n",
            "Iteration 4573, Loss: 0.016811249777674675\n",
            "Iteration 4574, Loss: 0.011762779206037521\n",
            "Iteration 4575, Loss: 0.023642010986804962\n",
            "Iteration 4576, Loss: 0.014025497250258923\n",
            "Iteration 4577, Loss: 0.012771247886121273\n",
            "Iteration 4578, Loss: 0.011167231947183609\n",
            "Iteration 4579, Loss: 0.01615186594426632\n",
            "Iteration 4580, Loss: 0.011554816737771034\n",
            "Iteration 4581, Loss: 0.007145571522414684\n",
            "Iteration 4582, Loss: 0.011161250062286854\n",
            "Iteration 4583, Loss: 0.015910787507891655\n",
            "Iteration 4584, Loss: 0.01149758230894804\n",
            "Iteration 4585, Loss: 0.014755732379853725\n",
            "Iteration 4586, Loss: 0.025352058932185173\n",
            "Iteration 4587, Loss: 0.014249942265450954\n",
            "Iteration 4588, Loss: 0.013052461668848991\n",
            "Iteration 4589, Loss: 0.01639324426651001\n",
            "Iteration 4590, Loss: 0.014172808267176151\n",
            "Iteration 4591, Loss: 0.016637127846479416\n",
            "Iteration 4592, Loss: 0.017200106754899025\n",
            "Iteration 4593, Loss: 0.013349097222089767\n",
            "Iteration 4594, Loss: 0.017776180058717728\n",
            "Iteration 4595, Loss: 0.013669336214661598\n",
            "Iteration 4596, Loss: 0.012415096163749695\n",
            "Iteration 4597, Loss: 0.01091083511710167\n",
            "Iteration 4598, Loss: 0.013405459932982922\n",
            "Iteration 4599, Loss: 0.012276813387870789\n",
            "Iteration 4600, Loss: 0.013161834329366684\n",
            "Iteration 4601, Loss: 0.01274146605283022\n",
            "Iteration 4602, Loss: 0.012508951127529144\n",
            "Iteration 4603, Loss: 0.01895170658826828\n",
            "Iteration 4604, Loss: 0.015076884999871254\n",
            "Iteration 4605, Loss: 0.016665013507008553\n",
            "Iteration 4606, Loss: 0.020152226090431213\n",
            "Iteration 4607, Loss: 0.01370495930314064\n",
            "Iteration 4608, Loss: 0.012521589174866676\n",
            "Iteration 4609, Loss: 0.013088593259453773\n",
            "Iteration 4610, Loss: 0.02235047146677971\n",
            "Iteration 4611, Loss: 0.012563313357532024\n",
            "Iteration 4612, Loss: 0.013245326466858387\n",
            "Iteration 4613, Loss: 0.013307535089552402\n",
            "Iteration 4614, Loss: 0.01476467028260231\n",
            "Iteration 4615, Loss: 0.01797199249267578\n",
            "Iteration 4616, Loss: 0.01775013841688633\n",
            "Iteration 4617, Loss: 0.019154347479343414\n",
            "Iteration 4618, Loss: 0.01313831191509962\n",
            "Iteration 4619, Loss: 0.02067675068974495\n",
            "Iteration 4620, Loss: 0.015121686272323132\n",
            "Iteration 4621, Loss: 0.017843928188085556\n",
            "Iteration 4622, Loss: 0.01825622282922268\n",
            "Iteration 4623, Loss: 0.015261993743479252\n",
            "Iteration 4624, Loss: 0.01709713228046894\n",
            "Iteration 4625, Loss: 0.01615959219634533\n",
            "Iteration 4626, Loss: 0.01376791950315237\n",
            "Iteration 4627, Loss: 0.00953281857073307\n",
            "Iteration 4628, Loss: 0.01686050184071064\n",
            "Iteration 4629, Loss: 0.019957510754466057\n",
            "Iteration 4630, Loss: 0.012098722159862518\n",
            "Iteration 4631, Loss: 0.016079973429441452\n",
            "Iteration 4632, Loss: 0.02112632803618908\n",
            "Iteration 4633, Loss: 0.018352195620536804\n",
            "Iteration 4634, Loss: 0.016107594594359398\n",
            "Iteration 4635, Loss: 0.012240903452038765\n",
            "Iteration 4636, Loss: 0.01661442220211029\n",
            "Iteration 4637, Loss: 0.012417023070156574\n",
            "Iteration 4638, Loss: 0.016449740156531334\n",
            "Iteration 4639, Loss: 0.015032434836030006\n",
            "Iteration 4640, Loss: 0.01889115199446678\n",
            "Iteration 4641, Loss: 0.02207799255847931\n",
            "Iteration 4642, Loss: 0.011958434246480465\n",
            "Iteration 4643, Loss: 0.013613251969218254\n",
            "Iteration 4644, Loss: 0.01675903983414173\n",
            "Iteration 4645, Loss: 0.021159879863262177\n",
            "Iteration 4646, Loss: 0.021955419331789017\n",
            "Iteration 4647, Loss: 0.015034621581435204\n",
            "Iteration 4648, Loss: 0.01671013794839382\n",
            "Iteration 4649, Loss: 0.019242793321609497\n",
            "Iteration 4650, Loss: 0.009960302151739597\n",
            "Iteration 4651, Loss: 0.013994491659104824\n",
            "Iteration 4652, Loss: 0.01167981419712305\n",
            "Iteration 4653, Loss: 0.015206466428935528\n",
            "Iteration 4654, Loss: 0.016755156219005585\n",
            "Iteration 4655, Loss: 0.022441478446125984\n",
            "Iteration 4656, Loss: 0.011238408274948597\n",
            "Iteration 4657, Loss: 0.0148122264072299\n",
            "Iteration 4658, Loss: 0.013970350846648216\n",
            "Iteration 4659, Loss: 0.018515855073928833\n",
            "Iteration 4660, Loss: 0.02151186391711235\n",
            "Iteration 4661, Loss: 0.014124644920229912\n",
            "Iteration 4662, Loss: 0.017121365293860435\n",
            "Iteration 4663, Loss: 0.016400150954723358\n",
            "Iteration 4664, Loss: 0.017346397042274475\n",
            "Iteration 4665, Loss: 0.018163979053497314\n",
            "Iteration 4666, Loss: 0.014504116959869862\n",
            "Iteration 4667, Loss: 0.0212734192609787\n",
            "Iteration 4668, Loss: 0.012745439074933529\n",
            "Iteration 4669, Loss: 0.014674792066216469\n",
            "Iteration 4670, Loss: 0.022901246324181557\n",
            "Iteration 4671, Loss: 0.016875943168997765\n",
            "Iteration 4672, Loss: 0.023305796086788177\n",
            "Iteration 4673, Loss: 0.01008288562297821\n",
            "Iteration 4674, Loss: 0.023068416863679886\n",
            "Iteration 4675, Loss: 0.01283404789865017\n",
            "Iteration 4676, Loss: 0.016910770907998085\n",
            "Iteration 4677, Loss: 0.0172493364661932\n",
            "Iteration 4678, Loss: 0.02230282686650753\n",
            "Iteration 4679, Loss: 0.018090257421135902\n",
            "Iteration 4680, Loss: 0.018828732892870903\n",
            "Iteration 4681, Loss: 0.015653571113944054\n",
            "Iteration 4682, Loss: 0.013942726887762547\n",
            "Iteration 4683, Loss: 0.01014823280274868\n",
            "Iteration 4684, Loss: 0.01753624714910984\n",
            "Iteration 4685, Loss: 0.019099224358797073\n",
            "Iteration 4686, Loss: 0.012235063128173351\n",
            "Iteration 4687, Loss: 0.010021577589213848\n",
            "Iteration 4688, Loss: 0.01463901624083519\n",
            "Iteration 4689, Loss: 0.018370995298027992\n",
            "Iteration 4690, Loss: 0.012817416340112686\n",
            "Iteration 4691, Loss: 0.018259327858686447\n",
            "Iteration 4692, Loss: 0.013058668933808804\n",
            "Iteration 4693, Loss: 0.012833703309297562\n",
            "Iteration 4694, Loss: 0.011507580056786537\n",
            "Iteration 4695, Loss: 0.015333862975239754\n",
            "Iteration 4696, Loss: 0.01887078396975994\n",
            "Iteration 4697, Loss: 0.015889972448349\n",
            "Iteration 4698, Loss: 0.013629837892949581\n",
            "Iteration 4699, Loss: 0.015506231226027012\n",
            "Iteration 4700, Loss: 0.01453135721385479\n",
            "Iteration 4701, Loss: 0.012301809154450893\n",
            "Iteration 4702, Loss: 0.017032446339726448\n",
            "Iteration 4703, Loss: 0.011642402969300747\n",
            "Iteration 4704, Loss: 0.012897809036076069\n",
            "Iteration 4705, Loss: 0.00930451788008213\n",
            "Iteration 4706, Loss: 0.019681541249155998\n",
            "Iteration 4707, Loss: 0.01569143682718277\n",
            "Iteration 4708, Loss: 0.02067050337791443\n",
            "Iteration 4709, Loss: 0.013358245603740215\n",
            "Iteration 4710, Loss: 0.01197598222643137\n",
            "Iteration 4711, Loss: 0.012149602174758911\n",
            "Iteration 4712, Loss: 0.01473924145102501\n",
            "Iteration 4713, Loss: 0.008528980426490307\n",
            "Iteration 4714, Loss: 0.021027913317084312\n",
            "Iteration 4715, Loss: 0.019201630726456642\n",
            "Iteration 4716, Loss: 0.015194959007203579\n",
            "Iteration 4717, Loss: 0.01231675036251545\n",
            "Iteration 4718, Loss: 0.01778551936149597\n",
            "Iteration 4719, Loss: 0.016141029074788094\n",
            "Iteration 4720, Loss: 0.009891008958220482\n",
            "Iteration 4721, Loss: 0.01736767776310444\n",
            "Iteration 4722, Loss: 0.013648280873894691\n",
            "Iteration 4723, Loss: 0.01242523081600666\n",
            "Iteration 4724, Loss: 0.01224698405712843\n",
            "Iteration 4725, Loss: 0.012150903232395649\n",
            "Iteration 4726, Loss: 0.01058487594127655\n",
            "Iteration 4727, Loss: 0.020118923857808113\n",
            "Iteration 4728, Loss: 0.02156136743724346\n",
            "Iteration 4729, Loss: 0.013199313543736935\n",
            "Iteration 4730, Loss: 0.013522041030228138\n",
            "Iteration 4731, Loss: 0.012635932303965092\n",
            "Iteration 4732, Loss: 0.010559399612247944\n",
            "Iteration 4733, Loss: 0.009812731295824051\n",
            "Iteration 4734, Loss: 0.01393039245158434\n",
            "Iteration 4735, Loss: 0.019039394333958626\n",
            "Iteration 4736, Loss: 0.011130141094326973\n",
            "Iteration 4737, Loss: 0.014018500223755836\n",
            "Iteration 4738, Loss: 0.01129613071680069\n",
            "Iteration 4739, Loss: 0.014993859454989433\n",
            "Iteration 4740, Loss: 0.014150380156934261\n",
            "Iteration 4741, Loss: 0.013853071257472038\n",
            "Iteration 4742, Loss: 0.016614705324172974\n",
            "Iteration 4743, Loss: 0.01209948305040598\n",
            "Iteration 4744, Loss: 0.013340754434466362\n",
            "Iteration 4745, Loss: 0.01479597482830286\n",
            "Iteration 4746, Loss: 0.01145823486149311\n",
            "Iteration 4747, Loss: 0.0217514019459486\n",
            "Iteration 4748, Loss: 0.014827853068709373\n",
            "Iteration 4749, Loss: 0.016568927094340324\n",
            "Iteration 4750, Loss: 0.01897147297859192\n",
            "Iteration 4751, Loss: 0.014380100183188915\n",
            "Iteration 4752, Loss: 0.014668827876448631\n",
            "Iteration 4753, Loss: 0.010365759953856468\n",
            "Iteration 4754, Loss: 0.01900196075439453\n",
            "Iteration 4755, Loss: 0.012861107476055622\n",
            "Iteration 4756, Loss: 0.018114382401108742\n",
            "Iteration 4757, Loss: 0.016329819336533546\n",
            "Iteration 4758, Loss: 0.014303243719041348\n",
            "Iteration 4759, Loss: 0.015044878236949444\n",
            "Iteration 4760, Loss: 0.011735140345990658\n",
            "Iteration 4761, Loss: 0.01321436371654272\n",
            "Iteration 4762, Loss: 0.012641356326639652\n",
            "Iteration 4763, Loss: 0.011086310259997845\n",
            "Iteration 4764, Loss: 0.011916371062397957\n",
            "Iteration 4765, Loss: 0.011500894092023373\n",
            "Iteration 4766, Loss: 0.01336971390992403\n",
            "Iteration 4767, Loss: 0.016154427081346512\n",
            "Iteration 4768, Loss: 0.017955444753170013\n",
            "Iteration 4769, Loss: 0.016115106642246246\n",
            "Iteration 4770, Loss: 0.010540562681853771\n",
            "Iteration 4771, Loss: 0.018648475408554077\n",
            "Iteration 4772, Loss: 0.009708158671855927\n",
            "Iteration 4773, Loss: 0.008467402309179306\n",
            "Iteration 4774, Loss: 0.010106923058629036\n",
            "Iteration 4775, Loss: 0.011729106307029724\n",
            "Iteration 4776, Loss: 0.011961680836975574\n",
            "Iteration 4777, Loss: 0.024125127121806145\n",
            "Iteration 4778, Loss: 0.02297374978661537\n",
            "Iteration 4779, Loss: 0.014797093346714973\n",
            "Iteration 4780, Loss: 0.010723628103733063\n",
            "Iteration 4781, Loss: 0.01602213829755783\n",
            "Iteration 4782, Loss: 0.016619879752397537\n",
            "Iteration 4783, Loss: 0.01742842234671116\n",
            "Iteration 4784, Loss: 0.010499183088541031\n",
            "Iteration 4785, Loss: 0.022039610892534256\n",
            "Iteration 4786, Loss: 0.013484306633472443\n",
            "Iteration 4787, Loss: 0.01101433765143156\n",
            "Iteration 4788, Loss: 0.01746026612818241\n",
            "Iteration 4789, Loss: 0.022366313263773918\n",
            "Iteration 4790, Loss: 0.015314861200749874\n",
            "Iteration 4791, Loss: 0.023010436445474625\n",
            "Iteration 4792, Loss: 0.013118538074195385\n",
            "Iteration 4793, Loss: 0.008919366635382175\n",
            "Iteration 4794, Loss: 0.020030692219734192\n",
            "Iteration 4795, Loss: 0.01416655071079731\n",
            "Iteration 4796, Loss: 0.01886415295302868\n",
            "Iteration 4797, Loss: 0.020115695893764496\n",
            "Iteration 4798, Loss: 0.01754896715283394\n",
            "Iteration 4799, Loss: 0.009553791955113411\n",
            "Iteration 4800, Loss: 0.01741993986070156\n",
            "Iteration 4801, Loss: 0.013974699191749096\n",
            "Iteration 4802, Loss: 0.015235349535942078\n",
            "Iteration 4803, Loss: 0.014645922929048538\n",
            "Iteration 4804, Loss: 0.019621774554252625\n",
            "Iteration 4805, Loss: 0.014728807844221592\n",
            "Iteration 4806, Loss: 0.011861270293593407\n",
            "Iteration 4807, Loss: 0.017172858119010925\n",
            "Iteration 4808, Loss: 0.010379754938185215\n",
            "Iteration 4809, Loss: 0.019240548834204674\n",
            "Iteration 4810, Loss: 0.012557581067085266\n",
            "Iteration 4811, Loss: 0.01962423138320446\n",
            "Iteration 4812, Loss: 0.011128399521112442\n",
            "Iteration 4813, Loss: 0.018716931343078613\n",
            "Iteration 4814, Loss: 0.019864380359649658\n",
            "Iteration 4815, Loss: 0.013624783605337143\n",
            "Iteration 4816, Loss: 0.017054656520485878\n",
            "Iteration 4817, Loss: 0.0182476956397295\n",
            "Iteration 4818, Loss: 0.017740217968821526\n",
            "Iteration 4819, Loss: 0.013317172415554523\n",
            "Iteration 4820, Loss: 0.016815032809972763\n",
            "Iteration 4821, Loss: 0.014920457266271114\n",
            "Iteration 4822, Loss: 0.01771865040063858\n",
            "Iteration 4823, Loss: 0.015015880577266216\n",
            "Iteration 4824, Loss: 0.008966008201241493\n",
            "Iteration 4825, Loss: 0.013499345630407333\n",
            "Iteration 4826, Loss: 0.012491278350353241\n",
            "Iteration 4827, Loss: 0.010935964994132519\n",
            "Iteration 4828, Loss: 0.012035421095788479\n",
            "Iteration 4829, Loss: 0.01782989129424095\n",
            "Iteration 4830, Loss: 0.013316656462848186\n",
            "Iteration 4831, Loss: 0.012980022467672825\n",
            "Iteration 4832, Loss: 0.01966596208512783\n",
            "Iteration 4833, Loss: 0.01229479257017374\n",
            "Iteration 4834, Loss: 0.009076264686882496\n",
            "Iteration 4835, Loss: 0.013366286642849445\n",
            "Iteration 4836, Loss: 0.020284350961446762\n",
            "Iteration 4837, Loss: 0.014415592886507511\n",
            "Iteration 4838, Loss: 0.014178385958075523\n",
            "Iteration 4839, Loss: 0.016036123037338257\n",
            "Iteration 4840, Loss: 0.0230682585388422\n",
            "Iteration 4841, Loss: 0.01263383124023676\n",
            "Iteration 4842, Loss: 0.020846163854002953\n",
            "Iteration 4843, Loss: 0.011313986033201218\n",
            "Iteration 4844, Loss: 0.008319327607750893\n",
            "Iteration 4845, Loss: 0.018546421080827713\n",
            "Iteration 4846, Loss: 0.011242641136050224\n",
            "Iteration 4847, Loss: 0.00989737082272768\n",
            "Iteration 4848, Loss: 0.012191965244710445\n",
            "Iteration 4849, Loss: 0.0163386519998312\n",
            "Iteration 4850, Loss: 0.01363828033208847\n",
            "Iteration 4851, Loss: 0.013396906666457653\n",
            "Iteration 4852, Loss: 0.011179332621395588\n",
            "Iteration 4853, Loss: 0.015645546838641167\n",
            "Iteration 4854, Loss: 0.015276089310646057\n",
            "Iteration 4855, Loss: 0.010855160653591156\n",
            "Iteration 4856, Loss: 0.015133947134017944\n",
            "Iteration 4857, Loss: 0.018135102465748787\n",
            "Iteration 4858, Loss: 0.022279612720012665\n",
            "Iteration 4859, Loss: 0.012121409177780151\n",
            "Iteration 4860, Loss: 0.01932690665125847\n",
            "Iteration 4861, Loss: 0.013536510057747364\n",
            "Iteration 4862, Loss: 0.01814200170338154\n",
            "Iteration 4863, Loss: 0.019401079043745995\n",
            "Iteration 4864, Loss: 0.01887374185025692\n",
            "Iteration 4865, Loss: 0.019213922321796417\n",
            "Iteration 4866, Loss: 0.0132913114503026\n",
            "Iteration 4867, Loss: 0.012486888095736504\n",
            "Iteration 4868, Loss: 0.010519750416278839\n",
            "Iteration 4869, Loss: 0.01517696212977171\n",
            "Iteration 4870, Loss: 0.013942609541118145\n",
            "Iteration 4871, Loss: 0.0199341531842947\n",
            "Iteration 4872, Loss: 0.010471695102751255\n",
            "Iteration 4873, Loss: 0.016984665766358376\n",
            "Iteration 4874, Loss: 0.014512126334011555\n",
            "Iteration 4875, Loss: 0.01158394105732441\n",
            "Iteration 4876, Loss: 0.013336440548300743\n",
            "Iteration 4877, Loss: 0.014541542157530785\n",
            "Iteration 4878, Loss: 0.01562000997364521\n",
            "Iteration 4879, Loss: 0.012825711630284786\n",
            "Iteration 4880, Loss: 0.015231689438223839\n",
            "Iteration 4881, Loss: 0.0064997561275959015\n",
            "Iteration 4882, Loss: 0.017443589866161346\n",
            "Iteration 4883, Loss: 0.010795357637107372\n",
            "Iteration 4884, Loss: 0.010895024053752422\n",
            "Iteration 4885, Loss: 0.012846669182181358\n",
            "Iteration 4886, Loss: 0.01296982355415821\n",
            "Iteration 4887, Loss: 0.013817832805216312\n",
            "Iteration 4888, Loss: 0.014309429563581944\n",
            "Iteration 4889, Loss: 0.011706432327628136\n",
            "Iteration 4890, Loss: 0.014599379152059555\n",
            "Iteration 4891, Loss: 0.01591620035469532\n",
            "Iteration 4892, Loss: 0.014696597121655941\n",
            "Iteration 4893, Loss: 0.015274533070623875\n",
            "Iteration 4894, Loss: 0.01983516663312912\n",
            "Iteration 4895, Loss: 0.019182266667485237\n",
            "Iteration 4896, Loss: 0.01694752275943756\n",
            "Iteration 4897, Loss: 0.016931597143411636\n",
            "Iteration 4898, Loss: 0.013222861103713512\n",
            "Iteration 4899, Loss: 0.014342579059302807\n",
            "Iteration 4900, Loss: 0.015022965148091316\n",
            "Iteration 4901, Loss: 0.012771766632795334\n",
            "Iteration 4902, Loss: 0.010545303113758564\n",
            "Iteration 4903, Loss: 0.013282244093716145\n",
            "Iteration 4904, Loss: 0.018364811316132545\n",
            "Iteration 4905, Loss: 0.011243777349591255\n",
            "Iteration 4906, Loss: 0.007504834793508053\n",
            "Iteration 4907, Loss: 0.011939533054828644\n",
            "Iteration 4908, Loss: 0.012044042348861694\n",
            "Iteration 4909, Loss: 0.021059462800621986\n",
            "Iteration 4910, Loss: 0.01212618499994278\n",
            "Iteration 4911, Loss: 0.010666923597455025\n",
            "Iteration 4912, Loss: 0.01777133345603943\n",
            "Iteration 4913, Loss: 0.012816069647669792\n",
            "Iteration 4914, Loss: 0.012516817077994347\n",
            "Iteration 4915, Loss: 0.016692109405994415\n",
            "Iteration 4916, Loss: 0.014548247680068016\n",
            "Iteration 4917, Loss: 0.011230604723095894\n",
            "Iteration 4918, Loss: 0.01333154458552599\n",
            "Iteration 4919, Loss: 0.015298983082175255\n",
            "Iteration 4920, Loss: 0.018802491948008537\n",
            "Iteration 4921, Loss: 0.013427340425550938\n",
            "Iteration 4922, Loss: 0.011894617229700089\n",
            "Iteration 4923, Loss: 0.009203536435961723\n",
            "Iteration 4924, Loss: 0.016567979007959366\n",
            "Iteration 4925, Loss: 0.014462443068623543\n",
            "Iteration 4926, Loss: 0.01571814715862274\n",
            "Iteration 4927, Loss: 0.01823180727660656\n",
            "Iteration 4928, Loss: 0.010813631117343903\n",
            "Iteration 4929, Loss: 0.016448987647891045\n",
            "Iteration 4930, Loss: 0.014977362006902695\n",
            "Iteration 4931, Loss: 0.016080811619758606\n",
            "Iteration 4932, Loss: 0.011363222263753414\n",
            "Iteration 4933, Loss: 0.008177628740668297\n",
            "Iteration 4934, Loss: 0.013312072493135929\n",
            "Iteration 4935, Loss: 0.011530225165188313\n",
            "Iteration 4936, Loss: 0.019970566034317017\n",
            "Iteration 4937, Loss: 0.014367297291755676\n",
            "Iteration 4938, Loss: 0.012764143757522106\n",
            "Iteration 4939, Loss: 0.019956594333052635\n",
            "Iteration 4940, Loss: 0.02137969620525837\n",
            "Iteration 4941, Loss: 0.017246440052986145\n",
            "Iteration 4942, Loss: 0.013908814638853073\n",
            "Iteration 4943, Loss: 0.02109966054558754\n",
            "Iteration 4944, Loss: 0.012733075767755508\n",
            "Iteration 4945, Loss: 0.009351816028356552\n",
            "Iteration 4946, Loss: 0.013290940783917904\n",
            "Iteration 4947, Loss: 0.01576124131679535\n",
            "Iteration 4948, Loss: 0.015602586790919304\n",
            "Iteration 4949, Loss: 0.014289828017354012\n",
            "Iteration 4950, Loss: 0.012937135994434357\n",
            "Iteration 4951, Loss: 0.0157601498067379\n",
            "Iteration 4952, Loss: 0.017367752268910408\n",
            "Iteration 4953, Loss: 0.012902291491627693\n",
            "Iteration 4954, Loss: 0.01226924080401659\n",
            "Iteration 4955, Loss: 0.020798340439796448\n",
            "Iteration 4956, Loss: 0.011190583929419518\n",
            "Iteration 4957, Loss: 0.018716515973210335\n",
            "Iteration 4958, Loss: 0.02134762704372406\n",
            "Iteration 4959, Loss: 0.016629664227366447\n",
            "Iteration 4960, Loss: 0.00797534454613924\n",
            "Iteration 4961, Loss: 0.014232881367206573\n",
            "Iteration 4962, Loss: 0.0181964710354805\n",
            "Iteration 4963, Loss: 0.017725108191370964\n",
            "Iteration 4964, Loss: 0.015253688208758831\n",
            "Iteration 4965, Loss: 0.011996051296591759\n",
            "Iteration 4966, Loss: 0.014813309535384178\n",
            "Iteration 4967, Loss: 0.010884123854339123\n",
            "Iteration 4968, Loss: 0.020318428054451942\n",
            "Iteration 4969, Loss: 0.010340634733438492\n",
            "Iteration 4970, Loss: 0.017395587638020515\n",
            "Iteration 4971, Loss: 0.015624887309968472\n",
            "Iteration 4972, Loss: 0.012127001769840717\n",
            "Iteration 4973, Loss: 0.016545407474040985\n",
            "Iteration 4974, Loss: 0.018199922516942024\n",
            "Iteration 4975, Loss: 0.0139729343354702\n",
            "Iteration 4976, Loss: 0.01433645747601986\n",
            "Iteration 4977, Loss: 0.012765930034220219\n",
            "Iteration 4978, Loss: 0.022138092666864395\n",
            "Iteration 4979, Loss: 0.012403775006532669\n",
            "Iteration 4980, Loss: 0.013454103842377663\n",
            "Iteration 4981, Loss: 0.017439739778637886\n",
            "Iteration 4982, Loss: 0.011567838490009308\n",
            "Iteration 4983, Loss: 0.014301477000117302\n",
            "Iteration 4984, Loss: 0.012146970257163048\n",
            "Iteration 4985, Loss: 0.009320897981524467\n",
            "Iteration 4986, Loss: 0.01732371747493744\n",
            "Iteration 4987, Loss: 0.009689283557236195\n",
            "Iteration 4988, Loss: 0.022259047254920006\n",
            "Iteration 4989, Loss: 0.020585207268595695\n",
            "Iteration 4990, Loss: 0.018026545643806458\n",
            "Iteration 4991, Loss: 0.019930584356188774\n",
            "Iteration 4992, Loss: 0.015767777338624\n",
            "Iteration 4993, Loss: 0.008420358411967754\n",
            "Iteration 4994, Loss: 0.013971114531159401\n",
            "Iteration 4995, Loss: 0.019811654463410378\n",
            "Iteration 4996, Loss: 0.014889501966536045\n",
            "Iteration 4997, Loss: 0.011579688638448715\n",
            "Iteration 4998, Loss: 0.010180439800024033\n",
            "Iteration 4999, Loss: 0.019791632890701294\n",
            "Iteration 5000, Loss: 0.013241762295365334\n",
            "Test Loss: 0.05257934331893921\n",
            "Iteration 5001, Loss: 0.014203812927007675\n",
            "Iteration 5002, Loss: 0.013769973069429398\n",
            "Iteration 5003, Loss: 0.012451324611902237\n",
            "Iteration 5004, Loss: 0.011197652667760849\n",
            "Iteration 5005, Loss: 0.010102125816047192\n",
            "Iteration 5006, Loss: 0.017907269299030304\n",
            "Iteration 5007, Loss: 0.01667124032974243\n",
            "Iteration 5008, Loss: 0.013624786399304867\n",
            "Iteration 5009, Loss: 0.01042239274829626\n",
            "Iteration 5010, Loss: 0.011338387615978718\n",
            "Iteration 5011, Loss: 0.013302845880389214\n",
            "Iteration 5012, Loss: 0.01986519619822502\n",
            "Iteration 5013, Loss: 0.016891270875930786\n",
            "Iteration 5014, Loss: 0.009983942843973637\n",
            "Iteration 5015, Loss: 0.023190101608633995\n",
            "Iteration 5016, Loss: 0.010932614095509052\n",
            "Iteration 5017, Loss: 0.013148248195648193\n",
            "Iteration 5018, Loss: 0.01683209463953972\n",
            "Iteration 5019, Loss: 0.01926644891500473\n",
            "Iteration 5020, Loss: 0.013719158247113228\n",
            "Iteration 5021, Loss: 0.013063650578260422\n",
            "Iteration 5022, Loss: 0.01262210588902235\n",
            "Iteration 5023, Loss: 0.015482357703149319\n",
            "Iteration 5024, Loss: 0.010559538379311562\n",
            "Iteration 5025, Loss: 0.024178601801395416\n",
            "Iteration 5026, Loss: 0.01589272730052471\n",
            "Iteration 5027, Loss: 0.026304569095373154\n",
            "Iteration 5028, Loss: 0.015920408070087433\n",
            "Iteration 5029, Loss: 0.018029101192951202\n",
            "Iteration 5030, Loss: 0.013779531233012676\n",
            "Iteration 5031, Loss: 0.014876002445816994\n",
            "Iteration 5032, Loss: 0.014376023784279823\n",
            "Iteration 5033, Loss: 0.01345259789377451\n",
            "Iteration 5034, Loss: 0.01341944932937622\n",
            "Iteration 5035, Loss: 0.00989492330700159\n",
            "Iteration 5036, Loss: 0.012143810279667377\n",
            "Iteration 5037, Loss: 0.014661288820207119\n",
            "Iteration 5038, Loss: 0.011265384033322334\n",
            "Iteration 5039, Loss: 0.008139943704009056\n",
            "Iteration 5040, Loss: 0.013057641685009003\n",
            "Iteration 5041, Loss: 0.012475804425776005\n",
            "Iteration 5042, Loss: 0.01543563511222601\n",
            "Iteration 5043, Loss: 0.011052625253796577\n",
            "Iteration 5044, Loss: 0.018098922446370125\n",
            "Iteration 5045, Loss: 0.015798170119524002\n",
            "Iteration 5046, Loss: 0.022376911714673042\n",
            "Iteration 5047, Loss: 0.01648789457976818\n",
            "Iteration 5048, Loss: 0.013158121146261692\n",
            "Iteration 5049, Loss: 0.017625540494918823\n",
            "Iteration 5050, Loss: 0.020143266767263412\n",
            "Iteration 5051, Loss: 0.019100302830338478\n",
            "Iteration 5052, Loss: 0.014863451942801476\n",
            "Iteration 5053, Loss: 0.013552293181419373\n",
            "Iteration 5054, Loss: 0.015744483098387718\n",
            "Iteration 5055, Loss: 0.01467014942318201\n",
            "Iteration 5056, Loss: 0.017184020951390266\n",
            "Iteration 5057, Loss: 0.017106235027313232\n",
            "Iteration 5058, Loss: 0.01602175459265709\n",
            "Iteration 5059, Loss: 0.007742228452116251\n",
            "Iteration 5060, Loss: 0.011420922353863716\n",
            "Iteration 5061, Loss: 0.010722915641963482\n",
            "Iteration 5062, Loss: 0.012408950366079807\n",
            "Iteration 5063, Loss: 0.017510851845145226\n",
            "Iteration 5064, Loss: 0.009832206182181835\n",
            "Iteration 5065, Loss: 0.013203434646129608\n",
            "Iteration 5066, Loss: 0.01602824032306671\n",
            "Iteration 5067, Loss: 0.01591971330344677\n",
            "Iteration 5068, Loss: 0.009029163047671318\n",
            "Iteration 5069, Loss: 0.014666728675365448\n",
            "Iteration 5070, Loss: 0.011244184337556362\n",
            "Iteration 5071, Loss: 0.013939117081463337\n",
            "Iteration 5072, Loss: 0.0067945984192192554\n",
            "Iteration 5073, Loss: 0.016565775498747826\n",
            "Iteration 5074, Loss: 0.01196928508579731\n",
            "Iteration 5075, Loss: 0.013757162727415562\n",
            "Iteration 5076, Loss: 0.012853101827204227\n",
            "Iteration 5077, Loss: 0.014702274464070797\n",
            "Iteration 5078, Loss: 0.010792852379381657\n",
            "Iteration 5079, Loss: 0.011642677709460258\n",
            "Iteration 5080, Loss: 0.014934117905795574\n",
            "Iteration 5081, Loss: 0.011073263362050056\n",
            "Iteration 5082, Loss: 0.01057833805680275\n",
            "Iteration 5083, Loss: 0.020912617444992065\n",
            "Iteration 5084, Loss: 0.013590792194008827\n",
            "Iteration 5085, Loss: 0.01455727219581604\n",
            "Iteration 5086, Loss: 0.009371640160679817\n",
            "Iteration 5087, Loss: 0.013162865303456783\n",
            "Iteration 5088, Loss: 0.019275778904557228\n",
            "Iteration 5089, Loss: 0.0158015675842762\n",
            "Iteration 5090, Loss: 0.019315075129270554\n",
            "Iteration 5091, Loss: 0.012014933861792088\n",
            "Iteration 5092, Loss: 0.013518673367798328\n",
            "Iteration 5093, Loss: 0.021458113566040993\n",
            "Iteration 5094, Loss: 0.015560702420771122\n",
            "Iteration 5095, Loss: 0.012663311325013638\n",
            "Iteration 5096, Loss: 0.01288478635251522\n",
            "Iteration 5097, Loss: 0.016795756295323372\n",
            "Iteration 5098, Loss: 0.013840782456099987\n",
            "Iteration 5099, Loss: 0.016106339171528816\n",
            "Iteration 5100, Loss: 0.01162668690085411\n",
            "Iteration 5101, Loss: 0.010751686058938503\n",
            "Iteration 5102, Loss: 0.015005765482783318\n",
            "Iteration 5103, Loss: 0.00788514781743288\n",
            "Iteration 5104, Loss: 0.016123931854963303\n",
            "Iteration 5105, Loss: 0.021996092051267624\n",
            "Iteration 5106, Loss: 0.019584618508815765\n",
            "Iteration 5107, Loss: 0.018834426999092102\n",
            "Iteration 5108, Loss: 0.014532155357301235\n",
            "Iteration 5109, Loss: 0.016439164057374\n",
            "Iteration 5110, Loss: 0.013076226226985455\n",
            "Iteration 5111, Loss: 0.015649273991584778\n",
            "Iteration 5112, Loss: 0.013349183835089207\n",
            "Iteration 5113, Loss: 0.007259330712258816\n",
            "Iteration 5114, Loss: 0.01450748648494482\n",
            "Iteration 5115, Loss: 0.021713795140385628\n",
            "Iteration 5116, Loss: 0.01166711188852787\n",
            "Iteration 5117, Loss: 0.012472470290958881\n",
            "Iteration 5118, Loss: 0.012081381864845753\n",
            "Iteration 5119, Loss: 0.011915137059986591\n",
            "Iteration 5120, Loss: 0.014598555862903595\n",
            "Iteration 5121, Loss: 0.01262563094496727\n",
            "Iteration 5122, Loss: 0.017380209639668465\n",
            "Iteration 5123, Loss: 0.018803196027874947\n",
            "Iteration 5124, Loss: 0.01514387782663107\n",
            "Iteration 5125, Loss: 0.01318156998604536\n",
            "Iteration 5126, Loss: 0.007409530691802502\n",
            "Iteration 5127, Loss: 0.007771947421133518\n",
            "Iteration 5128, Loss: 0.019879277795553207\n",
            "Iteration 5129, Loss: 0.020440541207790375\n",
            "Iteration 5130, Loss: 0.01668735221028328\n",
            "Iteration 5131, Loss: 0.008687145076692104\n",
            "Iteration 5132, Loss: 0.017973119392991066\n",
            "Iteration 5133, Loss: 0.013467442244291306\n",
            "Iteration 5134, Loss: 0.014192385599017143\n",
            "Iteration 5135, Loss: 0.01735592447221279\n",
            "Iteration 5136, Loss: 0.01878613606095314\n",
            "Iteration 5137, Loss: 0.016127489507198334\n",
            "Iteration 5138, Loss: 0.013524053618311882\n",
            "Iteration 5139, Loss: 0.012585150077939034\n",
            "Iteration 5140, Loss: 0.018710428848862648\n",
            "Iteration 5141, Loss: 0.007907858118414879\n",
            "Iteration 5142, Loss: 0.011163067072629929\n",
            "Iteration 5143, Loss: 0.01654013805091381\n",
            "Iteration 5144, Loss: 0.015866359695792198\n",
            "Iteration 5145, Loss: 0.018542824313044548\n",
            "Iteration 5146, Loss: 0.010188179090619087\n",
            "Iteration 5147, Loss: 0.015315880067646503\n",
            "Iteration 5148, Loss: 0.0146248210221529\n",
            "Iteration 5149, Loss: 0.011844435706734657\n",
            "Iteration 5150, Loss: 0.017023267224431038\n",
            "Iteration 5151, Loss: 0.0069357543252408504\n",
            "Iteration 5152, Loss: 0.017131511121988297\n",
            "Iteration 5153, Loss: 0.01028383057564497\n",
            "Iteration 5154, Loss: 0.01409479882568121\n",
            "Iteration 5155, Loss: 0.007001843303442001\n",
            "Iteration 5156, Loss: 0.01149110496044159\n",
            "Iteration 5157, Loss: 0.015399795956909657\n",
            "Iteration 5158, Loss: 0.009939879179000854\n",
            "Iteration 5159, Loss: 0.012555176392197609\n",
            "Iteration 5160, Loss: 0.01566755212843418\n",
            "Iteration 5161, Loss: 0.014786896295845509\n",
            "Iteration 5162, Loss: 0.014927743002772331\n",
            "Iteration 5163, Loss: 0.008093585260212421\n",
            "Iteration 5164, Loss: 0.020160270854830742\n",
            "Iteration 5165, Loss: 0.01184887532144785\n",
            "Iteration 5166, Loss: 0.014916242100298405\n",
            "Iteration 5167, Loss: 0.013147030025720596\n",
            "Iteration 5168, Loss: 0.014388768933713436\n",
            "Iteration 5169, Loss: 0.00952940620481968\n",
            "Iteration 5170, Loss: 0.011796459555625916\n",
            "Iteration 5171, Loss: 0.017214864492416382\n",
            "Iteration 5172, Loss: 0.014561621472239494\n",
            "Iteration 5173, Loss: 0.01408259104937315\n",
            "Iteration 5174, Loss: 0.022344199940562248\n",
            "Iteration 5175, Loss: 0.006707953289151192\n",
            "Iteration 5176, Loss: 0.014177988283336163\n",
            "Iteration 5177, Loss: 0.01561297383159399\n",
            "Iteration 5178, Loss: 0.016042618080973625\n",
            "Iteration 5179, Loss: 0.015617349185049534\n",
            "Iteration 5180, Loss: 0.015343994833528996\n",
            "Iteration 5181, Loss: 0.013890101574361324\n",
            "Iteration 5182, Loss: 0.008594089187681675\n",
            "Iteration 5183, Loss: 0.014195763505995274\n",
            "Iteration 5184, Loss: 0.009112320840358734\n",
            "Iteration 5185, Loss: 0.008317212574183941\n",
            "Iteration 5186, Loss: 0.008910810574889183\n",
            "Iteration 5187, Loss: 0.011821074411273003\n",
            "Iteration 5188, Loss: 0.014605904929339886\n",
            "Iteration 5189, Loss: 0.009643474593758583\n",
            "Iteration 5190, Loss: 0.013966599479317665\n",
            "Iteration 5191, Loss: 0.013862296007573605\n",
            "Iteration 5192, Loss: 0.012000524438917637\n",
            "Iteration 5193, Loss: 0.013711611740291119\n",
            "Iteration 5194, Loss: 0.011762057431042194\n",
            "Iteration 5195, Loss: 0.013949314132332802\n",
            "Iteration 5196, Loss: 0.010625408962368965\n",
            "Iteration 5197, Loss: 0.01373851764947176\n",
            "Iteration 5198, Loss: 0.026908133178949356\n",
            "Iteration 5199, Loss: 0.014408866874873638\n",
            "Iteration 5200, Loss: 0.011398647911846638\n",
            "Iteration 5201, Loss: 0.015849212184548378\n",
            "Iteration 5202, Loss: 0.011163001880049706\n",
            "Iteration 5203, Loss: 0.0075794393196702\n",
            "Iteration 5204, Loss: 0.013293864205479622\n",
            "Iteration 5205, Loss: 0.008159148506820202\n",
            "Iteration 5206, Loss: 0.01770090125501156\n",
            "Iteration 5207, Loss: 0.021417899057269096\n",
            "Iteration 5208, Loss: 0.014535898342728615\n",
            "Iteration 5209, Loss: 0.014304952695965767\n",
            "Iteration 5210, Loss: 0.017093556001782417\n",
            "Iteration 5211, Loss: 0.02033052034676075\n",
            "Iteration 5212, Loss: 0.013687461614608765\n",
            "Iteration 5213, Loss: 0.014162441715598106\n",
            "Iteration 5214, Loss: 0.012596569024026394\n",
            "Iteration 5215, Loss: 0.015959618613123894\n",
            "Iteration 5216, Loss: 0.014604019932448864\n",
            "Iteration 5217, Loss: 0.012318296357989311\n",
            "Iteration 5218, Loss: 0.01494854036718607\n",
            "Iteration 5219, Loss: 0.00969858281314373\n",
            "Iteration 5220, Loss: 0.019172310829162598\n",
            "Iteration 5221, Loss: 0.009587964974343777\n",
            "Iteration 5222, Loss: 0.01718081347644329\n",
            "Iteration 5223, Loss: 0.01227722316980362\n",
            "Iteration 5224, Loss: 0.00852231215685606\n",
            "Iteration 5225, Loss: 0.013693802990019321\n",
            "Iteration 5226, Loss: 0.011619765311479568\n",
            "Iteration 5227, Loss: 0.01563304103910923\n",
            "Iteration 5228, Loss: 0.012228812091052532\n",
            "Iteration 5229, Loss: 0.01510552130639553\n",
            "Iteration 5230, Loss: 0.012989496812224388\n",
            "Iteration 5231, Loss: 0.01372038945555687\n",
            "Iteration 5232, Loss: 0.010664418339729309\n",
            "Iteration 5233, Loss: 0.011194500140845776\n",
            "Iteration 5234, Loss: 0.016229161992669106\n",
            "Iteration 5235, Loss: 0.01699734479188919\n",
            "Iteration 5236, Loss: 0.01890682429075241\n",
            "Iteration 5237, Loss: 0.00951633881777525\n",
            "Iteration 5238, Loss: 0.011585574597120285\n",
            "Iteration 5239, Loss: 0.01308922003954649\n",
            "Iteration 5240, Loss: 0.008059392683207989\n",
            "Iteration 5241, Loss: 0.011754241771996021\n",
            "Iteration 5242, Loss: 0.007376335561275482\n",
            "Iteration 5243, Loss: 0.01391440536826849\n",
            "Iteration 5244, Loss: 0.010209712199866772\n",
            "Iteration 5245, Loss: 0.016608748584985733\n",
            "Iteration 5246, Loss: 0.010795654729008675\n",
            "Iteration 5247, Loss: 0.008076503872871399\n",
            "Iteration 5248, Loss: 0.013952473178505898\n",
            "Iteration 5249, Loss: 0.009219161234796047\n",
            "Iteration 5250, Loss: 0.013262564316391945\n",
            "Iteration 5251, Loss: 0.013356208801269531\n",
            "Iteration 5252, Loss: 0.010050366632640362\n",
            "Iteration 5253, Loss: 0.01508218515664339\n",
            "Iteration 5254, Loss: 0.012647498399019241\n",
            "Iteration 5255, Loss: 0.01211615651845932\n",
            "Iteration 5256, Loss: 0.017830004915595055\n",
            "Iteration 5257, Loss: 0.01537279598414898\n",
            "Iteration 5258, Loss: 0.012470262125134468\n",
            "Iteration 5259, Loss: 0.01231611892580986\n",
            "Iteration 5260, Loss: 0.021980859339237213\n",
            "Iteration 5261, Loss: 0.01609591394662857\n",
            "Iteration 5262, Loss: 0.013911529444158077\n",
            "Iteration 5263, Loss: 0.021851448342204094\n",
            "Iteration 5264, Loss: 0.015251816250383854\n",
            "Iteration 5265, Loss: 0.01619471237063408\n",
            "Iteration 5266, Loss: 0.00995598454028368\n",
            "Iteration 5267, Loss: 0.015515326522290707\n",
            "Iteration 5268, Loss: 0.010120106860995293\n",
            "Iteration 5269, Loss: 0.014197316020727158\n",
            "Iteration 5270, Loss: 0.014341061003506184\n",
            "Iteration 5271, Loss: 0.01701841503381729\n",
            "Iteration 5272, Loss: 0.01799906976521015\n",
            "Iteration 5273, Loss: 0.02175942435860634\n",
            "Iteration 5274, Loss: 0.015126198530197144\n",
            "Iteration 5275, Loss: 0.01505325362086296\n",
            "Iteration 5276, Loss: 0.015156193636357784\n",
            "Iteration 5277, Loss: 0.009919020347297192\n",
            "Iteration 5278, Loss: 0.01559707522392273\n",
            "Iteration 5279, Loss: 0.01481649000197649\n",
            "Iteration 5280, Loss: 0.01434821542352438\n",
            "Iteration 5281, Loss: 0.011397278867661953\n",
            "Iteration 5282, Loss: 0.009634826332330704\n",
            "Iteration 5283, Loss: 0.012000791728496552\n",
            "Iteration 5284, Loss: 0.01228523999452591\n",
            "Iteration 5285, Loss: 0.01353639829903841\n",
            "Iteration 5286, Loss: 0.012387119233608246\n",
            "Iteration 5287, Loss: 0.012227065861225128\n",
            "Iteration 5288, Loss: 0.008008116856217384\n",
            "Iteration 5289, Loss: 0.013445922173559666\n",
            "Iteration 5290, Loss: 0.00954308919608593\n",
            "Iteration 5291, Loss: 0.011749275960028172\n",
            "Iteration 5292, Loss: 0.014975509606301785\n",
            "Iteration 5293, Loss: 0.01244934368878603\n",
            "Iteration 5294, Loss: 0.013535547070205212\n",
            "Iteration 5295, Loss: 0.015864849090576172\n",
            "Iteration 5296, Loss: 0.013916103169322014\n",
            "Iteration 5297, Loss: 0.013942130841314793\n",
            "Iteration 5298, Loss: 0.011594885028898716\n",
            "Iteration 5299, Loss: 0.014553321525454521\n",
            "Iteration 5300, Loss: 0.008439592085778713\n",
            "Iteration 5301, Loss: 0.011876361444592476\n",
            "Iteration 5302, Loss: 0.009550224058330059\n",
            "Iteration 5303, Loss: 0.007953731343150139\n",
            "Iteration 5304, Loss: 0.007668690290302038\n",
            "Iteration 5305, Loss: 0.02043011225759983\n",
            "Iteration 5306, Loss: 0.015917731449007988\n",
            "Iteration 5307, Loss: 0.011800181120634079\n",
            "Iteration 5308, Loss: 0.01955021731555462\n",
            "Iteration 5309, Loss: 0.01664234884083271\n",
            "Iteration 5310, Loss: 0.012256421148777008\n",
            "Iteration 5311, Loss: 0.014578036963939667\n",
            "Iteration 5312, Loss: 0.013304540887475014\n",
            "Iteration 5313, Loss: 0.012403986416757107\n",
            "Iteration 5314, Loss: 0.011468606069684029\n",
            "Iteration 5315, Loss: 0.013703620061278343\n",
            "Iteration 5316, Loss: 0.022441105917096138\n",
            "Iteration 5317, Loss: 0.014936237595975399\n",
            "Iteration 5318, Loss: 0.008186406455934048\n",
            "Iteration 5319, Loss: 0.013277513906359673\n",
            "Iteration 5320, Loss: 0.015229396522045135\n",
            "Iteration 5321, Loss: 0.012570717372000217\n",
            "Iteration 5322, Loss: 0.010404696688055992\n",
            "Iteration 5323, Loss: 0.01408094260841608\n",
            "Iteration 5324, Loss: 0.019377663731575012\n",
            "Iteration 5325, Loss: 0.014562070369720459\n",
            "Iteration 5326, Loss: 0.0103450370952487\n",
            "Iteration 5327, Loss: 0.012542852200567722\n",
            "Iteration 5328, Loss: 0.011203230358660221\n",
            "Iteration 5329, Loss: 0.009474254213273525\n",
            "Iteration 5330, Loss: 0.011105968616902828\n",
            "Iteration 5331, Loss: 0.01092265360057354\n",
            "Iteration 5332, Loss: 0.01041777990758419\n",
            "Iteration 5333, Loss: 0.022272096946835518\n",
            "Iteration 5334, Loss: 0.01285422220826149\n",
            "Iteration 5335, Loss: 0.013242093846201897\n",
            "Iteration 5336, Loss: 0.013932246714830399\n",
            "Iteration 5337, Loss: 0.00970233604311943\n",
            "Iteration 5338, Loss: 0.015266058035194874\n",
            "Iteration 5339, Loss: 0.011601509526371956\n",
            "Iteration 5340, Loss: 0.020895609632134438\n",
            "Iteration 5341, Loss: 0.015364416874945164\n",
            "Iteration 5342, Loss: 0.014673255383968353\n",
            "Iteration 5343, Loss: 0.013082093559205532\n",
            "Iteration 5344, Loss: 0.01288631372153759\n",
            "Iteration 5345, Loss: 0.015301786363124847\n",
            "Iteration 5346, Loss: 0.011391641572117805\n",
            "Iteration 5347, Loss: 0.015140467323362827\n",
            "Iteration 5348, Loss: 0.015677478164434433\n",
            "Iteration 5349, Loss: 0.01715831272304058\n",
            "Iteration 5350, Loss: 0.010833741165697575\n",
            "Iteration 5351, Loss: 0.01708272658288479\n",
            "Iteration 5352, Loss: 0.012898770160973072\n",
            "Iteration 5353, Loss: 0.01682141423225403\n",
            "Iteration 5354, Loss: 0.013220888562500477\n",
            "Iteration 5355, Loss: 0.013626731932163239\n",
            "Iteration 5356, Loss: 0.013959824107587337\n",
            "Iteration 5357, Loss: 0.014237064868211746\n",
            "Iteration 5358, Loss: 0.010826106183230877\n",
            "Iteration 5359, Loss: 0.008178304880857468\n",
            "Iteration 5360, Loss: 0.006087005604058504\n",
            "Iteration 5361, Loss: 0.007633779663592577\n",
            "Iteration 5362, Loss: 0.00798719935119152\n",
            "Iteration 5363, Loss: 0.013046566396951675\n",
            "Iteration 5364, Loss: 0.012381164357066154\n",
            "Iteration 5365, Loss: 0.008773514069616795\n",
            "Iteration 5366, Loss: 0.013952595181763172\n",
            "Iteration 5367, Loss: 0.013686140067875385\n",
            "Iteration 5368, Loss: 0.01615731045603752\n",
            "Iteration 5369, Loss: 0.016229946166276932\n",
            "Iteration 5370, Loss: 0.011573937721550465\n",
            "Iteration 5371, Loss: 0.014017668552696705\n",
            "Iteration 5372, Loss: 0.0172434002161026\n",
            "Iteration 5373, Loss: 0.008924171328544617\n",
            "Iteration 5374, Loss: 0.015953710302710533\n",
            "Iteration 5375, Loss: 0.017913464456796646\n",
            "Iteration 5376, Loss: 0.012783938087522984\n",
            "Iteration 5377, Loss: 0.012173481285572052\n",
            "Iteration 5378, Loss: 0.0119705181568861\n",
            "Iteration 5379, Loss: 0.01154755987226963\n",
            "Iteration 5380, Loss: 0.014297530055046082\n",
            "Iteration 5381, Loss: 0.017267907038331032\n",
            "Iteration 5382, Loss: 0.015311584807932377\n",
            "Iteration 5383, Loss: 0.014163834974169731\n",
            "Iteration 5384, Loss: 0.012840086594223976\n",
            "Iteration 5385, Loss: 0.010455721989274025\n",
            "Iteration 5386, Loss: 0.008881917223334312\n",
            "Iteration 5387, Loss: 0.01031078677624464\n",
            "Iteration 5388, Loss: 0.009179248474538326\n",
            "Iteration 5389, Loss: 0.014619849622249603\n",
            "Iteration 5390, Loss: 0.0229366272687912\n",
            "Iteration 5391, Loss: 0.016251077875494957\n",
            "Iteration 5392, Loss: 0.015025615692138672\n",
            "Iteration 5393, Loss: 0.0149194011464715\n",
            "Iteration 5394, Loss: 0.00949383620172739\n",
            "Iteration 5395, Loss: 0.012823469005525112\n",
            "Iteration 5396, Loss: 0.013429669663310051\n",
            "Iteration 5397, Loss: 0.018239984288811684\n",
            "Iteration 5398, Loss: 0.010191889479756355\n",
            "Iteration 5399, Loss: 0.01053838711231947\n",
            "Iteration 5400, Loss: 0.0052525196224451065\n",
            "Iteration 5401, Loss: 0.021181456744670868\n",
            "Iteration 5402, Loss: 0.017165597528219223\n",
            "Iteration 5403, Loss: 0.012586863711476326\n",
            "Iteration 5404, Loss: 0.010923255234956741\n",
            "Iteration 5405, Loss: 0.01750919781625271\n",
            "Iteration 5406, Loss: 0.011836465448141098\n",
            "Iteration 5407, Loss: 0.01078405138105154\n",
            "Iteration 5408, Loss: 0.012489309534430504\n",
            "Iteration 5409, Loss: 0.012565667741000652\n",
            "Iteration 5410, Loss: 0.012392631731927395\n",
            "Iteration 5411, Loss: 0.015090878121554852\n",
            "Iteration 5412, Loss: 0.010321504436433315\n",
            "Iteration 5413, Loss: 0.012220712378621101\n",
            "Iteration 5414, Loss: 0.014705239795148373\n",
            "Iteration 5415, Loss: 0.01039891317486763\n",
            "Iteration 5416, Loss: 0.00885483343154192\n",
            "Iteration 5417, Loss: 0.019703317433595657\n",
            "Iteration 5418, Loss: 0.02042638696730137\n",
            "Iteration 5419, Loss: 0.014370888471603394\n",
            "Iteration 5420, Loss: 0.009933878667652607\n",
            "Iteration 5421, Loss: 0.013139480724930763\n",
            "Iteration 5422, Loss: 0.01589277759194374\n",
            "Iteration 5423, Loss: 0.013557695783674717\n",
            "Iteration 5424, Loss: 0.009409130550920963\n",
            "Iteration 5425, Loss: 0.012932551093399525\n",
            "Iteration 5426, Loss: 0.011536631733179092\n",
            "Iteration 5427, Loss: 0.010304152965545654\n",
            "Iteration 5428, Loss: 0.014423786662518978\n",
            "Iteration 5429, Loss: 0.008621837943792343\n",
            "Iteration 5430, Loss: 0.013328426517546177\n",
            "Iteration 5431, Loss: 0.007576860953122377\n",
            "Iteration 5432, Loss: 0.015372847206890583\n",
            "Iteration 5433, Loss: 0.012015897780656815\n",
            "Iteration 5434, Loss: 0.00814889557659626\n",
            "Iteration 5435, Loss: 0.015426663681864738\n",
            "Iteration 5436, Loss: 0.015147767961025238\n",
            "Iteration 5437, Loss: 0.01338830403983593\n",
            "Iteration 5438, Loss: 0.009481431916356087\n",
            "Iteration 5439, Loss: 0.007937022484838963\n",
            "Iteration 5440, Loss: 0.01373360212892294\n",
            "Iteration 5441, Loss: 0.011233411729335785\n",
            "Iteration 5442, Loss: 0.013887441717088223\n",
            "Iteration 5443, Loss: 0.017108239233493805\n",
            "Iteration 5444, Loss: 0.01599900983273983\n",
            "Iteration 5445, Loss: 0.01192712876945734\n",
            "Iteration 5446, Loss: 0.010652718134224415\n",
            "Iteration 5447, Loss: 0.014677946455776691\n",
            "Iteration 5448, Loss: 0.008670979179441929\n",
            "Iteration 5449, Loss: 0.010806969366967678\n",
            "Iteration 5450, Loss: 0.015021850354969501\n",
            "Iteration 5451, Loss: 0.019231563434004784\n",
            "Iteration 5452, Loss: 0.008574042469263077\n",
            "Iteration 5453, Loss: 0.0162363164126873\n",
            "Iteration 5454, Loss: 0.015970956534147263\n",
            "Iteration 5455, Loss: 0.009910890832543373\n",
            "Iteration 5456, Loss: 0.017793012782931328\n",
            "Iteration 5457, Loss: 0.013136518187820911\n",
            "Iteration 5458, Loss: 0.015599246136844158\n",
            "Iteration 5459, Loss: 0.013104764744639397\n",
            "Iteration 5460, Loss: 0.01068403385579586\n",
            "Iteration 5461, Loss: 0.013992362655699253\n",
            "Iteration 5462, Loss: 0.016195790842175484\n",
            "Iteration 5463, Loss: 0.009873805567622185\n",
            "Iteration 5464, Loss: 0.013380720280110836\n",
            "Iteration 5465, Loss: 0.014716963283717632\n",
            "Iteration 5466, Loss: 0.015496647916734219\n",
            "Iteration 5467, Loss: 0.012506160885095596\n",
            "Iteration 5468, Loss: 0.014366520568728447\n",
            "Iteration 5469, Loss: 0.00825493410229683\n",
            "Iteration 5470, Loss: 0.009498102590441704\n",
            "Iteration 5471, Loss: 0.011173146776854992\n",
            "Iteration 5472, Loss: 0.009431601502001286\n",
            "Iteration 5473, Loss: 0.013180818408727646\n",
            "Iteration 5474, Loss: 0.012464498169720173\n",
            "Iteration 5475, Loss: 0.016683928668498993\n",
            "Iteration 5476, Loss: 0.009147575125098228\n",
            "Iteration 5477, Loss: 0.014892369508743286\n",
            "Iteration 5478, Loss: 0.012043214403092861\n",
            "Iteration 5479, Loss: 0.010121882893145084\n",
            "Iteration 5480, Loss: 0.012480244040489197\n",
            "Iteration 5481, Loss: 0.016107194125652313\n",
            "Iteration 5482, Loss: 0.014504746533930302\n",
            "Iteration 5483, Loss: 0.014355980791151524\n",
            "Iteration 5484, Loss: 0.0064708697609603405\n",
            "Iteration 5485, Loss: 0.008225826546549797\n",
            "Iteration 5486, Loss: 0.01438260916620493\n",
            "Iteration 5487, Loss: 0.015345788560807705\n",
            "Iteration 5488, Loss: 0.016409175470471382\n",
            "Iteration 5489, Loss: 0.013301217928528786\n",
            "Iteration 5490, Loss: 0.011254394426941872\n",
            "Iteration 5491, Loss: 0.011548006907105446\n",
            "Iteration 5492, Loss: 0.013156328350305557\n",
            "Iteration 5493, Loss: 0.013380038551986217\n",
            "Iteration 5494, Loss: 0.01408248022198677\n",
            "Iteration 5495, Loss: 0.015372853726148605\n",
            "Iteration 5496, Loss: 0.01270038541406393\n",
            "Iteration 5497, Loss: 0.01133901160210371\n",
            "Iteration 5498, Loss: 0.020364612340927124\n",
            "Iteration 5499, Loss: 0.010882426053285599\n",
            "Iteration 5500, Loss: 0.012204671278595924\n",
            "Iteration 5501, Loss: 0.011096863076090813\n",
            "Iteration 5502, Loss: 0.011974127031862736\n",
            "Iteration 5503, Loss: 0.014493336901068687\n",
            "Iteration 5504, Loss: 0.0106052840128541\n",
            "Iteration 5505, Loss: 0.018775001168251038\n",
            "Iteration 5506, Loss: 0.014317071996629238\n",
            "Iteration 5507, Loss: 0.008768416941165924\n",
            "Iteration 5508, Loss: 0.014414018951356411\n",
            "Iteration 5509, Loss: 0.011815438978374004\n",
            "Iteration 5510, Loss: 0.015360601246356964\n",
            "Iteration 5511, Loss: 0.008755404502153397\n",
            "Iteration 5512, Loss: 0.016118325293064117\n",
            "Iteration 5513, Loss: 0.01158709917217493\n",
            "Iteration 5514, Loss: 0.01747778058052063\n",
            "Iteration 5515, Loss: 0.012179296463727951\n",
            "Iteration 5516, Loss: 0.009763376787304878\n",
            "Iteration 5517, Loss: 0.01261213794350624\n",
            "Iteration 5518, Loss: 0.013556933961808681\n",
            "Iteration 5519, Loss: 0.011585479602217674\n",
            "Iteration 5520, Loss: 0.012875289656221867\n",
            "Iteration 5521, Loss: 0.01613067090511322\n",
            "Iteration 5522, Loss: 0.009101100265979767\n",
            "Iteration 5523, Loss: 0.007043453864753246\n",
            "Iteration 5524, Loss: 0.0054958839900791645\n",
            "Iteration 5525, Loss: 0.006925557274371386\n",
            "Iteration 5526, Loss: 0.009234759956598282\n",
            "Iteration 5527, Loss: 0.012704359367489815\n",
            "Iteration 5528, Loss: 0.016246208921074867\n",
            "Iteration 5529, Loss: 0.013187100179493427\n",
            "Iteration 5530, Loss: 0.012981800362467766\n",
            "Iteration 5531, Loss: 0.01706262119114399\n",
            "Iteration 5532, Loss: 0.009934322908520699\n",
            "Iteration 5533, Loss: 0.01057300716638565\n",
            "Iteration 5534, Loss: 0.014852884225547314\n",
            "Iteration 5535, Loss: 0.015995608642697334\n",
            "Iteration 5536, Loss: 0.01452928502112627\n",
            "Iteration 5537, Loss: 0.015558574348688126\n",
            "Iteration 5538, Loss: 0.01608959212899208\n",
            "Iteration 5539, Loss: 0.011535138823091984\n",
            "Iteration 5540, Loss: 0.013693981803953648\n",
            "Iteration 5541, Loss: 0.011314970441162586\n",
            "Iteration 5542, Loss: 0.015893006697297096\n",
            "Iteration 5543, Loss: 0.010788443498313427\n",
            "Iteration 5544, Loss: 0.012811923399567604\n",
            "Iteration 5545, Loss: 0.014494281262159348\n",
            "Iteration 5546, Loss: 0.010801352560520172\n",
            "Iteration 5547, Loss: 0.022087469696998596\n",
            "Iteration 5548, Loss: 0.010252782143652439\n",
            "Iteration 5549, Loss: 0.011296194978058338\n",
            "Iteration 5550, Loss: 0.015441427938640118\n",
            "Iteration 5551, Loss: 0.012720529921352863\n",
            "Iteration 5552, Loss: 0.01388638187199831\n",
            "Iteration 5553, Loss: 0.018950099125504494\n",
            "Iteration 5554, Loss: 0.01173946913331747\n",
            "Iteration 5555, Loss: 0.014843689277768135\n",
            "Iteration 5556, Loss: 0.010969994589686394\n",
            "Iteration 5557, Loss: 0.013260605745017529\n",
            "Iteration 5558, Loss: 0.013494221493601799\n",
            "Iteration 5559, Loss: 0.013118798844516277\n",
            "Iteration 5560, Loss: 0.012586585246026516\n",
            "Iteration 5561, Loss: 0.01545802317559719\n",
            "Iteration 5562, Loss: 0.013049869798123837\n",
            "Iteration 5563, Loss: 0.010762670077383518\n",
            "Iteration 5564, Loss: 0.009906243532896042\n",
            "Iteration 5565, Loss: 0.016221972182393074\n",
            "Iteration 5566, Loss: 0.015494154766201973\n",
            "Iteration 5567, Loss: 0.01323228795081377\n",
            "Iteration 5568, Loss: 0.011563802137970924\n",
            "Iteration 5569, Loss: 0.013084398582577705\n",
            "Iteration 5570, Loss: 0.01178652048110962\n",
            "Iteration 5571, Loss: 0.009725137613713741\n",
            "Iteration 5572, Loss: 0.01208761241286993\n",
            "Iteration 5573, Loss: 0.010241922922432423\n",
            "Iteration 5574, Loss: 0.013517176732420921\n",
            "Iteration 5575, Loss: 0.01021867711097002\n",
            "Iteration 5576, Loss: 0.00900865439325571\n",
            "Iteration 5577, Loss: 0.011057732626795769\n",
            "Iteration 5578, Loss: 0.015116666443645954\n",
            "Iteration 5579, Loss: 0.01772722415626049\n",
            "Iteration 5580, Loss: 0.011324873194098473\n",
            "Iteration 5581, Loss: 0.017271418124437332\n",
            "Iteration 5582, Loss: 0.014018121175467968\n",
            "Iteration 5583, Loss: 0.01137390173971653\n",
            "Iteration 5584, Loss: 0.010326845571398735\n",
            "Iteration 5585, Loss: 0.01147470436990261\n",
            "Iteration 5586, Loss: 0.00951146800071001\n",
            "Iteration 5587, Loss: 0.00994388572871685\n",
            "Iteration 5588, Loss: 0.01179173681885004\n",
            "Iteration 5589, Loss: 0.012697311118245125\n",
            "Iteration 5590, Loss: 0.013248221948742867\n",
            "Iteration 5591, Loss: 0.015686659142374992\n",
            "Iteration 5592, Loss: 0.010647349990904331\n",
            "Iteration 5593, Loss: 0.015781866386532784\n",
            "Iteration 5594, Loss: 0.011207470670342445\n",
            "Iteration 5595, Loss: 0.007688329089432955\n",
            "Iteration 5596, Loss: 0.012943696230649948\n",
            "Iteration 5597, Loss: 0.013928486965596676\n",
            "Iteration 5598, Loss: 0.018711397424340248\n",
            "Iteration 5599, Loss: 0.01259968988597393\n",
            "Iteration 5600, Loss: 0.012696071527898312\n",
            "Iteration 5601, Loss: 0.014418371021747589\n",
            "Iteration 5602, Loss: 0.017678584903478622\n",
            "Iteration 5603, Loss: 0.01803823746740818\n",
            "Iteration 5604, Loss: 0.014470241032540798\n",
            "Iteration 5605, Loss: 0.014180531725287437\n",
            "Iteration 5606, Loss: 0.009946144185960293\n",
            "Iteration 5607, Loss: 0.011765433475375175\n",
            "Iteration 5608, Loss: 0.009976480156183243\n",
            "Iteration 5609, Loss: 0.011281783692538738\n",
            "Iteration 5610, Loss: 0.010820417664945126\n",
            "Iteration 5611, Loss: 0.01186528429389\n",
            "Iteration 5612, Loss: 0.019262555986642838\n",
            "Iteration 5613, Loss: 0.01690099947154522\n",
            "Iteration 5614, Loss: 0.007366283796727657\n",
            "Iteration 5615, Loss: 0.01334085501730442\n",
            "Iteration 5616, Loss: 0.014294221065938473\n",
            "Iteration 5617, Loss: 0.012032640166580677\n",
            "Iteration 5618, Loss: 0.01402347069233656\n",
            "Iteration 5619, Loss: 0.009715413674712181\n",
            "Iteration 5620, Loss: 0.008089895360171795\n",
            "Iteration 5621, Loss: 0.01669570989906788\n",
            "Iteration 5622, Loss: 0.012352664954960346\n",
            "Iteration 5623, Loss: 0.014581864699721336\n",
            "Iteration 5624, Loss: 0.006903310772031546\n",
            "Iteration 5625, Loss: 0.020328063517808914\n",
            "Iteration 5626, Loss: 0.01408333983272314\n",
            "Iteration 5627, Loss: 0.01365626323968172\n",
            "Iteration 5628, Loss: 0.01317121833562851\n",
            "Iteration 5629, Loss: 0.012999714352190495\n",
            "Iteration 5630, Loss: 0.011952030472457409\n",
            "Iteration 5631, Loss: 0.016338815912604332\n",
            "Iteration 5632, Loss: 0.012265472672879696\n",
            "Iteration 5633, Loss: 0.012964178808033466\n",
            "Iteration 5634, Loss: 0.02433142066001892\n",
            "Iteration 5635, Loss: 0.012279808521270752\n",
            "Iteration 5636, Loss: 0.014995873905718327\n",
            "Iteration 5637, Loss: 0.01812041737139225\n",
            "Iteration 5638, Loss: 0.022135984152555466\n",
            "Iteration 5639, Loss: 0.015954768285155296\n",
            "Iteration 5640, Loss: 0.009507579728960991\n",
            "Iteration 5641, Loss: 0.013778440654277802\n",
            "Iteration 5642, Loss: 0.012016812339425087\n",
            "Iteration 5643, Loss: 0.011679301969707012\n",
            "Iteration 5644, Loss: 0.013819625601172447\n",
            "Iteration 5645, Loss: 0.01690511405467987\n",
            "Iteration 5646, Loss: 0.008439993485808372\n",
            "Iteration 5647, Loss: 0.011010202579200268\n",
            "Iteration 5648, Loss: 0.015581074170768261\n",
            "Iteration 5649, Loss: 0.011376157402992249\n",
            "Iteration 5650, Loss: 0.01514696516096592\n",
            "Iteration 5651, Loss: 0.012888353317975998\n",
            "Iteration 5652, Loss: 0.01830681413412094\n",
            "Iteration 5653, Loss: 0.01601673848927021\n",
            "Iteration 5654, Loss: 0.008117255754768848\n",
            "Iteration 5655, Loss: 0.019656462594866753\n",
            "Iteration 5656, Loss: 0.01407049410045147\n",
            "Iteration 5657, Loss: 0.014751434326171875\n",
            "Iteration 5658, Loss: 0.012404562905430794\n",
            "Iteration 5659, Loss: 0.01672561652958393\n",
            "Iteration 5660, Loss: 0.010014734230935574\n",
            "Iteration 5661, Loss: 0.01349228248000145\n",
            "Iteration 5662, Loss: 0.015950584784150124\n",
            "Iteration 5663, Loss: 0.013072491623461246\n",
            "Iteration 5664, Loss: 0.010856006294488907\n",
            "Iteration 5665, Loss: 0.012150183320045471\n",
            "Iteration 5666, Loss: 0.015276013873517513\n",
            "Iteration 5667, Loss: 0.018511323258280754\n",
            "Iteration 5668, Loss: 0.013781693764030933\n",
            "Iteration 5669, Loss: 0.013750769197940826\n",
            "Iteration 5670, Loss: 0.017799943685531616\n",
            "Iteration 5671, Loss: 0.010256513021886349\n",
            "Iteration 5672, Loss: 0.011523060500621796\n",
            "Iteration 5673, Loss: 0.014464213512837887\n",
            "Iteration 5674, Loss: 0.011066805571317673\n",
            "Iteration 5675, Loss: 0.010886154137551785\n",
            "Iteration 5676, Loss: 0.014279453083872795\n",
            "Iteration 5677, Loss: 0.011951264925301075\n",
            "Iteration 5678, Loss: 0.01631021685898304\n",
            "Iteration 5679, Loss: 0.01113575417548418\n",
            "Iteration 5680, Loss: 0.01874857023358345\n",
            "Iteration 5681, Loss: 0.010432935319840908\n",
            "Iteration 5682, Loss: 0.013536748476326466\n",
            "Iteration 5683, Loss: 0.023810075595974922\n",
            "Iteration 5684, Loss: 0.01004342082887888\n",
            "Iteration 5685, Loss: 0.017861321568489075\n",
            "Iteration 5686, Loss: 0.009371047839522362\n",
            "Iteration 5687, Loss: 0.01589977741241455\n",
            "Iteration 5688, Loss: 0.010634131729602814\n",
            "Iteration 5689, Loss: 0.012252098880708218\n",
            "Iteration 5690, Loss: 0.016361914575099945\n",
            "Iteration 5691, Loss: 0.013309035450220108\n",
            "Iteration 5692, Loss: 0.01059117540717125\n",
            "Iteration 5693, Loss: 0.01028046477586031\n",
            "Iteration 5694, Loss: 0.015102437697350979\n",
            "Iteration 5695, Loss: 0.018980778753757477\n",
            "Iteration 5696, Loss: 0.014368191361427307\n",
            "Iteration 5697, Loss: 0.013450978323817253\n",
            "Iteration 5698, Loss: 0.008426209911704063\n",
            "Iteration 5699, Loss: 0.013558358885347843\n",
            "Iteration 5700, Loss: 0.011999670416116714\n",
            "Iteration 5701, Loss: 0.013325764797627926\n",
            "Iteration 5702, Loss: 0.020352547988295555\n",
            "Iteration 5703, Loss: 0.007971012033522129\n",
            "Iteration 5704, Loss: 0.013766433112323284\n",
            "Iteration 5705, Loss: 0.017938029021024704\n",
            "Iteration 5706, Loss: 0.011984407901763916\n",
            "Iteration 5707, Loss: 0.01196121983230114\n",
            "Iteration 5708, Loss: 0.021381467580795288\n",
            "Iteration 5709, Loss: 0.011399291455745697\n",
            "Iteration 5710, Loss: 0.0167002622038126\n",
            "Iteration 5711, Loss: 0.014918750151991844\n",
            "Iteration 5712, Loss: 0.01337557751685381\n",
            "Iteration 5713, Loss: 0.007489207200706005\n",
            "Iteration 5714, Loss: 0.01054312288761139\n",
            "Iteration 5715, Loss: 0.012737011536955833\n",
            "Iteration 5716, Loss: 0.009663335978984833\n",
            "Iteration 5717, Loss: 0.014695966616272926\n",
            "Iteration 5718, Loss: 0.017242705449461937\n",
            "Iteration 5719, Loss: 0.014151216484606266\n",
            "Iteration 5720, Loss: 0.009520263411104679\n",
            "Iteration 5721, Loss: 0.01457972638309002\n",
            "Iteration 5722, Loss: 0.00939804408699274\n",
            "Iteration 5723, Loss: 0.01592060923576355\n",
            "Iteration 5724, Loss: 0.014494473114609718\n",
            "Iteration 5725, Loss: 0.013938166201114655\n",
            "Iteration 5726, Loss: 0.01692688837647438\n",
            "Iteration 5727, Loss: 0.015603783540427685\n",
            "Iteration 5728, Loss: 0.01397215761244297\n",
            "Iteration 5729, Loss: 0.0104008624330163\n",
            "Iteration 5730, Loss: 0.015982648357748985\n",
            "Iteration 5731, Loss: 0.015500353649258614\n",
            "Iteration 5732, Loss: 0.0084934551268816\n",
            "Iteration 5733, Loss: 0.017290759831666946\n",
            "Iteration 5734, Loss: 0.010480433702468872\n",
            "Iteration 5735, Loss: 0.013847632333636284\n",
            "Iteration 5736, Loss: 0.016049742698669434\n",
            "Iteration 5737, Loss: 0.01107718050479889\n",
            "Iteration 5738, Loss: 0.01074705459177494\n",
            "Iteration 5739, Loss: 0.01042739488184452\n",
            "Iteration 5740, Loss: 0.011572898365557194\n",
            "Iteration 5741, Loss: 0.011780294589698315\n",
            "Iteration 5742, Loss: 0.015377757139503956\n",
            "Iteration 5743, Loss: 0.010042569600045681\n",
            "Iteration 5744, Loss: 0.011531151831150055\n",
            "Iteration 5745, Loss: 0.0188224408775568\n",
            "Iteration 5746, Loss: 0.01059865951538086\n",
            "Iteration 5747, Loss: 0.02309569902718067\n",
            "Iteration 5748, Loss: 0.01346297562122345\n",
            "Iteration 5749, Loss: 0.016195148229599\n",
            "Iteration 5750, Loss: 0.01761704497039318\n",
            "Iteration 5751, Loss: 0.013161365874111652\n",
            "Iteration 5752, Loss: 0.012929421849548817\n",
            "Iteration 5753, Loss: 0.02264494076371193\n",
            "Iteration 5754, Loss: 0.01222772803157568\n",
            "Iteration 5755, Loss: 0.008939558640122414\n",
            "Iteration 5756, Loss: 0.01596449688076973\n",
            "Iteration 5757, Loss: 0.018316570669412613\n",
            "Iteration 5758, Loss: 0.013641995377838612\n",
            "Iteration 5759, Loss: 0.013405770994722843\n",
            "Iteration 5760, Loss: 0.008360955864191055\n",
            "Iteration 5761, Loss: 0.012353376485407352\n",
            "Iteration 5762, Loss: 0.00861815083771944\n",
            "Iteration 5763, Loss: 0.007443919777870178\n",
            "Iteration 5764, Loss: 0.012937439605593681\n",
            "Iteration 5765, Loss: 0.012626111507415771\n",
            "Iteration 5766, Loss: 0.008920368738472462\n",
            "Iteration 5767, Loss: 0.013261350803077221\n",
            "Iteration 5768, Loss: 0.014299631118774414\n",
            "Iteration 5769, Loss: 0.007752834353595972\n",
            "Iteration 5770, Loss: 0.012187696062028408\n",
            "Iteration 5771, Loss: 0.011233181692659855\n",
            "Iteration 5772, Loss: 0.021076828241348267\n",
            "Iteration 5773, Loss: 0.018204467371106148\n",
            "Iteration 5774, Loss: 0.01580752059817314\n",
            "Iteration 5775, Loss: 0.014502832666039467\n",
            "Iteration 5776, Loss: 0.011550633236765862\n",
            "Iteration 5777, Loss: 0.01528676226735115\n",
            "Iteration 5778, Loss: 0.009059567004442215\n",
            "Iteration 5779, Loss: 0.012114117853343487\n",
            "Iteration 5780, Loss: 0.010446133092045784\n",
            "Iteration 5781, Loss: 0.018876010552048683\n",
            "Iteration 5782, Loss: 0.011095869354903698\n",
            "Iteration 5783, Loss: 0.015086798928678036\n",
            "Iteration 5784, Loss: 0.013962130062282085\n",
            "Iteration 5785, Loss: 0.011987147852778435\n",
            "Iteration 5786, Loss: 0.01983804441988468\n",
            "Iteration 5787, Loss: 0.013281065970659256\n",
            "Iteration 5788, Loss: 0.014745947904884815\n",
            "Iteration 5789, Loss: 0.013309046626091003\n",
            "Iteration 5790, Loss: 0.009765509516000748\n",
            "Iteration 5791, Loss: 0.01968768984079361\n",
            "Iteration 5792, Loss: 0.015607481822371483\n",
            "Iteration 5793, Loss: 0.013708565384149551\n",
            "Iteration 5794, Loss: 0.013064314611256123\n",
            "Iteration 5795, Loss: 0.011672697961330414\n",
            "Iteration 5796, Loss: 0.011278554797172546\n",
            "Iteration 5797, Loss: 0.00523328548297286\n",
            "Iteration 5798, Loss: 0.012074867263436317\n",
            "Iteration 5799, Loss: 0.013495223596692085\n",
            "Iteration 5800, Loss: 0.014769804663956165\n",
            "Iteration 5801, Loss: 0.008459034375846386\n",
            "Iteration 5802, Loss: 0.011356006376445293\n",
            "Iteration 5803, Loss: 0.01707354187965393\n",
            "Iteration 5804, Loss: 0.013902194797992706\n",
            "Iteration 5805, Loss: 0.009308476001024246\n",
            "Iteration 5806, Loss: 0.01587563380599022\n",
            "Iteration 5807, Loss: 0.014565359801054\n",
            "Iteration 5808, Loss: 0.011031481437385082\n",
            "Iteration 5809, Loss: 0.008387158624827862\n",
            "Iteration 5810, Loss: 0.007375332061201334\n",
            "Iteration 5811, Loss: 0.01201439555734396\n",
            "Iteration 5812, Loss: 0.005955256521701813\n",
            "Iteration 5813, Loss: 0.009177633561193943\n",
            "Iteration 5814, Loss: 0.009328539483249187\n",
            "Iteration 5815, Loss: 0.010682076215744019\n",
            "Iteration 5816, Loss: 0.009895558468997478\n",
            "Iteration 5817, Loss: 0.013095845468342304\n",
            "Iteration 5818, Loss: 0.012465550564229488\n",
            "Iteration 5819, Loss: 0.009812616743147373\n",
            "Iteration 5820, Loss: 0.012181650847196579\n",
            "Iteration 5821, Loss: 0.009606552310287952\n",
            "Iteration 5822, Loss: 0.011534043587744236\n",
            "Iteration 5823, Loss: 0.015247875824570656\n",
            "Iteration 5824, Loss: 0.01073630340397358\n",
            "Iteration 5825, Loss: 0.020232463255524635\n",
            "Iteration 5826, Loss: 0.01062693353742361\n",
            "Iteration 5827, Loss: 0.00901586189866066\n",
            "Iteration 5828, Loss: 0.008726769126951694\n",
            "Iteration 5829, Loss: 0.00920034758746624\n",
            "Iteration 5830, Loss: 0.018068760633468628\n",
            "Iteration 5831, Loss: 0.012304987758398056\n",
            "Iteration 5832, Loss: 0.009169629774987698\n",
            "Iteration 5833, Loss: 0.010150300338864326\n",
            "Iteration 5834, Loss: 0.011394457891583443\n",
            "Iteration 5835, Loss: 0.012781664729118347\n",
            "Iteration 5836, Loss: 0.010487078689038754\n",
            "Iteration 5837, Loss: 0.009598991833627224\n",
            "Iteration 5838, Loss: 0.011171096935868263\n",
            "Iteration 5839, Loss: 0.005257394630461931\n",
            "Iteration 5840, Loss: 0.01761084794998169\n",
            "Iteration 5841, Loss: 0.010837653651833534\n",
            "Iteration 5842, Loss: 0.006992802023887634\n",
            "Iteration 5843, Loss: 0.01326697040349245\n",
            "Iteration 5844, Loss: 0.009718400426208973\n",
            "Iteration 5845, Loss: 0.010757386684417725\n",
            "Iteration 5846, Loss: 0.010478870943188667\n",
            "Iteration 5847, Loss: 0.015500192530453205\n",
            "Iteration 5848, Loss: 0.014375661499798298\n",
            "Iteration 5849, Loss: 0.02067575603723526\n",
            "Iteration 5850, Loss: 0.014020557515323162\n",
            "Iteration 5851, Loss: 0.017308976501226425\n",
            "Iteration 5852, Loss: 0.018742498010396957\n",
            "Iteration 5853, Loss: 0.010528432205319405\n",
            "Iteration 5854, Loss: 0.009036378934979439\n",
            "Iteration 5855, Loss: 0.008031172677874565\n",
            "Iteration 5856, Loss: 0.014426411129534245\n",
            "Iteration 5857, Loss: 0.010645384900271893\n",
            "Iteration 5858, Loss: 0.021234337240457535\n",
            "Iteration 5859, Loss: 0.015485876239836216\n",
            "Iteration 5860, Loss: 0.012896265834569931\n",
            "Iteration 5861, Loss: 0.016647329553961754\n",
            "Iteration 5862, Loss: 0.010473338887095451\n",
            "Iteration 5863, Loss: 0.012779667973518372\n",
            "Iteration 5864, Loss: 0.01343551930040121\n",
            "Iteration 5865, Loss: 0.01474835816770792\n",
            "Iteration 5866, Loss: 0.011174853891134262\n",
            "Iteration 5867, Loss: 0.009581375867128372\n",
            "Iteration 5868, Loss: 0.013747054152190685\n",
            "Iteration 5869, Loss: 0.012325928546488285\n",
            "Iteration 5870, Loss: 0.01348421722650528\n",
            "Iteration 5871, Loss: 0.010157494805753231\n",
            "Iteration 5872, Loss: 0.010247155092656612\n",
            "Iteration 5873, Loss: 0.013801969587802887\n",
            "Iteration 5874, Loss: 0.012737968936562538\n",
            "Iteration 5875, Loss: 0.01025411393493414\n",
            "Iteration 5876, Loss: 0.01475636474788189\n",
            "Iteration 5877, Loss: 0.009119893424212933\n",
            "Iteration 5878, Loss: 0.010684268549084663\n",
            "Iteration 5879, Loss: 0.009515352547168732\n",
            "Iteration 5880, Loss: 0.016273584216833115\n",
            "Iteration 5881, Loss: 0.024043932557106018\n",
            "Iteration 5882, Loss: 0.008671420626342297\n",
            "Iteration 5883, Loss: 0.00930823665112257\n",
            "Iteration 5884, Loss: 0.012001153081655502\n",
            "Iteration 5885, Loss: 0.011811105534434319\n",
            "Iteration 5886, Loss: 0.010769631713628769\n",
            "Iteration 5887, Loss: 0.015143439173698425\n",
            "Iteration 5888, Loss: 0.013528832234442234\n",
            "Iteration 5889, Loss: 0.011523124761879444\n",
            "Iteration 5890, Loss: 0.01110141258686781\n",
            "Iteration 5891, Loss: 0.0191302802413702\n",
            "Iteration 5892, Loss: 0.017357155680656433\n",
            "Iteration 5893, Loss: 0.015010787174105644\n",
            "Iteration 5894, Loss: 0.010416338220238686\n",
            "Iteration 5895, Loss: 0.01022202055901289\n",
            "Iteration 5896, Loss: 0.010367186740040779\n",
            "Iteration 5897, Loss: 0.012625066563487053\n",
            "Iteration 5898, Loss: 0.010609728284180164\n",
            "Iteration 5899, Loss: 0.01447428110986948\n",
            "Iteration 5900, Loss: 0.013654317706823349\n",
            "Iteration 5901, Loss: 0.009154640138149261\n",
            "Iteration 5902, Loss: 0.014398985542356968\n",
            "Iteration 5903, Loss: 0.013751830905675888\n",
            "Iteration 5904, Loss: 0.012827977538108826\n",
            "Iteration 5905, Loss: 0.014477613382041454\n",
            "Iteration 5906, Loss: 0.013585046865046024\n",
            "Iteration 5907, Loss: 0.010091550648212433\n",
            "Iteration 5908, Loss: 0.015351805835962296\n",
            "Iteration 5909, Loss: 0.013684289529919624\n",
            "Iteration 5910, Loss: 0.012792842462658882\n",
            "Iteration 5911, Loss: 0.010924984700977802\n",
            "Iteration 5912, Loss: 0.013119828887283802\n",
            "Iteration 5913, Loss: 0.010748784989118576\n",
            "Iteration 5914, Loss: 0.008888238109648228\n",
            "Iteration 5915, Loss: 0.010049380362033844\n",
            "Iteration 5916, Loss: 0.012314329855144024\n",
            "Iteration 5917, Loss: 0.008889906108379364\n",
            "Iteration 5918, Loss: 0.011790590360760689\n",
            "Iteration 5919, Loss: 0.01443308312445879\n",
            "Iteration 5920, Loss: 0.00946534238755703\n",
            "Iteration 5921, Loss: 0.014392927289009094\n",
            "Iteration 5922, Loss: 0.006246102973818779\n",
            "Iteration 5923, Loss: 0.016767559573054314\n",
            "Iteration 5924, Loss: 0.014720562845468521\n",
            "Iteration 5925, Loss: 0.017052851617336273\n",
            "Iteration 5926, Loss: 0.01043003797531128\n",
            "Iteration 5927, Loss: 0.013836218975484371\n",
            "Iteration 5928, Loss: 0.013259549625217915\n",
            "Iteration 5929, Loss: 0.018368402495980263\n",
            "Iteration 5930, Loss: 0.010486365295946598\n",
            "Iteration 5931, Loss: 0.010697043500840664\n",
            "Iteration 5932, Loss: 0.008385674096643925\n",
            "Iteration 5933, Loss: 0.012163517996668816\n",
            "Iteration 5934, Loss: 0.011945568025112152\n",
            "Iteration 5935, Loss: 0.006834956351667643\n",
            "Iteration 5936, Loss: 0.012796303257346153\n",
            "Iteration 5937, Loss: 0.014992034994065762\n",
            "Iteration 5938, Loss: 0.010096670128405094\n",
            "Iteration 5939, Loss: 0.010348002426326275\n",
            "Iteration 5940, Loss: 0.01407482847571373\n",
            "Iteration 5941, Loss: 0.011703823693096638\n",
            "Iteration 5942, Loss: 0.010851285420358181\n",
            "Iteration 5943, Loss: 0.010882151313126087\n",
            "Iteration 5944, Loss: 0.013618312776088715\n",
            "Iteration 5945, Loss: 0.01216394454240799\n",
            "Iteration 5946, Loss: 0.01361177023500204\n",
            "Iteration 5947, Loss: 0.013325395993888378\n",
            "Iteration 5948, Loss: 0.012676685117185116\n",
            "Iteration 5949, Loss: 0.009754335507750511\n",
            "Iteration 5950, Loss: 0.013269670307636261\n",
            "Iteration 5951, Loss: 0.009044192731380463\n",
            "Iteration 5952, Loss: 0.009632186964154243\n",
            "Iteration 5953, Loss: 0.01482569519430399\n",
            "Iteration 5954, Loss: 0.013463936746120453\n",
            "Iteration 5955, Loss: 0.010692178271710873\n",
            "Iteration 5956, Loss: 0.009205647744238377\n",
            "Iteration 5957, Loss: 0.007282492704689503\n",
            "Iteration 5958, Loss: 0.012886895798146725\n",
            "Iteration 5959, Loss: 0.008780387230217457\n",
            "Iteration 5960, Loss: 0.022113071754574776\n",
            "Iteration 5961, Loss: 0.009670237079262733\n",
            "Iteration 5962, Loss: 0.012490611523389816\n",
            "Iteration 5963, Loss: 0.016338855028152466\n",
            "Iteration 5964, Loss: 0.012331178411841393\n",
            "Iteration 5965, Loss: 0.015763524919748306\n",
            "Iteration 5966, Loss: 0.01087146159261465\n",
            "Iteration 5967, Loss: 0.012211206369102001\n",
            "Iteration 5968, Loss: 0.012168421410024166\n",
            "Iteration 5969, Loss: 0.014773468486964703\n",
            "Iteration 5970, Loss: 0.011221237480640411\n",
            "Iteration 5971, Loss: 0.011736883781850338\n",
            "Iteration 5972, Loss: 0.0085153141990304\n",
            "Iteration 5973, Loss: 0.015920132398605347\n",
            "Iteration 5974, Loss: 0.013569245114922523\n",
            "Iteration 5975, Loss: 0.01384922955185175\n",
            "Iteration 5976, Loss: 0.009101605042815208\n",
            "Iteration 5977, Loss: 0.014225604943931103\n",
            "Iteration 5978, Loss: 0.018796607851982117\n",
            "Iteration 5979, Loss: 0.01773967407643795\n",
            "Iteration 5980, Loss: 0.009858814999461174\n",
            "Iteration 5981, Loss: 0.00864946935325861\n",
            "Iteration 5982, Loss: 0.013307119719684124\n",
            "Iteration 5983, Loss: 0.020194802433252335\n",
            "Iteration 5984, Loss: 0.006983288563787937\n",
            "Iteration 5985, Loss: 0.01951519399881363\n",
            "Iteration 5986, Loss: 0.015188611112535\n",
            "Iteration 5987, Loss: 0.01252467930316925\n",
            "Iteration 5988, Loss: 0.01818946748971939\n",
            "Iteration 5989, Loss: 0.01392303965985775\n",
            "Iteration 5990, Loss: 0.014102604240179062\n",
            "Iteration 5991, Loss: 0.012451154179871082\n",
            "Iteration 5992, Loss: 0.016689158976078033\n",
            "Iteration 5993, Loss: 0.014939475804567337\n",
            "Iteration 5994, Loss: 0.013460642658174038\n",
            "Iteration 5995, Loss: 0.012229922227561474\n",
            "Iteration 5996, Loss: 0.011910682544112206\n",
            "Iteration 5997, Loss: 0.008366659283638\n",
            "Iteration 5998, Loss: 0.012594970874488354\n",
            "Iteration 5999, Loss: 0.011583292856812477\n",
            "Iteration 6000, Loss: 0.010627510957419872\n",
            "Test Loss: 0.05511387810111046\n",
            "Iteration 6001, Loss: 0.008591661229729652\n",
            "Iteration 6002, Loss: 0.013816195540130138\n",
            "Iteration 6003, Loss: 0.01284428033977747\n",
            "Iteration 6004, Loss: 0.01257045567035675\n",
            "Iteration 6005, Loss: 0.005393983330577612\n",
            "Iteration 6006, Loss: 0.01321441400796175\n",
            "Iteration 6007, Loss: 0.012410778552293777\n",
            "Iteration 6008, Loss: 0.009540468454360962\n",
            "Iteration 6009, Loss: 0.013718490488827229\n",
            "Iteration 6010, Loss: 0.011181929148733616\n",
            "Iteration 6011, Loss: 0.016477178782224655\n",
            "Iteration 6012, Loss: 0.011651594191789627\n",
            "Iteration 6013, Loss: 0.012695846147835255\n",
            "Iteration 6014, Loss: 0.013030391186475754\n",
            "Iteration 6015, Loss: 0.013280755840241909\n",
            "Iteration 6016, Loss: 0.011991426348686218\n",
            "Iteration 6017, Loss: 0.011074995622038841\n",
            "Iteration 6018, Loss: 0.010180894285440445\n",
            "Iteration 6019, Loss: 0.01714077591896057\n",
            "Iteration 6020, Loss: 0.013861854560673237\n",
            "Iteration 6021, Loss: 0.009808678179979324\n",
            "Iteration 6022, Loss: 0.0101297153159976\n",
            "Iteration 6023, Loss: 0.011167572811245918\n",
            "Iteration 6024, Loss: 0.007785170339047909\n",
            "Iteration 6025, Loss: 0.017476867884397507\n",
            "Iteration 6026, Loss: 0.005602733232080936\n",
            "Iteration 6027, Loss: 0.01364679541438818\n",
            "Iteration 6028, Loss: 0.006915571633726358\n",
            "Iteration 6029, Loss: 0.008923017419874668\n",
            "Iteration 6030, Loss: 0.012597117573022842\n",
            "Iteration 6031, Loss: 0.014560539275407791\n",
            "Iteration 6032, Loss: 0.00973267201334238\n",
            "Iteration 6033, Loss: 0.018444692716002464\n",
            "Iteration 6034, Loss: 0.0073043182492256165\n",
            "Iteration 6035, Loss: 0.011297780089080334\n",
            "Iteration 6036, Loss: 0.01225061435252428\n",
            "Iteration 6037, Loss: 0.010964377783238888\n",
            "Iteration 6038, Loss: 0.012457317672669888\n",
            "Iteration 6039, Loss: 0.016022007912397385\n",
            "Iteration 6040, Loss: 0.015218188054859638\n",
            "Iteration 6041, Loss: 0.011167309246957302\n",
            "Iteration 6042, Loss: 0.00824548490345478\n",
            "Iteration 6043, Loss: 0.012354970909655094\n",
            "Iteration 6044, Loss: 0.010484131053090096\n",
            "Iteration 6045, Loss: 0.012981005944311619\n",
            "Iteration 6046, Loss: 0.010438029654324055\n",
            "Iteration 6047, Loss: 0.010939381085336208\n",
            "Iteration 6048, Loss: 0.01544142421334982\n",
            "Iteration 6049, Loss: 0.014291140250861645\n",
            "Iteration 6050, Loss: 0.014318275265395641\n",
            "Iteration 6051, Loss: 0.015920981764793396\n",
            "Iteration 6052, Loss: 0.010323584079742432\n",
            "Iteration 6053, Loss: 0.01740422658622265\n",
            "Iteration 6054, Loss: 0.007900000549852848\n",
            "Iteration 6055, Loss: 0.014436155557632446\n",
            "Iteration 6056, Loss: 0.010459613054990768\n",
            "Iteration 6057, Loss: 0.010588500648736954\n",
            "Iteration 6058, Loss: 0.009580199606716633\n",
            "Iteration 6059, Loss: 0.01687307469546795\n",
            "Iteration 6060, Loss: 0.014328227378427982\n",
            "Iteration 6061, Loss: 0.010402319021522999\n",
            "Iteration 6062, Loss: 0.014947650954127312\n",
            "Iteration 6063, Loss: 0.011399905197322369\n",
            "Iteration 6064, Loss: 0.014693805016577244\n",
            "Iteration 6065, Loss: 0.011010730639100075\n",
            "Iteration 6066, Loss: 0.01287051010876894\n",
            "Iteration 6067, Loss: 0.015537930652499199\n",
            "Iteration 6068, Loss: 0.009948346763849258\n",
            "Iteration 6069, Loss: 0.010084301233291626\n",
            "Iteration 6070, Loss: 0.009745221585035324\n",
            "Iteration 6071, Loss: 0.008864708244800568\n",
            "Iteration 6072, Loss: 0.007291935384273529\n",
            "Iteration 6073, Loss: 0.018336234614253044\n",
            "Iteration 6074, Loss: 0.010033630765974522\n",
            "Iteration 6075, Loss: 0.014765226282179356\n",
            "Iteration 6076, Loss: 0.0198505986481905\n",
            "Iteration 6077, Loss: 0.015852877870202065\n",
            "Iteration 6078, Loss: 0.020011411979794502\n",
            "Iteration 6079, Loss: 0.013273684307932854\n",
            "Iteration 6080, Loss: 0.02428111620247364\n",
            "Iteration 6081, Loss: 0.010485231876373291\n",
            "Iteration 6082, Loss: 0.01945168524980545\n",
            "Iteration 6083, Loss: 0.01395843829959631\n",
            "Iteration 6084, Loss: 0.010111053474247456\n",
            "Iteration 6085, Loss: 0.011014661751687527\n",
            "Iteration 6086, Loss: 0.01718158647418022\n",
            "Iteration 6087, Loss: 0.014899938367307186\n",
            "Iteration 6088, Loss: 0.012411464005708694\n",
            "Iteration 6089, Loss: 0.013850273564457893\n",
            "Iteration 6090, Loss: 0.012492519803345203\n",
            "Iteration 6091, Loss: 0.014374573715031147\n",
            "Iteration 6092, Loss: 0.013229218311607838\n",
            "Iteration 6093, Loss: 0.00905355904251337\n",
            "Iteration 6094, Loss: 0.00790329184383154\n",
            "Iteration 6095, Loss: 0.016110362485051155\n",
            "Iteration 6096, Loss: 0.014818013645708561\n",
            "Iteration 6097, Loss: 0.014161472208797932\n",
            "Iteration 6098, Loss: 0.010812279768288136\n",
            "Iteration 6099, Loss: 0.010806155391037464\n",
            "Iteration 6100, Loss: 0.010326084680855274\n",
            "Iteration 6101, Loss: 0.011128928512334824\n",
            "Iteration 6102, Loss: 0.01247568242251873\n",
            "Iteration 6103, Loss: 0.012098176404833794\n",
            "Iteration 6104, Loss: 0.021458886563777924\n",
            "Iteration 6105, Loss: 0.013020655140280724\n",
            "Iteration 6106, Loss: 0.0089919688180089\n",
            "Iteration 6107, Loss: 0.015339958481490612\n",
            "Iteration 6108, Loss: 0.011990740895271301\n",
            "Iteration 6109, Loss: 0.017656398937106133\n",
            "Iteration 6110, Loss: 0.014077087864279747\n",
            "Iteration 6111, Loss: 0.013683696277439594\n",
            "Iteration 6112, Loss: 0.010785586200654507\n",
            "Iteration 6113, Loss: 0.005812153220176697\n",
            "Iteration 6114, Loss: 0.014079299755394459\n",
            "Iteration 6115, Loss: 0.01590387150645256\n",
            "Iteration 6116, Loss: 0.009729756973683834\n",
            "Iteration 6117, Loss: 0.010829450562596321\n",
            "Iteration 6118, Loss: 0.010901139117777348\n",
            "Iteration 6119, Loss: 0.007447373121976852\n",
            "Iteration 6120, Loss: 0.011125823482871056\n",
            "Iteration 6121, Loss: 0.014871422201395035\n",
            "Iteration 6122, Loss: 0.011604945175349712\n",
            "Iteration 6123, Loss: 0.014840798452496529\n",
            "Iteration 6124, Loss: 0.014146522618830204\n",
            "Iteration 6125, Loss: 0.01431776862591505\n",
            "Iteration 6126, Loss: 0.011654745787382126\n",
            "Iteration 6127, Loss: 0.013716698624193668\n",
            "Iteration 6128, Loss: 0.007056133355945349\n",
            "Iteration 6129, Loss: 0.011221042834222317\n",
            "Iteration 6130, Loss: 0.013561457395553589\n",
            "Iteration 6131, Loss: 0.019369564950466156\n",
            "Iteration 6132, Loss: 0.013396569527685642\n",
            "Iteration 6133, Loss: 0.011462164111435413\n",
            "Iteration 6134, Loss: 0.012949299067258835\n",
            "Iteration 6135, Loss: 0.009620925411581993\n",
            "Iteration 6136, Loss: 0.016303814947605133\n",
            "Iteration 6137, Loss: 0.012780996970832348\n",
            "Iteration 6138, Loss: 0.010065693408250809\n",
            "Iteration 6139, Loss: 0.009767157025635242\n",
            "Iteration 6140, Loss: 0.01858781836926937\n",
            "Iteration 6141, Loss: 0.014189078472554684\n",
            "Iteration 6142, Loss: 0.014617875218391418\n",
            "Iteration 6143, Loss: 0.009797678329050541\n",
            "Iteration 6144, Loss: 0.014661052264273167\n",
            "Iteration 6145, Loss: 0.009038680233061314\n",
            "Iteration 6146, Loss: 0.012512607499957085\n",
            "Iteration 6147, Loss: 0.011356829665601254\n",
            "Iteration 6148, Loss: 0.014010784216225147\n",
            "Iteration 6149, Loss: 0.01080477423965931\n",
            "Iteration 6150, Loss: 0.013664694502949715\n",
            "Iteration 6151, Loss: 0.011936918832361698\n",
            "Iteration 6152, Loss: 0.009939326904714108\n",
            "Iteration 6153, Loss: 0.009468365460634232\n",
            "Iteration 6154, Loss: 0.00942685455083847\n",
            "Iteration 6155, Loss: 0.010760215111076832\n",
            "Iteration 6156, Loss: 0.00998457707464695\n",
            "Iteration 6157, Loss: 0.017155133187770844\n",
            "Iteration 6158, Loss: 0.015474488958716393\n",
            "Iteration 6159, Loss: 0.011374736204743385\n",
            "Iteration 6160, Loss: 0.01460031233727932\n",
            "Iteration 6161, Loss: 0.012996826320886612\n",
            "Iteration 6162, Loss: 0.013555764220654964\n",
            "Iteration 6163, Loss: 0.012903258204460144\n",
            "Iteration 6164, Loss: 0.00992579385638237\n",
            "Iteration 6165, Loss: 0.010478694923222065\n",
            "Iteration 6166, Loss: 0.012418190017342567\n",
            "Iteration 6167, Loss: 0.006017562933266163\n",
            "Iteration 6168, Loss: 0.01071274746209383\n",
            "Iteration 6169, Loss: 0.021400442346930504\n",
            "Iteration 6170, Loss: 0.015042105689644814\n",
            "Iteration 6171, Loss: 0.015234163962304592\n",
            "Iteration 6172, Loss: 0.009894325397908688\n",
            "Iteration 6173, Loss: 0.009871161542832851\n",
            "Iteration 6174, Loss: 0.013403586111962795\n",
            "Iteration 6175, Loss: 0.011362365446984768\n",
            "Iteration 6176, Loss: 0.010932007804512978\n",
            "Iteration 6177, Loss: 0.011540304869413376\n",
            "Iteration 6178, Loss: 0.010086345486342907\n",
            "Iteration 6179, Loss: 0.011535867117345333\n",
            "Iteration 6180, Loss: 0.015650173649191856\n",
            "Iteration 6181, Loss: 0.01231235358864069\n",
            "Iteration 6182, Loss: 0.009048945270478725\n",
            "Iteration 6183, Loss: 0.012393554672598839\n",
            "Iteration 6184, Loss: 0.010042481124401093\n",
            "Iteration 6185, Loss: 0.011097751557826996\n",
            "Iteration 6186, Loss: 0.011153892613947392\n",
            "Iteration 6187, Loss: 0.012940291315317154\n",
            "Iteration 6188, Loss: 0.012355968356132507\n",
            "Iteration 6189, Loss: 0.015363846905529499\n",
            "Iteration 6190, Loss: 0.017460055649280548\n",
            "Iteration 6191, Loss: 0.015271199867129326\n",
            "Iteration 6192, Loss: 0.011609447188675404\n",
            "Iteration 6193, Loss: 0.01538149081170559\n",
            "Iteration 6194, Loss: 0.01691504754126072\n",
            "Iteration 6195, Loss: 0.008661437779664993\n",
            "Iteration 6196, Loss: 0.01123944390565157\n",
            "Iteration 6197, Loss: 0.016421843320131302\n",
            "Iteration 6198, Loss: 0.010268112644553185\n",
            "Iteration 6199, Loss: 0.01777624897658825\n",
            "Iteration 6200, Loss: 0.01390470378100872\n",
            "Iteration 6201, Loss: 0.008385088294744492\n",
            "Iteration 6202, Loss: 0.010989726521074772\n",
            "Iteration 6203, Loss: 0.014110119082033634\n",
            "Iteration 6204, Loss: 0.011795556172728539\n",
            "Iteration 6205, Loss: 0.01661110483109951\n",
            "Iteration 6206, Loss: 0.01059388555586338\n",
            "Iteration 6207, Loss: 0.011544925160706043\n",
            "Iteration 6208, Loss: 0.015327385626733303\n",
            "Iteration 6209, Loss: 0.008201655000448227\n",
            "Iteration 6210, Loss: 0.010553651489317417\n",
            "Iteration 6211, Loss: 0.01757686957716942\n",
            "Iteration 6212, Loss: 0.013311869464814663\n",
            "Iteration 6213, Loss: 0.010757822543382645\n",
            "Iteration 6214, Loss: 0.007170849945396185\n",
            "Iteration 6215, Loss: 0.015840278938412666\n",
            "Iteration 6216, Loss: 0.00790610071271658\n",
            "Iteration 6217, Loss: 0.009499581530690193\n",
            "Iteration 6218, Loss: 0.00883275456726551\n",
            "Iteration 6219, Loss: 0.011061289347708225\n",
            "Iteration 6220, Loss: 0.0151805579662323\n",
            "Iteration 6221, Loss: 0.017143193632364273\n",
            "Iteration 6222, Loss: 0.011361036449670792\n",
            "Iteration 6223, Loss: 0.008563391864299774\n",
            "Iteration 6224, Loss: 0.01727249100804329\n",
            "Iteration 6225, Loss: 0.012573054991662502\n",
            "Iteration 6226, Loss: 0.014694815501570702\n",
            "Iteration 6227, Loss: 0.017049212008714676\n",
            "Iteration 6228, Loss: 0.010345086455345154\n",
            "Iteration 6229, Loss: 0.012103262357413769\n",
            "Iteration 6230, Loss: 0.009038247168064117\n",
            "Iteration 6231, Loss: 0.015758249908685684\n",
            "Iteration 6232, Loss: 0.011251233518123627\n",
            "Iteration 6233, Loss: 0.016621718183159828\n",
            "Iteration 6234, Loss: 0.010156097821891308\n",
            "Iteration 6235, Loss: 0.009470405988395214\n",
            "Iteration 6236, Loss: 0.017674164846539497\n",
            "Iteration 6237, Loss: 0.009377839043736458\n",
            "Iteration 6238, Loss: 0.010247383266687393\n",
            "Iteration 6239, Loss: 0.008776450529694557\n",
            "Iteration 6240, Loss: 0.019165407866239548\n",
            "Iteration 6241, Loss: 0.01479028258472681\n",
            "Iteration 6242, Loss: 0.012927668169140816\n",
            "Iteration 6243, Loss: 0.009142238646745682\n",
            "Iteration 6244, Loss: 0.012760310433804989\n",
            "Iteration 6245, Loss: 0.008259815163910389\n",
            "Iteration 6246, Loss: 0.012750525027513504\n",
            "Iteration 6247, Loss: 0.011937879957258701\n",
            "Iteration 6248, Loss: 0.012574555352330208\n",
            "Iteration 6249, Loss: 0.007369533646851778\n",
            "Iteration 6250, Loss: 0.01395199541002512\n",
            "Iteration 6251, Loss: 0.01049911417067051\n",
            "Iteration 6252, Loss: 0.010133248753845692\n",
            "Iteration 6253, Loss: 0.012561913579702377\n",
            "Iteration 6254, Loss: 0.012370923534035683\n",
            "Iteration 6255, Loss: 0.012476429343223572\n",
            "Iteration 6256, Loss: 0.010193883441388607\n",
            "Iteration 6257, Loss: 0.016659626737236977\n",
            "Iteration 6258, Loss: 0.010598571971058846\n",
            "Iteration 6259, Loss: 0.011595744639635086\n",
            "Iteration 6260, Loss: 0.015329808928072453\n",
            "Iteration 6261, Loss: 0.009610488079488277\n",
            "Iteration 6262, Loss: 0.009639252908527851\n",
            "Iteration 6263, Loss: 0.015151753090322018\n",
            "Iteration 6264, Loss: 0.008843716233968735\n",
            "Iteration 6265, Loss: 0.015049990266561508\n",
            "Iteration 6266, Loss: 0.010787888430058956\n",
            "Iteration 6267, Loss: 0.009519467130303383\n",
            "Iteration 6268, Loss: 0.00603104941546917\n",
            "Iteration 6269, Loss: 0.009744814597070217\n",
            "Iteration 6270, Loss: 0.007723156828433275\n",
            "Iteration 6271, Loss: 0.011198648251593113\n",
            "Iteration 6272, Loss: 0.011792834848165512\n",
            "Iteration 6273, Loss: 0.009845882654190063\n",
            "Iteration 6274, Loss: 0.007785651832818985\n",
            "Iteration 6275, Loss: 0.01404549740254879\n",
            "Iteration 6276, Loss: 0.012097993865609169\n",
            "Iteration 6277, Loss: 0.015829306095838547\n",
            "Iteration 6278, Loss: 0.013092809356749058\n",
            "Iteration 6279, Loss: 0.010407250374555588\n",
            "Iteration 6280, Loss: 0.012470383197069168\n",
            "Iteration 6281, Loss: 0.008213224820792675\n",
            "Iteration 6282, Loss: 0.0095203947275877\n",
            "Iteration 6283, Loss: 0.012868607416749\n",
            "Iteration 6284, Loss: 0.014326010830700397\n",
            "Iteration 6285, Loss: 0.016736479476094246\n",
            "Iteration 6286, Loss: 0.010177713818848133\n",
            "Iteration 6287, Loss: 0.012502972036600113\n",
            "Iteration 6288, Loss: 0.012399842031300068\n",
            "Iteration 6289, Loss: 0.006808414123952389\n",
            "Iteration 6290, Loss: 0.00826555397361517\n",
            "Iteration 6291, Loss: 0.008859028108417988\n",
            "Iteration 6292, Loss: 0.011772300116717815\n",
            "Iteration 6293, Loss: 0.008425751700997353\n",
            "Iteration 6294, Loss: 0.01767696626484394\n",
            "Iteration 6295, Loss: 0.011174732819199562\n",
            "Iteration 6296, Loss: 0.011925069615244865\n",
            "Iteration 6297, Loss: 0.015110986307263374\n",
            "Iteration 6298, Loss: 0.01335812546312809\n",
            "Iteration 6299, Loss: 0.012715931981801987\n",
            "Iteration 6300, Loss: 0.012637048028409481\n",
            "Iteration 6301, Loss: 0.01173030212521553\n",
            "Iteration 6302, Loss: 0.013578768819570541\n",
            "Iteration 6303, Loss: 0.00994080863893032\n",
            "Iteration 6304, Loss: 0.008507058024406433\n",
            "Iteration 6305, Loss: 0.01204819604754448\n",
            "Iteration 6306, Loss: 0.007202030159533024\n",
            "Iteration 6307, Loss: 0.01452670432627201\n",
            "Iteration 6308, Loss: 0.010283016599714756\n",
            "Iteration 6309, Loss: 0.015240075998008251\n",
            "Iteration 6310, Loss: 0.014093264937400818\n",
            "Iteration 6311, Loss: 0.010659118182957172\n",
            "Iteration 6312, Loss: 0.010046756826341152\n",
            "Iteration 6313, Loss: 0.01302407681941986\n",
            "Iteration 6314, Loss: 0.011360803619027138\n",
            "Iteration 6315, Loss: 0.012890650890767574\n",
            "Iteration 6316, Loss: 0.01811610534787178\n",
            "Iteration 6317, Loss: 0.013072212226688862\n",
            "Iteration 6318, Loss: 0.016043322160840034\n",
            "Iteration 6319, Loss: 0.013524487614631653\n",
            "Iteration 6320, Loss: 0.013126332312822342\n",
            "Iteration 6321, Loss: 0.013518963009119034\n",
            "Iteration 6322, Loss: 0.011477522552013397\n",
            "Iteration 6323, Loss: 0.017306720837950706\n",
            "Iteration 6324, Loss: 0.010487502440810204\n",
            "Iteration 6325, Loss: 0.014752019196748734\n",
            "Iteration 6326, Loss: 0.007636915892362595\n",
            "Iteration 6327, Loss: 0.00809395406395197\n",
            "Iteration 6328, Loss: 0.010641508735716343\n",
            "Iteration 6329, Loss: 0.009575414471328259\n",
            "Iteration 6330, Loss: 0.012384198606014252\n",
            "Iteration 6331, Loss: 0.012962301261723042\n",
            "Iteration 6332, Loss: 0.012559059076011181\n",
            "Iteration 6333, Loss: 0.01623953878879547\n",
            "Iteration 6334, Loss: 0.013794115744531155\n",
            "Iteration 6335, Loss: 0.015903515741229057\n",
            "Iteration 6336, Loss: 0.012643983587622643\n",
            "Iteration 6337, Loss: 0.01956554874777794\n",
            "Iteration 6338, Loss: 0.013679050840437412\n",
            "Iteration 6339, Loss: 0.00872399564832449\n",
            "Iteration 6340, Loss: 0.013281509280204773\n",
            "Iteration 6341, Loss: 0.011722802184522152\n",
            "Iteration 6342, Loss: 0.008027193136513233\n",
            "Iteration 6343, Loss: 0.009175483137369156\n",
            "Iteration 6344, Loss: 0.011667400598526001\n",
            "Iteration 6345, Loss: 0.013740950264036655\n",
            "Iteration 6346, Loss: 0.010666721500456333\n",
            "Iteration 6347, Loss: 0.012126621790230274\n",
            "Iteration 6348, Loss: 0.01123132836073637\n",
            "Iteration 6349, Loss: 0.010821912437677383\n",
            "Iteration 6350, Loss: 0.011997729539871216\n",
            "Iteration 6351, Loss: 0.013247838243842125\n",
            "Iteration 6352, Loss: 0.013787656091153622\n",
            "Iteration 6353, Loss: 0.011943093501031399\n",
            "Iteration 6354, Loss: 0.009960833005607128\n",
            "Iteration 6355, Loss: 0.01530433353036642\n",
            "Iteration 6356, Loss: 0.012387676164507866\n",
            "Iteration 6357, Loss: 0.01390032283961773\n",
            "Iteration 6358, Loss: 0.011810055002570152\n",
            "Iteration 6359, Loss: 0.015082367695868015\n",
            "Iteration 6360, Loss: 0.011593670584261417\n",
            "Iteration 6361, Loss: 0.011099248193204403\n",
            "Iteration 6362, Loss: 0.01015264168381691\n",
            "Iteration 6363, Loss: 0.013478812761604786\n",
            "Iteration 6364, Loss: 0.011830534785985947\n",
            "Iteration 6365, Loss: 0.01403024047613144\n",
            "Iteration 6366, Loss: 0.013138050213456154\n",
            "Iteration 6367, Loss: 0.012498792260885239\n",
            "Iteration 6368, Loss: 0.016055550426244736\n",
            "Iteration 6369, Loss: 0.013322246260941029\n",
            "Iteration 6370, Loss: 0.013383490033447742\n",
            "Iteration 6371, Loss: 0.014476644806563854\n",
            "Iteration 6372, Loss: 0.010779189877212048\n",
            "Iteration 6373, Loss: 0.00761373620480299\n",
            "Iteration 6374, Loss: 0.011911105364561081\n",
            "Iteration 6375, Loss: 0.010033546015620232\n",
            "Iteration 6376, Loss: 0.011798245832324028\n",
            "Iteration 6377, Loss: 0.00730389216914773\n",
            "Iteration 6378, Loss: 0.015218157321214676\n",
            "Iteration 6379, Loss: 0.015108780935406685\n",
            "Iteration 6380, Loss: 0.013713895343244076\n",
            "Iteration 6381, Loss: 0.010965402238070965\n",
            "Iteration 6382, Loss: 0.008179808966815472\n",
            "Iteration 6383, Loss: 0.009002740494906902\n",
            "Iteration 6384, Loss: 0.009050729684531689\n",
            "Iteration 6385, Loss: 0.009953786619007587\n",
            "Iteration 6386, Loss: 0.011441538110375404\n",
            "Iteration 6387, Loss: 0.013465804979205132\n",
            "Iteration 6388, Loss: 0.014676847495138645\n",
            "Iteration 6389, Loss: 0.012686998583376408\n",
            "Iteration 6390, Loss: 0.013065285049378872\n",
            "Iteration 6391, Loss: 0.008259761147201061\n",
            "Iteration 6392, Loss: 0.01508971769362688\n",
            "Iteration 6393, Loss: 0.01617508940398693\n",
            "Iteration 6394, Loss: 0.00633293716236949\n",
            "Iteration 6395, Loss: 0.015812642872333527\n",
            "Iteration 6396, Loss: 0.011701206676661968\n",
            "Iteration 6397, Loss: 0.014068334363400936\n",
            "Iteration 6398, Loss: 0.01639890857040882\n",
            "Iteration 6399, Loss: 0.009356200695037842\n",
            "Iteration 6400, Loss: 0.006907948292791843\n",
            "Iteration 6401, Loss: 0.008944729343056679\n",
            "Iteration 6402, Loss: 0.01350381225347519\n",
            "Iteration 6403, Loss: 0.013883452862501144\n",
            "Iteration 6404, Loss: 0.00808035023510456\n",
            "Iteration 6405, Loss: 0.011922562494874\n",
            "Iteration 6406, Loss: 0.00973247829824686\n",
            "Iteration 6407, Loss: 0.009795340709388256\n",
            "Iteration 6408, Loss: 0.011405165307223797\n",
            "Iteration 6409, Loss: 0.012823178432881832\n",
            "Iteration 6410, Loss: 0.00649685924872756\n",
            "Iteration 6411, Loss: 0.016340866684913635\n",
            "Iteration 6412, Loss: 0.014276058413088322\n",
            "Iteration 6413, Loss: 0.009298333898186684\n",
            "Iteration 6414, Loss: 0.0064195431768894196\n",
            "Iteration 6415, Loss: 0.012163388542830944\n",
            "Iteration 6416, Loss: 0.009377749636769295\n",
            "Iteration 6417, Loss: 0.006513367872685194\n",
            "Iteration 6418, Loss: 0.0072600687853991985\n",
            "Iteration 6419, Loss: 0.012501316145062447\n",
            "Iteration 6420, Loss: 0.01613960973918438\n",
            "Iteration 6421, Loss: 0.012979825027287006\n",
            "Iteration 6422, Loss: 0.006508800201117992\n",
            "Iteration 6423, Loss: 0.01662413589656353\n",
            "Iteration 6424, Loss: 0.007579719182103872\n",
            "Iteration 6425, Loss: 0.007895240560173988\n",
            "Iteration 6426, Loss: 0.01875169761478901\n",
            "Iteration 6427, Loss: 0.0157589353621006\n",
            "Iteration 6428, Loss: 0.011466865427792072\n",
            "Iteration 6429, Loss: 0.015701860189437866\n",
            "Iteration 6430, Loss: 0.008778777904808521\n",
            "Iteration 6431, Loss: 0.010130263864994049\n",
            "Iteration 6432, Loss: 0.009400205686688423\n",
            "Iteration 6433, Loss: 0.009295676834881306\n",
            "Iteration 6434, Loss: 0.010030506178736687\n",
            "Iteration 6435, Loss: 0.007746551185846329\n",
            "Iteration 6436, Loss: 0.013755098916590214\n",
            "Iteration 6437, Loss: 0.01831802912056446\n",
            "Iteration 6438, Loss: 0.0072858151979744434\n",
            "Iteration 6439, Loss: 0.00858556292951107\n",
            "Iteration 6440, Loss: 0.009334838949143887\n",
            "Iteration 6441, Loss: 0.014131272211670876\n",
            "Iteration 6442, Loss: 0.01375670824199915\n",
            "Iteration 6443, Loss: 0.011200559325516224\n",
            "Iteration 6444, Loss: 0.014075422659516335\n",
            "Iteration 6445, Loss: 0.016363346949219704\n",
            "Iteration 6446, Loss: 0.012175217270851135\n",
            "Iteration 6447, Loss: 0.009573301300406456\n",
            "Iteration 6448, Loss: 0.01369959581643343\n",
            "Iteration 6449, Loss: 0.01113487035036087\n",
            "Iteration 6450, Loss: 0.014769204892218113\n",
            "Iteration 6451, Loss: 0.010136779397726059\n",
            "Iteration 6452, Loss: 0.011811159551143646\n",
            "Iteration 6453, Loss: 0.011658404022455215\n",
            "Iteration 6454, Loss: 0.010387533344328403\n",
            "Iteration 6455, Loss: 0.011281550861895084\n",
            "Iteration 6456, Loss: 0.009693007916212082\n",
            "Iteration 6457, Loss: 0.007117941975593567\n",
            "Iteration 6458, Loss: 0.018226804211735725\n",
            "Iteration 6459, Loss: 0.011229624971747398\n",
            "Iteration 6460, Loss: 0.00657649477943778\n",
            "Iteration 6461, Loss: 0.014247463084757328\n",
            "Iteration 6462, Loss: 0.00974366907030344\n",
            "Iteration 6463, Loss: 0.011291329748928547\n",
            "Iteration 6464, Loss: 0.016209367662668228\n",
            "Iteration 6465, Loss: 0.011824699118733406\n",
            "Iteration 6466, Loss: 0.009377103298902512\n",
            "Iteration 6467, Loss: 0.011292578652501106\n",
            "Iteration 6468, Loss: 0.013776416890323162\n",
            "Iteration 6469, Loss: 0.009750569239258766\n",
            "Iteration 6470, Loss: 0.007537631783634424\n",
            "Iteration 6471, Loss: 0.01365493144840002\n",
            "Iteration 6472, Loss: 0.014657633379101753\n",
            "Iteration 6473, Loss: 0.014405585825443268\n",
            "Iteration 6474, Loss: 0.010291668586432934\n",
            "Iteration 6475, Loss: 0.014854315668344498\n",
            "Iteration 6476, Loss: 0.01214004959911108\n",
            "Iteration 6477, Loss: 0.009212996810674667\n",
            "Iteration 6478, Loss: 0.007710958365350962\n",
            "Iteration 6479, Loss: 0.012015934102237225\n",
            "Iteration 6480, Loss: 0.006832821760326624\n",
            "Iteration 6481, Loss: 0.01585104502737522\n",
            "Iteration 6482, Loss: 0.014589954167604446\n",
            "Iteration 6483, Loss: 0.013861186802387238\n",
            "Iteration 6484, Loss: 0.0164856668561697\n",
            "Iteration 6485, Loss: 0.011848416179418564\n",
            "Iteration 6486, Loss: 0.010892276652157307\n",
            "Iteration 6487, Loss: 0.011890416033565998\n",
            "Iteration 6488, Loss: 0.014203681610524654\n",
            "Iteration 6489, Loss: 0.010121599771082401\n",
            "Iteration 6490, Loss: 0.014083067886531353\n",
            "Iteration 6491, Loss: 0.008317300118505955\n",
            "Iteration 6492, Loss: 0.009902344085276127\n",
            "Iteration 6493, Loss: 0.012785025872290134\n",
            "Iteration 6494, Loss: 0.01029137521982193\n",
            "Iteration 6495, Loss: 0.008623788133263588\n",
            "Iteration 6496, Loss: 0.009514396078884602\n",
            "Iteration 6497, Loss: 0.009072558023035526\n",
            "Iteration 6498, Loss: 0.011799519881606102\n",
            "Iteration 6499, Loss: 0.01246197521686554\n",
            "Iteration 6500, Loss: 0.011393049731850624\n",
            "Iteration 6501, Loss: 0.01027527917176485\n",
            "Iteration 6502, Loss: 0.009705966338515282\n",
            "Iteration 6503, Loss: 0.011541277170181274\n",
            "Iteration 6504, Loss: 0.011557337827980518\n",
            "Iteration 6505, Loss: 0.009619190357625484\n",
            "Iteration 6506, Loss: 0.015642495825886726\n",
            "Iteration 6507, Loss: 0.013828547671437263\n",
            "Iteration 6508, Loss: 0.008717227727174759\n",
            "Iteration 6509, Loss: 0.008668278343975544\n",
            "Iteration 6510, Loss: 0.011529989540576935\n",
            "Iteration 6511, Loss: 0.01502911001443863\n",
            "Iteration 6512, Loss: 0.01314306166023016\n",
            "Iteration 6513, Loss: 0.007712559774518013\n",
            "Iteration 6514, Loss: 0.006191969383507967\n",
            "Iteration 6515, Loss: 0.01357470266520977\n",
            "Iteration 6516, Loss: 0.012785553000867367\n",
            "Iteration 6517, Loss: 0.008649904280900955\n",
            "Iteration 6518, Loss: 0.0073797693476080894\n",
            "Iteration 6519, Loss: 0.012578856199979782\n",
            "Iteration 6520, Loss: 0.01205529272556305\n",
            "Iteration 6521, Loss: 0.014020321890711784\n",
            "Iteration 6522, Loss: 0.013375280424952507\n",
            "Iteration 6523, Loss: 0.014120479114353657\n",
            "Iteration 6524, Loss: 0.016623780131340027\n",
            "Iteration 6525, Loss: 0.006998416036367416\n",
            "Iteration 6526, Loss: 0.014358628541231155\n",
            "Iteration 6527, Loss: 0.010269728489220142\n",
            "Iteration 6528, Loss: 0.008800315670669079\n",
            "Iteration 6529, Loss: 0.013822410255670547\n",
            "Iteration 6530, Loss: 0.011113356798887253\n",
            "Iteration 6531, Loss: 0.010273841209709644\n",
            "Iteration 6532, Loss: 0.00985549483448267\n",
            "Iteration 6533, Loss: 0.009345163591206074\n",
            "Iteration 6534, Loss: 0.009367402642965317\n",
            "Iteration 6535, Loss: 0.011234183795750141\n",
            "Iteration 6536, Loss: 0.0075323148630559444\n",
            "Iteration 6537, Loss: 0.010936685837805271\n",
            "Iteration 6538, Loss: 0.015817830339074135\n",
            "Iteration 6539, Loss: 0.008342732675373554\n",
            "Iteration 6540, Loss: 0.014872102998197079\n",
            "Iteration 6541, Loss: 0.01484287716448307\n",
            "Iteration 6542, Loss: 0.010057579725980759\n",
            "Iteration 6543, Loss: 0.010678470134735107\n",
            "Iteration 6544, Loss: 0.008666835725307465\n",
            "Iteration 6545, Loss: 0.014029034413397312\n",
            "Iteration 6546, Loss: 0.015219956636428833\n",
            "Iteration 6547, Loss: 0.012129699811339378\n",
            "Iteration 6548, Loss: 0.007868356071412563\n",
            "Iteration 6549, Loss: 0.008044692687690258\n",
            "Iteration 6550, Loss: 0.015766944736242294\n",
            "Iteration 6551, Loss: 0.014343720860779285\n",
            "Iteration 6552, Loss: 0.008115969598293304\n",
            "Iteration 6553, Loss: 0.01103147305548191\n",
            "Iteration 6554, Loss: 0.014324803836643696\n",
            "Iteration 6555, Loss: 0.011648117564618587\n",
            "Iteration 6556, Loss: 0.008803009986877441\n",
            "Iteration 6557, Loss: 0.01709911786019802\n",
            "Iteration 6558, Loss: 0.011485790833830833\n",
            "Iteration 6559, Loss: 0.01157215517014265\n",
            "Iteration 6560, Loss: 0.01151615846902132\n",
            "Iteration 6561, Loss: 0.013721805065870285\n",
            "Iteration 6562, Loss: 0.013682601042091846\n",
            "Iteration 6563, Loss: 0.007454000413417816\n",
            "Iteration 6564, Loss: 0.008894887752830982\n",
            "Iteration 6565, Loss: 0.01242323312908411\n",
            "Iteration 6566, Loss: 0.011701859533786774\n",
            "Iteration 6567, Loss: 0.011696492321789265\n",
            "Iteration 6568, Loss: 0.010201034136116505\n",
            "Iteration 6569, Loss: 0.011255859397351742\n",
            "Iteration 6570, Loss: 0.012296070344746113\n",
            "Iteration 6571, Loss: 0.011888407170772552\n",
            "Iteration 6572, Loss: 0.011153277941048145\n",
            "Iteration 6573, Loss: 0.01344939973205328\n",
            "Iteration 6574, Loss: 0.010780390352010727\n",
            "Iteration 6575, Loss: 0.0076584592461586\n",
            "Iteration 6576, Loss: 0.01129462756216526\n",
            "Iteration 6577, Loss: 0.011687124148011208\n",
            "Iteration 6578, Loss: 0.01704433560371399\n",
            "Iteration 6579, Loss: 0.01396686490625143\n",
            "Iteration 6580, Loss: 0.009649213403463364\n",
            "Iteration 6581, Loss: 0.008909626863896847\n",
            "Iteration 6582, Loss: 0.010935403406620026\n",
            "Iteration 6583, Loss: 0.009385842829942703\n",
            "Iteration 6584, Loss: 0.012973974458873272\n",
            "Iteration 6585, Loss: 0.013685831800103188\n",
            "Iteration 6586, Loss: 0.015073562040925026\n",
            "Iteration 6587, Loss: 0.010875923559069633\n",
            "Iteration 6588, Loss: 0.013050519861280918\n",
            "Iteration 6589, Loss: 0.014075793325901031\n",
            "Iteration 6590, Loss: 0.013113492168486118\n",
            "Iteration 6591, Loss: 0.013245660811662674\n",
            "Iteration 6592, Loss: 0.00960445124655962\n",
            "Iteration 6593, Loss: 0.014868920668959618\n",
            "Iteration 6594, Loss: 0.013016422279179096\n",
            "Iteration 6595, Loss: 0.009510050527751446\n",
            "Iteration 6596, Loss: 0.00882181990891695\n",
            "Iteration 6597, Loss: 0.012543903663754463\n",
            "Iteration 6598, Loss: 0.012315618805587292\n",
            "Iteration 6599, Loss: 0.010909480974078178\n",
            "Iteration 6600, Loss: 0.010741503909230232\n",
            "Iteration 6601, Loss: 0.01265935692936182\n",
            "Iteration 6602, Loss: 0.012966501526534557\n",
            "Iteration 6603, Loss: 0.01030303817242384\n",
            "Iteration 6604, Loss: 0.011280861683189869\n",
            "Iteration 6605, Loss: 0.01100801583379507\n",
            "Iteration 6606, Loss: 0.01121250819414854\n",
            "Iteration 6607, Loss: 0.008145250380039215\n",
            "Iteration 6608, Loss: 0.016196126118302345\n",
            "Iteration 6609, Loss: 0.008503361605107784\n",
            "Iteration 6610, Loss: 0.014823718927800655\n",
            "Iteration 6611, Loss: 0.011448032222688198\n",
            "Iteration 6612, Loss: 0.016569510102272034\n",
            "Iteration 6613, Loss: 0.007572194095700979\n",
            "Iteration 6614, Loss: 0.010494050569832325\n",
            "Iteration 6615, Loss: 0.010422592051327229\n",
            "Iteration 6616, Loss: 0.011450107209384441\n",
            "Iteration 6617, Loss: 0.009454688988626003\n",
            "Iteration 6618, Loss: 0.011344042606651783\n",
            "Iteration 6619, Loss: 0.010696664452552795\n",
            "Iteration 6620, Loss: 0.013065921142697334\n",
            "Iteration 6621, Loss: 0.01338788028806448\n",
            "Iteration 6622, Loss: 0.006699301768094301\n",
            "Iteration 6623, Loss: 0.010364485904574394\n",
            "Iteration 6624, Loss: 0.011172125115990639\n",
            "Iteration 6625, Loss: 0.010891716927289963\n",
            "Iteration 6626, Loss: 0.01308987382799387\n",
            "Iteration 6627, Loss: 0.010306429117918015\n",
            "Iteration 6628, Loss: 0.014293181709945202\n",
            "Iteration 6629, Loss: 0.008373848162591457\n",
            "Iteration 6630, Loss: 0.008319001644849777\n",
            "Iteration 6631, Loss: 0.012213761918246746\n",
            "Iteration 6632, Loss: 0.008622596971690655\n",
            "Iteration 6633, Loss: 0.007580875419080257\n",
            "Iteration 6634, Loss: 0.009186838753521442\n",
            "Iteration 6635, Loss: 0.008308758959174156\n",
            "Iteration 6636, Loss: 0.006550428457558155\n",
            "Iteration 6637, Loss: 0.01673717051744461\n",
            "Iteration 6638, Loss: 0.015043163672089577\n",
            "Iteration 6639, Loss: 0.011767835356295109\n",
            "Iteration 6640, Loss: 0.012437926605343819\n",
            "Iteration 6641, Loss: 0.009684783406555653\n",
            "Iteration 6642, Loss: 0.011221983470022678\n",
            "Iteration 6643, Loss: 0.008610628545284271\n",
            "Iteration 6644, Loss: 0.011792035773396492\n",
            "Iteration 6645, Loss: 0.01175777055323124\n",
            "Iteration 6646, Loss: 0.0095669561997056\n",
            "Iteration 6647, Loss: 0.014077496714890003\n",
            "Iteration 6648, Loss: 0.012737169861793518\n",
            "Iteration 6649, Loss: 0.01180106122046709\n",
            "Iteration 6650, Loss: 0.010332661680877209\n",
            "Iteration 6651, Loss: 0.01064937375485897\n",
            "Iteration 6652, Loss: 0.012684921734035015\n",
            "Iteration 6653, Loss: 0.009527299553155899\n",
            "Iteration 6654, Loss: 0.011653385125100613\n",
            "Iteration 6655, Loss: 0.014090371318161488\n",
            "Iteration 6656, Loss: 0.006368846166878939\n",
            "Iteration 6657, Loss: 0.012992721050977707\n",
            "Iteration 6658, Loss: 0.01058492437005043\n",
            "Iteration 6659, Loss: 0.009066134691238403\n",
            "Iteration 6660, Loss: 0.011087382212281227\n",
            "Iteration 6661, Loss: 0.014682806096971035\n",
            "Iteration 6662, Loss: 0.00554178562015295\n",
            "Iteration 6663, Loss: 0.014663182199001312\n",
            "Iteration 6664, Loss: 0.011030809953808784\n",
            "Iteration 6665, Loss: 0.012467035092413425\n",
            "Iteration 6666, Loss: 0.008023733273148537\n",
            "Iteration 6667, Loss: 0.009231681935489178\n",
            "Iteration 6668, Loss: 0.007294593844562769\n",
            "Iteration 6669, Loss: 0.0153512479737401\n",
            "Iteration 6670, Loss: 0.015240867622196674\n",
            "Iteration 6671, Loss: 0.012434938922524452\n",
            "Iteration 6672, Loss: 0.009821023792028427\n",
            "Iteration 6673, Loss: 0.01642102189362049\n",
            "Iteration 6674, Loss: 0.009971342980861664\n",
            "Iteration 6675, Loss: 0.009327929466962814\n",
            "Iteration 6676, Loss: 0.013059156015515327\n",
            "Iteration 6677, Loss: 0.010302609764039516\n",
            "Iteration 6678, Loss: 0.010411768220365047\n",
            "Iteration 6679, Loss: 0.012318631634116173\n",
            "Iteration 6680, Loss: 0.013597384095191956\n",
            "Iteration 6681, Loss: 0.007306433282792568\n",
            "Iteration 6682, Loss: 0.012349247001111507\n",
            "Iteration 6683, Loss: 0.008033295162022114\n",
            "Iteration 6684, Loss: 0.01001510489732027\n",
            "Iteration 6685, Loss: 0.01824200712144375\n",
            "Iteration 6686, Loss: 0.007939442992210388\n",
            "Iteration 6687, Loss: 0.009526200592517853\n",
            "Iteration 6688, Loss: 0.008964166045188904\n",
            "Iteration 6689, Loss: 0.01845371164381504\n",
            "Iteration 6690, Loss: 0.01626559905707836\n",
            "Iteration 6691, Loss: 0.00725356163457036\n",
            "Iteration 6692, Loss: 0.006819895002990961\n",
            "Iteration 6693, Loss: 0.00742028234526515\n",
            "Iteration 6694, Loss: 0.011422255076467991\n",
            "Iteration 6695, Loss: 0.01149684563279152\n",
            "Iteration 6696, Loss: 0.012930131517350674\n",
            "Iteration 6697, Loss: 0.011629263870418072\n",
            "Iteration 6698, Loss: 0.010644445195794106\n",
            "Iteration 6699, Loss: 0.014003362506628036\n",
            "Iteration 6700, Loss: 0.014361795969307423\n",
            "Iteration 6701, Loss: 0.010394261218607426\n",
            "Iteration 6702, Loss: 0.010663892142474651\n",
            "Iteration 6703, Loss: 0.014287111349403858\n",
            "Iteration 6704, Loss: 0.012995126657187939\n",
            "Iteration 6705, Loss: 0.009504929184913635\n",
            "Iteration 6706, Loss: 0.011058691889047623\n",
            "Iteration 6707, Loss: 0.010133360512554646\n",
            "Iteration 6708, Loss: 0.008085177280008793\n",
            "Iteration 6709, Loss: 0.012438965030014515\n",
            "Iteration 6710, Loss: 0.0073471409268677235\n",
            "Iteration 6711, Loss: 0.013754432089626789\n",
            "Iteration 6712, Loss: 0.012780174612998962\n",
            "Iteration 6713, Loss: 0.014023401774466038\n",
            "Iteration 6714, Loss: 0.006276754662394524\n",
            "Iteration 6715, Loss: 0.009541621431708336\n",
            "Iteration 6716, Loss: 0.012132814154028893\n",
            "Iteration 6717, Loss: 0.009870759211480618\n",
            "Iteration 6718, Loss: 0.01086856983602047\n",
            "Iteration 6719, Loss: 0.011188955046236515\n",
            "Iteration 6720, Loss: 0.016275722533464432\n",
            "Iteration 6721, Loss: 0.009159586392343044\n",
            "Iteration 6722, Loss: 0.007376506924629211\n",
            "Iteration 6723, Loss: 0.009563401341438293\n",
            "Iteration 6724, Loss: 0.014700054191052914\n",
            "Iteration 6725, Loss: 0.01312747411429882\n",
            "Iteration 6726, Loss: 0.015377537347376347\n",
            "Iteration 6727, Loss: 0.01891121082007885\n",
            "Iteration 6728, Loss: 0.01602158509194851\n",
            "Iteration 6729, Loss: 0.007436094805598259\n",
            "Iteration 6730, Loss: 0.0068756528198719025\n",
            "Iteration 6731, Loss: 0.013917498290538788\n",
            "Iteration 6732, Loss: 0.011607411317527294\n",
            "Iteration 6733, Loss: 0.008540455251932144\n",
            "Iteration 6734, Loss: 0.014090227894484997\n",
            "Iteration 6735, Loss: 0.012136010453104973\n",
            "Iteration 6736, Loss: 0.007503950502723455\n",
            "Iteration 6737, Loss: 0.013200231827795506\n",
            "Iteration 6738, Loss: 0.008779053576290607\n",
            "Iteration 6739, Loss: 0.008296823129057884\n",
            "Iteration 6740, Loss: 0.01434507966041565\n",
            "Iteration 6741, Loss: 0.012003227137029171\n",
            "Iteration 6742, Loss: 0.01062929816544056\n",
            "Iteration 6743, Loss: 0.00901066605001688\n",
            "Iteration 6744, Loss: 0.008859435096383095\n",
            "Iteration 6745, Loss: 0.008387668989598751\n",
            "Iteration 6746, Loss: 0.00989004597067833\n",
            "Iteration 6747, Loss: 0.012291740626096725\n",
            "Iteration 6748, Loss: 0.011574319563806057\n",
            "Iteration 6749, Loss: 0.012184648774564266\n",
            "Iteration 6750, Loss: 0.009323366917669773\n",
            "Iteration 6751, Loss: 0.010859640315175056\n",
            "Iteration 6752, Loss: 0.008294014260172844\n",
            "Iteration 6753, Loss: 0.0142015740275383\n",
            "Iteration 6754, Loss: 0.008510695770382881\n",
            "Iteration 6755, Loss: 0.01156680192798376\n",
            "Iteration 6756, Loss: 0.013373582623898983\n",
            "Iteration 6757, Loss: 0.011549119837582111\n",
            "Iteration 6758, Loss: 0.01025326270610094\n",
            "Iteration 6759, Loss: 0.009525668807327747\n",
            "Iteration 6760, Loss: 0.00786692462861538\n",
            "Iteration 6761, Loss: 0.0135053601115942\n",
            "Iteration 6762, Loss: 0.005928661208599806\n",
            "Iteration 6763, Loss: 0.007261907681822777\n",
            "Iteration 6764, Loss: 0.007676859386265278\n",
            "Iteration 6765, Loss: 0.008684365078806877\n",
            "Iteration 6766, Loss: 0.010112682357430458\n",
            "Iteration 6767, Loss: 0.01207008957862854\n",
            "Iteration 6768, Loss: 0.011604784987866879\n",
            "Iteration 6769, Loss: 0.014478081837296486\n",
            "Iteration 6770, Loss: 0.010870717465877533\n",
            "Iteration 6771, Loss: 0.012916067615151405\n",
            "Iteration 6772, Loss: 0.006731756031513214\n",
            "Iteration 6773, Loss: 0.008894272148609161\n",
            "Iteration 6774, Loss: 0.006990558933466673\n",
            "Iteration 6775, Loss: 0.012550040148198605\n",
            "Iteration 6776, Loss: 0.009695786982774734\n",
            "Iteration 6777, Loss: 0.010400992818176746\n",
            "Iteration 6778, Loss: 0.013014750555157661\n",
            "Iteration 6779, Loss: 0.014639176428318024\n",
            "Iteration 6780, Loss: 0.011262981221079826\n",
            "Iteration 6781, Loss: 0.011097160167992115\n",
            "Iteration 6782, Loss: 0.010861790738999844\n",
            "Iteration 6783, Loss: 0.008955316618084908\n",
            "Iteration 6784, Loss: 0.01300756260752678\n",
            "Iteration 6785, Loss: 0.005681786686182022\n",
            "Iteration 6786, Loss: 0.010099798440933228\n",
            "Iteration 6787, Loss: 0.009221885353326797\n",
            "Iteration 6788, Loss: 0.015721481293439865\n",
            "Iteration 6789, Loss: 0.00988723710179329\n",
            "Iteration 6790, Loss: 0.01131412759423256\n",
            "Iteration 6791, Loss: 0.01025015115737915\n",
            "Iteration 6792, Loss: 0.009513577446341515\n",
            "Iteration 6793, Loss: 0.012178643606603146\n",
            "Iteration 6794, Loss: 0.015451266430318356\n",
            "Iteration 6795, Loss: 0.009734186343848705\n",
            "Iteration 6796, Loss: 0.013815478421747684\n",
            "Iteration 6797, Loss: 0.012125054374337196\n",
            "Iteration 6798, Loss: 0.012393762357532978\n",
            "Iteration 6799, Loss: 0.008991585113108158\n",
            "Iteration 6800, Loss: 0.01272151991724968\n",
            "Iteration 6801, Loss: 0.013453511521220207\n",
            "Iteration 6802, Loss: 0.010250932537019253\n",
            "Iteration 6803, Loss: 0.013924388214945793\n",
            "Iteration 6804, Loss: 0.011053510941565037\n",
            "Iteration 6805, Loss: 0.01714778132736683\n",
            "Iteration 6806, Loss: 0.010450808331370354\n",
            "Iteration 6807, Loss: 0.008564593270421028\n",
            "Iteration 6808, Loss: 0.012509312480688095\n",
            "Iteration 6809, Loss: 0.013656653463840485\n",
            "Iteration 6810, Loss: 0.010621675290167332\n",
            "Iteration 6811, Loss: 0.010218797251582146\n",
            "Iteration 6812, Loss: 0.012057344429194927\n",
            "Iteration 6813, Loss: 0.014245626516640186\n",
            "Iteration 6814, Loss: 0.015940820798277855\n",
            "Iteration 6815, Loss: 0.014561905525624752\n",
            "Iteration 6816, Loss: 0.010532672517001629\n",
            "Iteration 6817, Loss: 0.011994163505733013\n",
            "Iteration 6818, Loss: 0.008421685546636581\n",
            "Iteration 6819, Loss: 0.010026705451309681\n",
            "Iteration 6820, Loss: 0.01354370079934597\n",
            "Iteration 6821, Loss: 0.008299658074975014\n",
            "Iteration 6822, Loss: 0.010530287399888039\n",
            "Iteration 6823, Loss: 0.008486364036798477\n",
            "Iteration 6824, Loss: 0.011944638565182686\n",
            "Iteration 6825, Loss: 0.008658714592456818\n",
            "Iteration 6826, Loss: 0.014075135812163353\n",
            "Iteration 6827, Loss: 0.015783291310071945\n",
            "Iteration 6828, Loss: 0.013945240527391434\n",
            "Iteration 6829, Loss: 0.013913561590015888\n",
            "Iteration 6830, Loss: 0.00922592356801033\n",
            "Iteration 6831, Loss: 0.010911766439676285\n",
            "Iteration 6832, Loss: 0.012284412980079651\n",
            "Iteration 6833, Loss: 0.012146414257586002\n",
            "Iteration 6834, Loss: 0.014214822091162205\n",
            "Iteration 6835, Loss: 0.015668384730815887\n",
            "Iteration 6836, Loss: 0.016355197876691818\n",
            "Iteration 6837, Loss: 0.016274169087409973\n",
            "Iteration 6838, Loss: 0.011742628179490566\n",
            "Iteration 6839, Loss: 0.014870509505271912\n",
            "Iteration 6840, Loss: 0.014048599638044834\n",
            "Iteration 6841, Loss: 0.014901786111295223\n",
            "Iteration 6842, Loss: 0.0069583626464009285\n",
            "Iteration 6843, Loss: 0.015254838392138481\n",
            "Iteration 6844, Loss: 0.009505923837423325\n",
            "Iteration 6845, Loss: 0.009378171525895596\n",
            "Iteration 6846, Loss: 0.008112210780382156\n",
            "Iteration 6847, Loss: 0.01176322903484106\n",
            "Iteration 6848, Loss: 0.014415087178349495\n",
            "Iteration 6849, Loss: 0.009989601559937\n",
            "Iteration 6850, Loss: 0.013746531680226326\n",
            "Iteration 6851, Loss: 0.01219685934484005\n",
            "Iteration 6852, Loss: 0.01119482982903719\n",
            "Iteration 6853, Loss: 0.008284500800073147\n",
            "Iteration 6854, Loss: 0.017452513799071312\n",
            "Iteration 6855, Loss: 0.009200464934110641\n",
            "Iteration 6856, Loss: 0.01493469811975956\n",
            "Iteration 6857, Loss: 0.008949141949415207\n",
            "Iteration 6858, Loss: 0.008387678302824497\n",
            "Iteration 6859, Loss: 0.009655230678617954\n",
            "Iteration 6860, Loss: 0.008953789249062538\n",
            "Iteration 6861, Loss: 0.009280819445848465\n",
            "Iteration 6862, Loss: 0.014427723363041878\n",
            "Iteration 6863, Loss: 0.014249629341065884\n",
            "Iteration 6864, Loss: 0.012188580818474293\n",
            "Iteration 6865, Loss: 0.010110237635672092\n",
            "Iteration 6866, Loss: 0.01295545231550932\n",
            "Iteration 6867, Loss: 0.01768222078680992\n",
            "Iteration 6868, Loss: 0.009405292570590973\n",
            "Iteration 6869, Loss: 0.01314625609666109\n",
            "Iteration 6870, Loss: 0.00820357259362936\n",
            "Iteration 6871, Loss: 0.012110373005270958\n",
            "Iteration 6872, Loss: 0.015021699480712414\n",
            "Iteration 6873, Loss: 0.01544913463294506\n",
            "Iteration 6874, Loss: 0.00984331127256155\n",
            "Iteration 6875, Loss: 0.00986669585108757\n",
            "Iteration 6876, Loss: 0.009881828911602497\n",
            "Iteration 6877, Loss: 0.0124112693592906\n",
            "Iteration 6878, Loss: 0.014245088212192059\n",
            "Iteration 6879, Loss: 0.009946410544216633\n",
            "Iteration 6880, Loss: 0.018412426114082336\n",
            "Iteration 6881, Loss: 0.01554208341985941\n",
            "Iteration 6882, Loss: 0.010103665292263031\n",
            "Iteration 6883, Loss: 0.014854155480861664\n",
            "Iteration 6884, Loss: 0.016437601298093796\n",
            "Iteration 6885, Loss: 0.01112150214612484\n",
            "Iteration 6886, Loss: 0.00919909868389368\n",
            "Iteration 6887, Loss: 0.00902970414608717\n",
            "Iteration 6888, Loss: 0.013929042033851147\n",
            "Iteration 6889, Loss: 0.01582638919353485\n",
            "Iteration 6890, Loss: 0.01926429383456707\n",
            "Iteration 6891, Loss: 0.006882940884679556\n",
            "Iteration 6892, Loss: 0.009977289475500584\n",
            "Iteration 6893, Loss: 0.007319429889321327\n",
            "Iteration 6894, Loss: 0.013202328234910965\n",
            "Iteration 6895, Loss: 0.012039094232022762\n",
            "Iteration 6896, Loss: 0.01313762180507183\n",
            "Iteration 6897, Loss: 0.010861415416002274\n",
            "Iteration 6898, Loss: 0.01061190664768219\n",
            "Iteration 6899, Loss: 0.010830615647137165\n",
            "Iteration 6900, Loss: 0.011227652430534363\n",
            "Iteration 6901, Loss: 0.014014991000294685\n",
            "Iteration 6902, Loss: 0.012740934267640114\n",
            "Iteration 6903, Loss: 0.018557552248239517\n",
            "Iteration 6904, Loss: 0.013721215538680553\n",
            "Iteration 6905, Loss: 0.01119482796639204\n",
            "Iteration 6906, Loss: 0.01201125793159008\n",
            "Iteration 6907, Loss: 0.012239412404596806\n",
            "Iteration 6908, Loss: 0.015423188917338848\n",
            "Iteration 6909, Loss: 0.008876310661435127\n",
            "Iteration 6910, Loss: 0.0135183772072196\n",
            "Iteration 6911, Loss: 0.012889420613646507\n",
            "Iteration 6912, Loss: 0.010607503354549408\n",
            "Iteration 6913, Loss: 0.011424044147133827\n",
            "Iteration 6914, Loss: 0.01127376314252615\n",
            "Iteration 6915, Loss: 0.007260862272232771\n",
            "Iteration 6916, Loss: 0.014371679164469242\n",
            "Iteration 6917, Loss: 0.012279285117983818\n",
            "Iteration 6918, Loss: 0.012936936691403389\n",
            "Iteration 6919, Loss: 0.01148935779929161\n",
            "Iteration 6920, Loss: 0.01214727945625782\n",
            "Iteration 6921, Loss: 0.010067125782370567\n",
            "Iteration 6922, Loss: 0.01636371575295925\n",
            "Iteration 6923, Loss: 0.004619669169187546\n",
            "Iteration 6924, Loss: 0.011081621050834656\n",
            "Iteration 6925, Loss: 0.013652760535478592\n",
            "Iteration 6926, Loss: 0.012109685689210892\n",
            "Iteration 6927, Loss: 0.007607444655150175\n",
            "Iteration 6928, Loss: 0.008976452052593231\n",
            "Iteration 6929, Loss: 0.008815508335828781\n",
            "Iteration 6930, Loss: 0.008612122386693954\n",
            "Iteration 6931, Loss: 0.010054949671030045\n",
            "Iteration 6932, Loss: 0.011251055635511875\n",
            "Iteration 6933, Loss: 0.011014633812010288\n",
            "Iteration 6934, Loss: 0.012932292185723782\n",
            "Iteration 6935, Loss: 0.017777252942323685\n",
            "Iteration 6936, Loss: 0.017193539068102837\n",
            "Iteration 6937, Loss: 0.012205749750137329\n",
            "Iteration 6938, Loss: 0.013644885271787643\n",
            "Iteration 6939, Loss: 0.014569852501153946\n",
            "Iteration 6940, Loss: 0.010600359179079533\n",
            "Iteration 6941, Loss: 0.012068499810993671\n",
            "Iteration 6942, Loss: 0.009607129730284214\n",
            "Iteration 6943, Loss: 0.00880291685461998\n",
            "Iteration 6944, Loss: 0.013624096289277077\n",
            "Iteration 6945, Loss: 0.017590966075658798\n",
            "Iteration 6946, Loss: 0.0070040225982666016\n",
            "Iteration 6947, Loss: 0.011914083734154701\n",
            "Iteration 6948, Loss: 0.013505566865205765\n",
            "Iteration 6949, Loss: 0.011793157085776329\n",
            "Iteration 6950, Loss: 0.010579309426248074\n",
            "Iteration 6951, Loss: 0.013157200999557972\n",
            "Iteration 6952, Loss: 0.010026473551988602\n",
            "Iteration 6953, Loss: 0.012357563711702824\n",
            "Iteration 6954, Loss: 0.009938089177012444\n",
            "Iteration 6955, Loss: 0.017352361232042313\n",
            "Iteration 6956, Loss: 0.01552128791809082\n",
            "Iteration 6957, Loss: 0.011992285028100014\n",
            "Iteration 6958, Loss: 0.0072657521814107895\n",
            "Iteration 6959, Loss: 0.009507637470960617\n",
            "Iteration 6960, Loss: 0.008647380396723747\n",
            "Iteration 6961, Loss: 0.016772814095020294\n",
            "Iteration 6962, Loss: 0.009684668853878975\n",
            "Iteration 6963, Loss: 0.01739366166293621\n",
            "Iteration 6964, Loss: 0.014804452657699585\n",
            "Iteration 6965, Loss: 0.01332128420472145\n",
            "Iteration 6966, Loss: 0.01340743899345398\n",
            "Iteration 6967, Loss: 0.010371837764978409\n",
            "Iteration 6968, Loss: 0.015131267718970776\n",
            "Iteration 6969, Loss: 0.013201575726270676\n",
            "Iteration 6970, Loss: 0.00747392512857914\n",
            "Iteration 6971, Loss: 0.016786718741059303\n",
            "Iteration 6972, Loss: 0.009237486869096756\n",
            "Iteration 6973, Loss: 0.007138871122151613\n",
            "Iteration 6974, Loss: 0.0036598355509340763\n",
            "Iteration 6975, Loss: 0.00880312267690897\n",
            "Iteration 6976, Loss: 0.008284089155495167\n",
            "Iteration 6977, Loss: 0.011241711676120758\n",
            "Iteration 6978, Loss: 0.013416560366749763\n",
            "Iteration 6979, Loss: 0.010152262635529041\n",
            "Iteration 6980, Loss: 0.01219809427857399\n",
            "Iteration 6981, Loss: 0.013489307835698128\n",
            "Iteration 6982, Loss: 0.016094842925667763\n",
            "Iteration 6983, Loss: 0.012083776295185089\n",
            "Iteration 6984, Loss: 0.016701778396964073\n",
            "Iteration 6985, Loss: 0.008838662877678871\n",
            "Iteration 6986, Loss: 0.008630940690636635\n",
            "Iteration 6987, Loss: 0.006388160865753889\n",
            "Iteration 6988, Loss: 0.010571171529591084\n",
            "Iteration 6989, Loss: 0.01367118302732706\n",
            "Iteration 6990, Loss: 0.007277687080204487\n",
            "Iteration 6991, Loss: 0.011485077440738678\n",
            "Iteration 6992, Loss: 0.006617926061153412\n",
            "Iteration 6993, Loss: 0.013482497073709965\n",
            "Iteration 6994, Loss: 0.010069254785776138\n",
            "Iteration 6995, Loss: 0.010269411839544773\n",
            "Iteration 6996, Loss: 0.00999189168214798\n",
            "Iteration 6997, Loss: 0.010260326787829399\n",
            "Iteration 6998, Loss: 0.009121755138039589\n",
            "Iteration 6999, Loss: 0.013883253559470177\n",
            "Iteration 7000, Loss: 0.007592212874442339\n",
            "Test Loss: 0.06376121938228607\n",
            "Iteration 7001, Loss: 0.007381667383015156\n",
            "Iteration 7002, Loss: 0.01858990266919136\n",
            "Iteration 7003, Loss: 0.01303868182003498\n",
            "Iteration 7004, Loss: 0.009312600828707218\n",
            "Iteration 7005, Loss: 0.005948589649051428\n",
            "Iteration 7006, Loss: 0.011285568587481976\n",
            "Iteration 7007, Loss: 0.012503362260758877\n",
            "Iteration 7008, Loss: 0.02097386308014393\n",
            "Iteration 7009, Loss: 0.011249242350459099\n",
            "Iteration 7010, Loss: 0.011143152602016926\n",
            "Iteration 7011, Loss: 0.01167046558111906\n",
            "Iteration 7012, Loss: 0.007894963957369328\n",
            "Iteration 7013, Loss: 0.011080573312938213\n",
            "Iteration 7014, Loss: 0.013589965179562569\n",
            "Iteration 7015, Loss: 0.007492612581700087\n",
            "Iteration 7016, Loss: 0.014930871315300465\n",
            "Iteration 7017, Loss: 0.015062971971929073\n",
            "Iteration 7018, Loss: 0.011900469660758972\n",
            "Iteration 7019, Loss: 0.012984308414161205\n",
            "Iteration 7020, Loss: 0.014031488448381424\n",
            "Iteration 7021, Loss: 0.013878143392503262\n",
            "Iteration 7022, Loss: 0.016405906528234482\n",
            "Iteration 7023, Loss: 0.011378913186490536\n",
            "Iteration 7024, Loss: 0.015009598806500435\n",
            "Iteration 7025, Loss: 0.01040616910904646\n",
            "Iteration 7026, Loss: 0.010978536680340767\n",
            "Iteration 7027, Loss: 0.007970481179654598\n",
            "Iteration 7028, Loss: 0.00993360485881567\n",
            "Iteration 7029, Loss: 0.009676482528448105\n",
            "Iteration 7030, Loss: 0.011225797235965729\n",
            "Iteration 7031, Loss: 0.007497864775359631\n",
            "Iteration 7032, Loss: 0.012899346649646759\n",
            "Iteration 7033, Loss: 0.010305183939635754\n",
            "Iteration 7034, Loss: 0.013002836145460606\n",
            "Iteration 7035, Loss: 0.011185074225068092\n",
            "Iteration 7036, Loss: 0.009170806966722012\n",
            "Iteration 7037, Loss: 0.011515101417899132\n",
            "Iteration 7038, Loss: 0.014836014248430729\n",
            "Iteration 7039, Loss: 0.013296406716108322\n",
            "Iteration 7040, Loss: 0.01409714575856924\n",
            "Iteration 7041, Loss: 0.010057130828499794\n",
            "Iteration 7042, Loss: 0.009960375726222992\n",
            "Iteration 7043, Loss: 0.006218550261110067\n",
            "Iteration 7044, Loss: 0.013224678114056587\n",
            "Iteration 7045, Loss: 0.008061030879616737\n",
            "Iteration 7046, Loss: 0.013399917632341385\n",
            "Iteration 7047, Loss: 0.009196336381137371\n",
            "Iteration 7048, Loss: 0.014259197749197483\n",
            "Iteration 7049, Loss: 0.007503869477659464\n",
            "Iteration 7050, Loss: 0.01043143030256033\n",
            "Iteration 7051, Loss: 0.012727668508887291\n",
            "Iteration 7052, Loss: 0.008366476744413376\n",
            "Iteration 7053, Loss: 0.011815637350082397\n",
            "Iteration 7054, Loss: 0.01427076943218708\n",
            "Iteration 7055, Loss: 0.015881966799497604\n",
            "Iteration 7056, Loss: 0.01492325123399496\n",
            "Iteration 7057, Loss: 0.011059396900236607\n",
            "Iteration 7058, Loss: 0.015704507008194923\n",
            "Iteration 7059, Loss: 0.014769692905247211\n",
            "Iteration 7060, Loss: 0.009435542859137058\n",
            "Iteration 7061, Loss: 0.009525515139102936\n",
            "Iteration 7062, Loss: 0.013041894882917404\n",
            "Iteration 7063, Loss: 0.00981611292809248\n",
            "Iteration 7064, Loss: 0.015386529266834259\n",
            "Iteration 7065, Loss: 0.014809240587055683\n",
            "Iteration 7066, Loss: 0.010329047217965126\n",
            "Iteration 7067, Loss: 0.01299643237143755\n",
            "Iteration 7068, Loss: 0.007559256628155708\n",
            "Iteration 7069, Loss: 0.013303004205226898\n",
            "Iteration 7070, Loss: 0.0126640098169446\n",
            "Iteration 7071, Loss: 0.008962390013039112\n",
            "Iteration 7072, Loss: 0.009782915003597736\n",
            "Iteration 7073, Loss: 0.01435801386833191\n",
            "Iteration 7074, Loss: 0.005118685774505138\n",
            "Iteration 7075, Loss: 0.01198617648333311\n",
            "Iteration 7076, Loss: 0.01298365369439125\n",
            "Iteration 7077, Loss: 0.0077672298066318035\n",
            "Iteration 7078, Loss: 0.009654837660491467\n",
            "Iteration 7079, Loss: 0.011723543517291546\n",
            "Iteration 7080, Loss: 0.011819598264992237\n",
            "Iteration 7081, Loss: 0.007659050170332193\n",
            "Iteration 7082, Loss: 0.0112733393907547\n",
            "Iteration 7083, Loss: 0.015820002183318138\n",
            "Iteration 7084, Loss: 0.013040859252214432\n",
            "Iteration 7085, Loss: 0.008329179137945175\n",
            "Iteration 7086, Loss: 0.007337622810155153\n",
            "Iteration 7087, Loss: 0.011092386208474636\n",
            "Iteration 7088, Loss: 0.00867387279868126\n",
            "Iteration 7089, Loss: 0.010346349328756332\n",
            "Iteration 7090, Loss: 0.013882853090763092\n",
            "Iteration 7091, Loss: 0.008473637513816357\n",
            "Iteration 7092, Loss: 0.014135242439806461\n",
            "Iteration 7093, Loss: 0.008071210235357285\n",
            "Iteration 7094, Loss: 0.012603997252881527\n",
            "Iteration 7095, Loss: 0.00749058136716485\n",
            "Iteration 7096, Loss: 0.00915470439940691\n",
            "Iteration 7097, Loss: 0.008263062685728073\n",
            "Iteration 7098, Loss: 0.012249321676790714\n",
            "Iteration 7099, Loss: 0.0063062128610908985\n",
            "Iteration 7100, Loss: 0.013781552202999592\n",
            "Iteration 7101, Loss: 0.012923777103424072\n",
            "Iteration 7102, Loss: 0.008349842391908169\n",
            "Iteration 7103, Loss: 0.01136234775185585\n",
            "Iteration 7104, Loss: 0.010250096209347248\n",
            "Iteration 7105, Loss: 0.010293846018612385\n",
            "Iteration 7106, Loss: 0.008494800888001919\n",
            "Iteration 7107, Loss: 0.012090700678527355\n",
            "Iteration 7108, Loss: 0.012164866551756859\n",
            "Iteration 7109, Loss: 0.02126746065914631\n",
            "Iteration 7110, Loss: 0.007649315521121025\n",
            "Iteration 7111, Loss: 0.008714167401194572\n",
            "Iteration 7112, Loss: 0.01350548304617405\n",
            "Iteration 7113, Loss: 0.00989064946770668\n",
            "Iteration 7114, Loss: 0.009004579856991768\n",
            "Iteration 7115, Loss: 0.007345275953412056\n",
            "Iteration 7116, Loss: 0.0053147319704294205\n",
            "Iteration 7117, Loss: 0.010178222320973873\n",
            "Iteration 7118, Loss: 0.01820828765630722\n",
            "Iteration 7119, Loss: 0.010455676354467869\n",
            "Iteration 7120, Loss: 0.013525668531656265\n",
            "Iteration 7121, Loss: 0.010658453218638897\n",
            "Iteration 7122, Loss: 0.01028341706842184\n",
            "Iteration 7123, Loss: 0.011306149885058403\n",
            "Iteration 7124, Loss: 0.012148316018283367\n",
            "Iteration 7125, Loss: 0.010602196678519249\n",
            "Iteration 7126, Loss: 0.011198295280337334\n",
            "Iteration 7127, Loss: 0.010710792616009712\n",
            "Iteration 7128, Loss: 0.007432278711348772\n",
            "Iteration 7129, Loss: 0.015337137505412102\n",
            "Iteration 7130, Loss: 0.012433269061148167\n",
            "Iteration 7131, Loss: 0.011027302592992783\n",
            "Iteration 7132, Loss: 0.013820042833685875\n",
            "Iteration 7133, Loss: 0.011395333334803581\n",
            "Iteration 7134, Loss: 0.012501183897256851\n",
            "Iteration 7135, Loss: 0.01233316957950592\n",
            "Iteration 7136, Loss: 0.00922474730759859\n",
            "Iteration 7137, Loss: 0.009433324448764324\n",
            "Iteration 7138, Loss: 0.010155126452445984\n",
            "Iteration 7139, Loss: 0.013260025531053543\n",
            "Iteration 7140, Loss: 0.014017149806022644\n",
            "Iteration 7141, Loss: 0.01125209778547287\n",
            "Iteration 7142, Loss: 0.012691219337284565\n",
            "Iteration 7143, Loss: 0.0042832414619624615\n",
            "Iteration 7144, Loss: 0.008936134167015553\n",
            "Iteration 7145, Loss: 0.0083241555839777\n",
            "Iteration 7146, Loss: 0.011905957944691181\n",
            "Iteration 7147, Loss: 0.013501345179975033\n",
            "Iteration 7148, Loss: 0.008862351067364216\n",
            "Iteration 7149, Loss: 0.005445849150419235\n",
            "Iteration 7150, Loss: 0.009683316573500633\n",
            "Iteration 7151, Loss: 0.009631585329771042\n",
            "Iteration 7152, Loss: 0.011891947127878666\n",
            "Iteration 7153, Loss: 0.013010072521865368\n",
            "Iteration 7154, Loss: 0.009927000850439072\n",
            "Iteration 7155, Loss: 0.010113703086972237\n",
            "Iteration 7156, Loss: 0.006512344349175692\n",
            "Iteration 7157, Loss: 0.00788014754652977\n",
            "Iteration 7158, Loss: 0.013252456672489643\n",
            "Iteration 7159, Loss: 0.011310514062643051\n",
            "Iteration 7160, Loss: 0.010928306728601456\n",
            "Iteration 7161, Loss: 0.012285641394555569\n",
            "Iteration 7162, Loss: 0.011983325704932213\n",
            "Iteration 7163, Loss: 0.009116478264331818\n",
            "Iteration 7164, Loss: 0.01576276868581772\n",
            "Iteration 7165, Loss: 0.01364913396537304\n",
            "Iteration 7166, Loss: 0.011211227625608444\n",
            "Iteration 7167, Loss: 0.010460780002176762\n",
            "Iteration 7168, Loss: 0.013351294212043285\n",
            "Iteration 7169, Loss: 0.005729882046580315\n",
            "Iteration 7170, Loss: 0.0109258396551013\n",
            "Iteration 7171, Loss: 0.013041936792433262\n",
            "Iteration 7172, Loss: 0.01009608805179596\n",
            "Iteration 7173, Loss: 0.010440818965435028\n",
            "Iteration 7174, Loss: 0.00963583029806614\n",
            "Iteration 7175, Loss: 0.014141159132122993\n",
            "Iteration 7176, Loss: 0.013334685005247593\n",
            "Iteration 7177, Loss: 0.007562149781733751\n",
            "Iteration 7178, Loss: 0.011204763315618038\n",
            "Iteration 7179, Loss: 0.012958189472556114\n",
            "Iteration 7180, Loss: 0.008564061485230923\n",
            "Iteration 7181, Loss: 0.012526833452284336\n",
            "Iteration 7182, Loss: 0.01510237343609333\n",
            "Iteration 7183, Loss: 0.0110392514616251\n",
            "Iteration 7184, Loss: 0.014642647467553616\n",
            "Iteration 7185, Loss: 0.010576732456684113\n",
            "Iteration 7186, Loss: 0.01068098470568657\n",
            "Iteration 7187, Loss: 0.009285940788686275\n",
            "Iteration 7188, Loss: 0.009491952136158943\n",
            "Iteration 7189, Loss: 0.012447742745280266\n",
            "Iteration 7190, Loss: 0.009591679088771343\n",
            "Iteration 7191, Loss: 0.012170436792075634\n",
            "Iteration 7192, Loss: 0.009747141972184181\n",
            "Iteration 7193, Loss: 0.01181696355342865\n",
            "Iteration 7194, Loss: 0.010297577828168869\n",
            "Iteration 7195, Loss: 0.006683742627501488\n",
            "Iteration 7196, Loss: 0.014251815155148506\n",
            "Iteration 7197, Loss: 0.006821860559284687\n",
            "Iteration 7198, Loss: 0.01620004139840603\n",
            "Iteration 7199, Loss: 0.009241050109267235\n",
            "Iteration 7200, Loss: 0.00866709928959608\n",
            "Iteration 7201, Loss: 0.013552158139646053\n",
            "Iteration 7202, Loss: 0.009969999082386494\n",
            "Iteration 7203, Loss: 0.013642174191772938\n",
            "Iteration 7204, Loss: 0.010527323931455612\n",
            "Iteration 7205, Loss: 0.012649016454815865\n",
            "Iteration 7206, Loss: 0.008723918348550797\n",
            "Iteration 7207, Loss: 0.013647250831127167\n",
            "Iteration 7208, Loss: 0.012982816435396671\n",
            "Iteration 7209, Loss: 0.011952214874327183\n",
            "Iteration 7210, Loss: 0.01336002815514803\n",
            "Iteration 7211, Loss: 0.012597245164215565\n",
            "Iteration 7212, Loss: 0.014147288165986538\n",
            "Iteration 7213, Loss: 0.017984937876462936\n",
            "Iteration 7214, Loss: 0.007541934493929148\n",
            "Iteration 7215, Loss: 0.009006896987557411\n",
            "Iteration 7216, Loss: 0.011900617741048336\n",
            "Iteration 7217, Loss: 0.008415098302066326\n",
            "Iteration 7218, Loss: 0.009880095720291138\n",
            "Iteration 7219, Loss: 0.013449573889374733\n",
            "Iteration 7220, Loss: 0.006111766677349806\n",
            "Iteration 7221, Loss: 0.010988165624439716\n",
            "Iteration 7222, Loss: 0.008297319523990154\n",
            "Iteration 7223, Loss: 0.013254871591925621\n",
            "Iteration 7224, Loss: 0.013148139230906963\n",
            "Iteration 7225, Loss: 0.007837334647774696\n",
            "Iteration 7226, Loss: 0.008978421799838543\n",
            "Iteration 7227, Loss: 0.014264564961194992\n",
            "Iteration 7228, Loss: 0.009395732544362545\n",
            "Iteration 7229, Loss: 0.01239981409162283\n",
            "Iteration 7230, Loss: 0.014895967207849026\n",
            "Iteration 7231, Loss: 0.016660545021295547\n",
            "Iteration 7232, Loss: 0.01346979383379221\n",
            "Iteration 7233, Loss: 0.008976844139397144\n",
            "Iteration 7234, Loss: 0.015393859706819057\n",
            "Iteration 7235, Loss: 0.011341889388859272\n",
            "Iteration 7236, Loss: 0.012587998993694782\n",
            "Iteration 7237, Loss: 0.007460739463567734\n",
            "Iteration 7238, Loss: 0.0078349644318223\n",
            "Iteration 7239, Loss: 0.015026499517261982\n",
            "Iteration 7240, Loss: 0.012900223955512047\n",
            "Iteration 7241, Loss: 0.0139760822057724\n",
            "Iteration 7242, Loss: 0.0075519937090575695\n",
            "Iteration 7243, Loss: 0.009853665716946125\n",
            "Iteration 7244, Loss: 0.008275563828647137\n",
            "Iteration 7245, Loss: 0.014910273253917694\n",
            "Iteration 7246, Loss: 0.015977367758750916\n",
            "Iteration 7247, Loss: 0.011377496644854546\n",
            "Iteration 7248, Loss: 0.01358856912702322\n",
            "Iteration 7249, Loss: 0.00771326245740056\n",
            "Iteration 7250, Loss: 0.012318375520408154\n",
            "Iteration 7251, Loss: 0.012030706740915775\n",
            "Iteration 7252, Loss: 0.011959116905927658\n",
            "Iteration 7253, Loss: 0.010710984468460083\n",
            "Iteration 7254, Loss: 0.01327512040734291\n",
            "Iteration 7255, Loss: 0.0087997792288661\n",
            "Iteration 7256, Loss: 0.012125899083912373\n",
            "Iteration 7257, Loss: 0.013248953968286514\n",
            "Iteration 7258, Loss: 0.012457768432796001\n",
            "Iteration 7259, Loss: 0.011088724248111248\n",
            "Iteration 7260, Loss: 0.01618455909192562\n",
            "Iteration 7261, Loss: 0.009944599121809006\n",
            "Iteration 7262, Loss: 0.012810176238417625\n",
            "Iteration 7263, Loss: 0.014164443127810955\n",
            "Iteration 7264, Loss: 0.012545304372906685\n",
            "Iteration 7265, Loss: 0.013792506419122219\n",
            "Iteration 7266, Loss: 0.00648895651102066\n",
            "Iteration 7267, Loss: 0.0161906685680151\n",
            "Iteration 7268, Loss: 0.013140574097633362\n",
            "Iteration 7269, Loss: 0.01414919551461935\n",
            "Iteration 7270, Loss: 0.014229321852326393\n",
            "Iteration 7271, Loss: 0.008381099440157413\n",
            "Iteration 7272, Loss: 0.00891349371522665\n",
            "Iteration 7273, Loss: 0.009691147133708\n",
            "Iteration 7274, Loss: 0.010114524513483047\n",
            "Iteration 7275, Loss: 0.01137741282582283\n",
            "Iteration 7276, Loss: 0.012467867694795132\n",
            "Iteration 7277, Loss: 0.007248008623719215\n",
            "Iteration 7278, Loss: 0.012506376020610332\n",
            "Iteration 7279, Loss: 0.00861389935016632\n",
            "Iteration 7280, Loss: 0.006751456763595343\n",
            "Iteration 7281, Loss: 0.012966458685696125\n",
            "Iteration 7282, Loss: 0.011366261169314384\n",
            "Iteration 7283, Loss: 0.013675456866621971\n",
            "Iteration 7284, Loss: 0.0105204489082098\n",
            "Iteration 7285, Loss: 0.008807792328298092\n",
            "Iteration 7286, Loss: 0.015028608031570911\n",
            "Iteration 7287, Loss: 0.011667421087622643\n",
            "Iteration 7288, Loss: 0.0081520676612854\n",
            "Iteration 7289, Loss: 0.010843053460121155\n",
            "Iteration 7290, Loss: 0.01431707851588726\n",
            "Iteration 7291, Loss: 0.007506499998271465\n",
            "Iteration 7292, Loss: 0.005760831292718649\n",
            "Iteration 7293, Loss: 0.009049964137375355\n",
            "Iteration 7294, Loss: 0.014264247380197048\n",
            "Iteration 7295, Loss: 0.010019302368164062\n",
            "Iteration 7296, Loss: 0.010732781141996384\n",
            "Iteration 7297, Loss: 0.008988287299871445\n",
            "Iteration 7298, Loss: 0.00969067495316267\n",
            "Iteration 7299, Loss: 0.009351147338747978\n",
            "Iteration 7300, Loss: 0.008961766958236694\n",
            "Iteration 7301, Loss: 0.006879568099975586\n",
            "Iteration 7302, Loss: 0.007793920114636421\n",
            "Iteration 7303, Loss: 0.011099705472588539\n",
            "Iteration 7304, Loss: 0.011669138446450233\n",
            "Iteration 7305, Loss: 0.008328521624207497\n",
            "Iteration 7306, Loss: 0.012892194092273712\n",
            "Iteration 7307, Loss: 0.011359605938196182\n",
            "Iteration 7308, Loss: 0.01009583380073309\n",
            "Iteration 7309, Loss: 0.00943602155894041\n",
            "Iteration 7310, Loss: 0.009601715952157974\n",
            "Iteration 7311, Loss: 0.01321796327829361\n",
            "Iteration 7312, Loss: 0.008924441412091255\n",
            "Iteration 7313, Loss: 0.01157225389033556\n",
            "Iteration 7314, Loss: 0.007309056352823973\n",
            "Iteration 7315, Loss: 0.00994008406996727\n",
            "Iteration 7316, Loss: 0.011236113496124744\n",
            "Iteration 7317, Loss: 0.012681806460022926\n",
            "Iteration 7318, Loss: 0.014764810912311077\n",
            "Iteration 7319, Loss: 0.01711457408964634\n",
            "Iteration 7320, Loss: 0.015284290537238121\n",
            "Iteration 7321, Loss: 0.00849505327641964\n",
            "Iteration 7322, Loss: 0.007674509659409523\n",
            "Iteration 7323, Loss: 0.010050118900835514\n",
            "Iteration 7324, Loss: 0.014101535081863403\n",
            "Iteration 7325, Loss: 0.009084493853151798\n",
            "Iteration 7326, Loss: 0.010596085339784622\n",
            "Iteration 7327, Loss: 0.011681880801916122\n",
            "Iteration 7328, Loss: 0.01249146368354559\n",
            "Iteration 7329, Loss: 0.014816680923104286\n",
            "Iteration 7330, Loss: 0.010968958958983421\n",
            "Iteration 7331, Loss: 0.01079932413995266\n",
            "Iteration 7332, Loss: 0.009066300466656685\n",
            "Iteration 7333, Loss: 0.00973656214773655\n",
            "Iteration 7334, Loss: 0.010711279697716236\n",
            "Iteration 7335, Loss: 0.0093352310359478\n",
            "Iteration 7336, Loss: 0.014587195590138435\n",
            "Iteration 7337, Loss: 0.009694123640656471\n",
            "Iteration 7338, Loss: 0.011006588116288185\n",
            "Iteration 7339, Loss: 0.013304450549185276\n",
            "Iteration 7340, Loss: 0.01045379601418972\n",
            "Iteration 7341, Loss: 0.007986713200807571\n",
            "Iteration 7342, Loss: 0.01137625053524971\n",
            "Iteration 7343, Loss: 0.007678143680095673\n",
            "Iteration 7344, Loss: 0.010159210301935673\n",
            "Iteration 7345, Loss: 0.009912644512951374\n",
            "Iteration 7346, Loss: 0.012542319484055042\n",
            "Iteration 7347, Loss: 0.01290526706725359\n",
            "Iteration 7348, Loss: 0.010271190665662289\n",
            "Iteration 7349, Loss: 0.010521926917135715\n",
            "Iteration 7350, Loss: 0.01099281944334507\n",
            "Iteration 7351, Loss: 0.009472518227994442\n",
            "Iteration 7352, Loss: 0.012820842675864697\n",
            "Iteration 7353, Loss: 0.009965568780899048\n",
            "Iteration 7354, Loss: 0.006388508249074221\n",
            "Iteration 7355, Loss: 0.010756420902907848\n",
            "Iteration 7356, Loss: 0.012702971696853638\n",
            "Iteration 7357, Loss: 0.009337580762803555\n",
            "Iteration 7358, Loss: 0.006480683106929064\n",
            "Iteration 7359, Loss: 0.01618208736181259\n",
            "Iteration 7360, Loss: 0.014161632396280766\n",
            "Iteration 7361, Loss: 0.004856358747929335\n",
            "Iteration 7362, Loss: 0.009497739374637604\n",
            "Iteration 7363, Loss: 0.008550658822059631\n",
            "Iteration 7364, Loss: 0.010178646072745323\n",
            "Iteration 7365, Loss: 0.0090879425406456\n",
            "Iteration 7366, Loss: 0.012341957539319992\n",
            "Iteration 7367, Loss: 0.009408922865986824\n",
            "Iteration 7368, Loss: 0.011240102350711823\n",
            "Iteration 7369, Loss: 0.014035018160939217\n",
            "Iteration 7370, Loss: 0.011375660076737404\n",
            "Iteration 7371, Loss: 0.0055903661996126175\n",
            "Iteration 7372, Loss: 0.012804938480257988\n",
            "Iteration 7373, Loss: 0.008784344419836998\n",
            "Iteration 7374, Loss: 0.00841790996491909\n",
            "Iteration 7375, Loss: 0.008934104815125465\n",
            "Iteration 7376, Loss: 0.010977135971188545\n",
            "Iteration 7377, Loss: 0.015887416899204254\n",
            "Iteration 7378, Loss: 0.010029974393546581\n",
            "Iteration 7379, Loss: 0.008659695275127888\n",
            "Iteration 7380, Loss: 0.014837967231869698\n",
            "Iteration 7381, Loss: 0.012744231149554253\n",
            "Iteration 7382, Loss: 0.010577226057648659\n",
            "Iteration 7383, Loss: 0.008622562512755394\n",
            "Iteration 7384, Loss: 0.010056741535663605\n",
            "Iteration 7385, Loss: 0.012323319911956787\n",
            "Iteration 7386, Loss: 0.012951526790857315\n",
            "Iteration 7387, Loss: 0.01039732713252306\n",
            "Iteration 7388, Loss: 0.008297349326312542\n",
            "Iteration 7389, Loss: 0.009881164878606796\n",
            "Iteration 7390, Loss: 0.008700961247086525\n",
            "Iteration 7391, Loss: 0.012219375930726528\n",
            "Iteration 7392, Loss: 0.009580914862453938\n",
            "Iteration 7393, Loss: 0.009232834912836552\n",
            "Iteration 7394, Loss: 0.013962523080408573\n",
            "Iteration 7395, Loss: 0.01197313517332077\n",
            "Iteration 7396, Loss: 0.010274362750351429\n",
            "Iteration 7397, Loss: 0.010519374161958694\n",
            "Iteration 7398, Loss: 0.010534523986279964\n",
            "Iteration 7399, Loss: 0.011306559666991234\n",
            "Iteration 7400, Loss: 0.00739899231120944\n",
            "Iteration 7401, Loss: 0.010309985838830471\n",
            "Iteration 7402, Loss: 0.012539643794298172\n",
            "Iteration 7403, Loss: 0.007655562367290258\n",
            "Iteration 7404, Loss: 0.0061389487236738205\n",
            "Iteration 7405, Loss: 0.010378970764577389\n",
            "Iteration 7406, Loss: 0.011534420773386955\n",
            "Iteration 7407, Loss: 0.012492549605667591\n",
            "Iteration 7408, Loss: 0.00898764282464981\n",
            "Iteration 7409, Loss: 0.009579726494848728\n",
            "Iteration 7410, Loss: 0.010079991072416306\n",
            "Iteration 7411, Loss: 0.009836637414991856\n",
            "Iteration 7412, Loss: 0.010298541747033596\n",
            "Iteration 7413, Loss: 0.010686083696782589\n",
            "Iteration 7414, Loss: 0.01132784504443407\n",
            "Iteration 7415, Loss: 0.011304416693747044\n",
            "Iteration 7416, Loss: 0.013961110264062881\n",
            "Iteration 7417, Loss: 0.009641577489674091\n",
            "Iteration 7418, Loss: 0.009665664285421371\n",
            "Iteration 7419, Loss: 0.012727304361760616\n",
            "Iteration 7420, Loss: 0.015349635854363441\n",
            "Iteration 7421, Loss: 0.00720363250002265\n",
            "Iteration 7422, Loss: 0.014917241409420967\n",
            "Iteration 7423, Loss: 0.010424821637570858\n",
            "Iteration 7424, Loss: 0.011892897076904774\n",
            "Iteration 7425, Loss: 0.010759163647890091\n",
            "Iteration 7426, Loss: 0.010701320134103298\n",
            "Iteration 7427, Loss: 0.012796465307474136\n",
            "Iteration 7428, Loss: 0.011104211211204529\n",
            "Iteration 7429, Loss: 0.015173763036727905\n",
            "Iteration 7430, Loss: 0.012514996342360973\n",
            "Iteration 7431, Loss: 0.010158098302781582\n",
            "Iteration 7432, Loss: 0.0048547969199717045\n",
            "Iteration 7433, Loss: 0.013244976289570332\n",
            "Iteration 7434, Loss: 0.013287792913615704\n",
            "Iteration 7435, Loss: 0.013088084757328033\n",
            "Iteration 7436, Loss: 0.014423314481973648\n",
            "Iteration 7437, Loss: 0.007071337662637234\n",
            "Iteration 7438, Loss: 0.008472168818116188\n",
            "Iteration 7439, Loss: 0.013742844574153423\n",
            "Iteration 7440, Loss: 0.007584540639072657\n",
            "Iteration 7441, Loss: 0.00783482100814581\n",
            "Iteration 7442, Loss: 0.006744326092302799\n",
            "Iteration 7443, Loss: 0.011126480996608734\n",
            "Iteration 7444, Loss: 0.008734403178095818\n",
            "Iteration 7445, Loss: 0.010755496099591255\n",
            "Iteration 7446, Loss: 0.014673939906060696\n",
            "Iteration 7447, Loss: 0.010958317667245865\n",
            "Iteration 7448, Loss: 0.00813339464366436\n",
            "Iteration 7449, Loss: 0.008923587389290333\n",
            "Iteration 7450, Loss: 0.011364243924617767\n",
            "Iteration 7451, Loss: 0.010171681642532349\n",
            "Iteration 7452, Loss: 0.010864198207855225\n",
            "Iteration 7453, Loss: 0.01041534636169672\n",
            "Iteration 7454, Loss: 0.010507353581488132\n",
            "Iteration 7455, Loss: 0.007700141053646803\n",
            "Iteration 7456, Loss: 0.009641698561608791\n",
            "Iteration 7457, Loss: 0.011540492065250874\n",
            "Iteration 7458, Loss: 0.010569990612566471\n",
            "Iteration 7459, Loss: 0.012267827056348324\n",
            "Iteration 7460, Loss: 0.009883906692266464\n",
            "Iteration 7461, Loss: 0.013520902022719383\n",
            "Iteration 7462, Loss: 0.011392675340175629\n",
            "Iteration 7463, Loss: 0.008469044230878353\n",
            "Iteration 7464, Loss: 0.011693768203258514\n",
            "Iteration 7465, Loss: 0.014011384919285774\n",
            "Iteration 7466, Loss: 0.012788757681846619\n",
            "Iteration 7467, Loss: 0.011236435733735561\n",
            "Iteration 7468, Loss: 0.011435090564191341\n",
            "Iteration 7469, Loss: 0.007961499504745007\n",
            "Iteration 7470, Loss: 0.007250174880027771\n",
            "Iteration 7471, Loss: 0.006089233793318272\n",
            "Iteration 7472, Loss: 0.011418345384299755\n",
            "Iteration 7473, Loss: 0.01123443990945816\n",
            "Iteration 7474, Loss: 0.012582004070281982\n",
            "Iteration 7475, Loss: 0.011762833222746849\n",
            "Iteration 7476, Loss: 0.01146181020885706\n",
            "Iteration 7477, Loss: 0.01026440504938364\n",
            "Iteration 7478, Loss: 0.009106220677495003\n",
            "Iteration 7479, Loss: 0.01602308824658394\n",
            "Iteration 7480, Loss: 0.00882002990692854\n",
            "Iteration 7481, Loss: 0.008166762068867683\n",
            "Iteration 7482, Loss: 0.01221136748790741\n",
            "Iteration 7483, Loss: 0.01133112981915474\n",
            "Iteration 7484, Loss: 0.014136884361505508\n",
            "Iteration 7485, Loss: 0.013466840609908104\n",
            "Iteration 7486, Loss: 0.005270376335829496\n",
            "Iteration 7487, Loss: 0.011232281103730202\n",
            "Iteration 7488, Loss: 0.015892932191491127\n",
            "Iteration 7489, Loss: 0.008983532898128033\n",
            "Iteration 7490, Loss: 0.0077640893869102\n",
            "Iteration 7491, Loss: 0.010761624202132225\n",
            "Iteration 7492, Loss: 0.014287742786109447\n",
            "Iteration 7493, Loss: 0.013602325692772865\n",
            "Iteration 7494, Loss: 0.007943171076476574\n",
            "Iteration 7495, Loss: 0.0061776721850037575\n",
            "Iteration 7496, Loss: 0.00914780329912901\n",
            "Iteration 7497, Loss: 0.011956007219851017\n",
            "Iteration 7498, Loss: 0.01065520104020834\n",
            "Iteration 7499, Loss: 0.009136224165558815\n",
            "Iteration 7500, Loss: 0.010813264176249504\n",
            "Iteration 7501, Loss: 0.00847531110048294\n",
            "Iteration 7502, Loss: 0.005625802557915449\n",
            "Iteration 7503, Loss: 0.008246613666415215\n",
            "Iteration 7504, Loss: 0.0058554294519126415\n",
            "Iteration 7505, Loss: 0.01293655950576067\n",
            "Iteration 7506, Loss: 0.01414375752210617\n",
            "Iteration 7507, Loss: 0.006905782967805862\n",
            "Iteration 7508, Loss: 0.012554312124848366\n",
            "Iteration 7509, Loss: 0.012983967550098896\n",
            "Iteration 7510, Loss: 0.005887550767511129\n",
            "Iteration 7511, Loss: 0.005322236102074385\n",
            "Iteration 7512, Loss: 0.013589462265372276\n",
            "Iteration 7513, Loss: 0.0070349667221307755\n",
            "Iteration 7514, Loss: 0.01373019814491272\n",
            "Iteration 7515, Loss: 0.01077048759907484\n",
            "Iteration 7516, Loss: 0.013791523873806\n",
            "Iteration 7517, Loss: 0.013886477798223495\n",
            "Iteration 7518, Loss: 0.011218487285077572\n",
            "Iteration 7519, Loss: 0.00942703802138567\n",
            "Iteration 7520, Loss: 0.014583935961127281\n",
            "Iteration 7521, Loss: 0.011948183178901672\n",
            "Iteration 7522, Loss: 0.00646065641194582\n",
            "Iteration 7523, Loss: 0.009774534031748772\n",
            "Iteration 7524, Loss: 0.013898865319788456\n",
            "Iteration 7525, Loss: 0.01952901855111122\n",
            "Iteration 7526, Loss: 0.00936516560614109\n",
            "Iteration 7527, Loss: 0.011984935961663723\n",
            "Iteration 7528, Loss: 0.00986486580222845\n",
            "Iteration 7529, Loss: 0.008344707079231739\n",
            "Iteration 7530, Loss: 0.00963529758155346\n",
            "Iteration 7531, Loss: 0.0076683699153363705\n",
            "Iteration 7532, Loss: 0.01516900584101677\n",
            "Iteration 7533, Loss: 0.011689070612192154\n",
            "Iteration 7534, Loss: 0.015854427590966225\n",
            "Iteration 7535, Loss: 0.004686560016125441\n",
            "Iteration 7536, Loss: 0.012466673739254475\n",
            "Iteration 7537, Loss: 0.013421026058495045\n",
            "Iteration 7538, Loss: 0.011592293158173561\n",
            "Iteration 7539, Loss: 0.008754492737352848\n",
            "Iteration 7540, Loss: 0.012208051048219204\n",
            "Iteration 7541, Loss: 0.007580310106277466\n",
            "Iteration 7542, Loss: 0.014283614233136177\n",
            "Iteration 7543, Loss: 0.015265855938196182\n",
            "Iteration 7544, Loss: 0.010468115098774433\n",
            "Iteration 7545, Loss: 0.0057606142945587635\n",
            "Iteration 7546, Loss: 0.01075994037091732\n",
            "Iteration 7547, Loss: 0.008291955105960369\n",
            "Iteration 7548, Loss: 0.01469229906797409\n",
            "Iteration 7549, Loss: 0.004275890998542309\n",
            "Iteration 7550, Loss: 0.004892654716968536\n",
            "Iteration 7551, Loss: 0.005769285839051008\n",
            "Iteration 7552, Loss: 0.006889046169817448\n",
            "Iteration 7553, Loss: 0.011760322377085686\n",
            "Iteration 7554, Loss: 0.014421522617340088\n",
            "Iteration 7555, Loss: 0.0036398645024746656\n",
            "Iteration 7556, Loss: 0.013288450427353382\n",
            "Iteration 7557, Loss: 0.01642702706158161\n",
            "Iteration 7558, Loss: 0.012029724195599556\n",
            "Iteration 7559, Loss: 0.01113160327076912\n",
            "Iteration 7560, Loss: 0.009304237551987171\n",
            "Iteration 7561, Loss: 0.014390446245670319\n",
            "Iteration 7562, Loss: 0.011809203773736954\n",
            "Iteration 7563, Loss: 0.012100224383175373\n",
            "Iteration 7564, Loss: 0.01019190065562725\n",
            "Iteration 7565, Loss: 0.011434352956712246\n",
            "Iteration 7566, Loss: 0.009248440153896809\n",
            "Iteration 7567, Loss: 0.012308581732213497\n",
            "Iteration 7568, Loss: 0.007695467211306095\n",
            "Iteration 7569, Loss: 0.015054081566631794\n",
            "Iteration 7570, Loss: 0.007497619837522507\n",
            "Iteration 7571, Loss: 0.01633586175739765\n",
            "Iteration 7572, Loss: 0.00869203545153141\n",
            "Iteration 7573, Loss: 0.01091292779892683\n",
            "Iteration 7574, Loss: 0.007153518497943878\n",
            "Iteration 7575, Loss: 0.009236237034201622\n",
            "Iteration 7576, Loss: 0.011094114743173122\n",
            "Iteration 7577, Loss: 0.010592347010970116\n",
            "Iteration 7578, Loss: 0.0078862514346838\n",
            "Iteration 7579, Loss: 0.010118355974555016\n",
            "Iteration 7580, Loss: 0.012567240744829178\n",
            "Iteration 7581, Loss: 0.011253166012465954\n",
            "Iteration 7582, Loss: 0.007543063722550869\n",
            "Iteration 7583, Loss: 0.011645639315247536\n",
            "Iteration 7584, Loss: 0.012859043665230274\n",
            "Iteration 7585, Loss: 0.015200147405266762\n",
            "Iteration 7586, Loss: 0.011325148865580559\n",
            "Iteration 7587, Loss: 0.00745404651388526\n",
            "Iteration 7588, Loss: 0.008961373008787632\n",
            "Iteration 7589, Loss: 0.013774726539850235\n",
            "Iteration 7590, Loss: 0.009359128773212433\n",
            "Iteration 7591, Loss: 0.01022182498127222\n",
            "Iteration 7592, Loss: 0.015338682569563389\n",
            "Iteration 7593, Loss: 0.009827629663050175\n",
            "Iteration 7594, Loss: 0.0100302929058671\n",
            "Iteration 7595, Loss: 0.008626718074083328\n",
            "Iteration 7596, Loss: 0.010715116746723652\n",
            "Iteration 7597, Loss: 0.010689688846468925\n",
            "Iteration 7598, Loss: 0.010451283305883408\n",
            "Iteration 7599, Loss: 0.01014750450849533\n",
            "Iteration 7600, Loss: 0.008775925263762474\n",
            "Iteration 7601, Loss: 0.017766952514648438\n",
            "Iteration 7602, Loss: 0.010343119502067566\n",
            "Iteration 7603, Loss: 0.009715587832033634\n",
            "Iteration 7604, Loss: 0.012486973777413368\n",
            "Iteration 7605, Loss: 0.008433173410594463\n",
            "Iteration 7606, Loss: 0.01236534584313631\n",
            "Iteration 7607, Loss: 0.00979637075215578\n",
            "Iteration 7608, Loss: 0.012367551214993\n",
            "Iteration 7609, Loss: 0.011165396310389042\n",
            "Iteration 7610, Loss: 0.008084344677627087\n",
            "Iteration 7611, Loss: 0.009977323934435844\n",
            "Iteration 7612, Loss: 0.007927230559289455\n",
            "Iteration 7613, Loss: 0.008707833476364613\n",
            "Iteration 7614, Loss: 0.009160656481981277\n",
            "Iteration 7615, Loss: 0.012183453887701035\n",
            "Iteration 7616, Loss: 0.013951399363577366\n",
            "Iteration 7617, Loss: 0.010930754244327545\n",
            "Iteration 7618, Loss: 0.014346513897180557\n",
            "Iteration 7619, Loss: 0.014760162681341171\n",
            "Iteration 7620, Loss: 0.008176906034350395\n",
            "Iteration 7621, Loss: 0.01641079969704151\n",
            "Iteration 7622, Loss: 0.007395880296826363\n",
            "Iteration 7623, Loss: 0.013093309476971626\n",
            "Iteration 7624, Loss: 0.009927683509886265\n",
            "Iteration 7625, Loss: 0.0072909630835056305\n",
            "Iteration 7626, Loss: 0.013811498880386353\n",
            "Iteration 7627, Loss: 0.0095409220084548\n",
            "Iteration 7628, Loss: 0.005439931061118841\n",
            "Iteration 7629, Loss: 0.009757237508893013\n",
            "Iteration 7630, Loss: 0.010288679972290993\n",
            "Iteration 7631, Loss: 0.009678180329501629\n",
            "Iteration 7632, Loss: 0.010073821060359478\n",
            "Iteration 7633, Loss: 0.012057356536388397\n",
            "Iteration 7634, Loss: 0.0062202587723731995\n",
            "Iteration 7635, Loss: 0.007686000782996416\n",
            "Iteration 7636, Loss: 0.011509370058774948\n",
            "Iteration 7637, Loss: 0.009387453086674213\n",
            "Iteration 7638, Loss: 0.008471881039440632\n",
            "Iteration 7639, Loss: 0.008449897170066833\n",
            "Iteration 7640, Loss: 0.011066953651607037\n",
            "Iteration 7641, Loss: 0.010103232227265835\n",
            "Iteration 7642, Loss: 0.014386085793375969\n",
            "Iteration 7643, Loss: 0.00933460146188736\n",
            "Iteration 7644, Loss: 0.011329236440360546\n",
            "Iteration 7645, Loss: 0.009683818556368351\n",
            "Iteration 7646, Loss: 0.007781679276376963\n",
            "Iteration 7647, Loss: 0.010680670849978924\n",
            "Iteration 7648, Loss: 0.010120798833668232\n",
            "Iteration 7649, Loss: 0.005886184051632881\n",
            "Iteration 7650, Loss: 0.01321023516356945\n",
            "Iteration 7651, Loss: 0.012807615101337433\n",
            "Iteration 7652, Loss: 0.011407105252146721\n",
            "Iteration 7653, Loss: 0.009416910819709301\n",
            "Iteration 7654, Loss: 0.010627557523548603\n",
            "Iteration 7655, Loss: 0.006519782822579145\n",
            "Iteration 7656, Loss: 0.01303983386605978\n",
            "Iteration 7657, Loss: 0.01516569871455431\n",
            "Iteration 7658, Loss: 0.012410915456712246\n",
            "Iteration 7659, Loss: 0.009704411961138248\n",
            "Iteration 7660, Loss: 0.012162319384515285\n",
            "Iteration 7661, Loss: 0.012823857367038727\n",
            "Iteration 7662, Loss: 0.010732416994869709\n",
            "Iteration 7663, Loss: 0.006646640598773956\n",
            "Iteration 7664, Loss: 0.009446952491998672\n",
            "Iteration 7665, Loss: 0.00832226499915123\n",
            "Iteration 7666, Loss: 0.007978168316185474\n",
            "Iteration 7667, Loss: 0.006902026943862438\n",
            "Iteration 7668, Loss: 0.013508236035704613\n",
            "Iteration 7669, Loss: 0.006042215041816235\n",
            "Iteration 7670, Loss: 0.007934763096272945\n",
            "Iteration 7671, Loss: 0.012476770207285881\n",
            "Iteration 7672, Loss: 0.011109309270977974\n",
            "Iteration 7673, Loss: 0.010991610586643219\n",
            "Iteration 7674, Loss: 0.00785873644053936\n",
            "Iteration 7675, Loss: 0.009084319695830345\n",
            "Iteration 7676, Loss: 0.013152578845620155\n",
            "Iteration 7677, Loss: 0.007445934694260359\n",
            "Iteration 7678, Loss: 0.01426069438457489\n",
            "Iteration 7679, Loss: 0.01128470990806818\n",
            "Iteration 7680, Loss: 0.01071422453969717\n",
            "Iteration 7681, Loss: 0.009624125435948372\n",
            "Iteration 7682, Loss: 0.015380545519292355\n",
            "Iteration 7683, Loss: 0.01181881409138441\n",
            "Iteration 7684, Loss: 0.010329484939575195\n",
            "Iteration 7685, Loss: 0.008908609859645367\n",
            "Iteration 7686, Loss: 0.0057973978109657764\n",
            "Iteration 7687, Loss: 0.012480340898036957\n",
            "Iteration 7688, Loss: 0.009171535260975361\n",
            "Iteration 7689, Loss: 0.013583773747086525\n",
            "Iteration 7690, Loss: 0.008623103611171246\n",
            "Iteration 7691, Loss: 0.01267219614237547\n",
            "Iteration 7692, Loss: 0.010620109736919403\n",
            "Iteration 7693, Loss: 0.012011072598397732\n",
            "Iteration 7694, Loss: 0.008681457489728928\n",
            "Iteration 7695, Loss: 0.017137473449110985\n",
            "Iteration 7696, Loss: 0.010531654581427574\n",
            "Iteration 7697, Loss: 0.010631236247718334\n",
            "Iteration 7698, Loss: 0.011094494722783566\n",
            "Iteration 7699, Loss: 0.01037922129034996\n",
            "Iteration 7700, Loss: 0.008953574113547802\n",
            "Iteration 7701, Loss: 0.01044777687638998\n",
            "Iteration 7702, Loss: 0.01313094887882471\n",
            "Iteration 7703, Loss: 0.01180819608271122\n",
            "Iteration 7704, Loss: 0.01223053690046072\n",
            "Iteration 7705, Loss: 0.01049141027033329\n",
            "Iteration 7706, Loss: 0.014161067083477974\n",
            "Iteration 7707, Loss: 0.008625257760286331\n",
            "Iteration 7708, Loss: 0.008338729850947857\n",
            "Iteration 7709, Loss: 0.0072816601023077965\n",
            "Iteration 7710, Loss: 0.007904420606791973\n",
            "Iteration 7711, Loss: 0.014777574688196182\n",
            "Iteration 7712, Loss: 0.009682798758149147\n",
            "Iteration 7713, Loss: 0.008594942279160023\n",
            "Iteration 7714, Loss: 0.012034774757921696\n",
            "Iteration 7715, Loss: 0.010743490420281887\n",
            "Iteration 7716, Loss: 0.011697341687977314\n",
            "Iteration 7717, Loss: 0.011432411149144173\n",
            "Iteration 7718, Loss: 0.012232080101966858\n",
            "Iteration 7719, Loss: 0.014230561442673206\n",
            "Iteration 7720, Loss: 0.005965265911072493\n",
            "Iteration 7721, Loss: 0.009619731456041336\n",
            "Iteration 7722, Loss: 0.017817670479416847\n",
            "Iteration 7723, Loss: 0.011473346501588821\n",
            "Iteration 7724, Loss: 0.007595371454954147\n",
            "Iteration 7725, Loss: 0.013087126426398754\n",
            "Iteration 7726, Loss: 0.012573990039527416\n",
            "Iteration 7727, Loss: 0.01053044106811285\n",
            "Iteration 7728, Loss: 0.012322565540671349\n",
            "Iteration 7729, Loss: 0.01030255388468504\n",
            "Iteration 7730, Loss: 0.010133120231330395\n",
            "Iteration 7731, Loss: 0.010981611907482147\n",
            "Iteration 7732, Loss: 0.006821434013545513\n",
            "Iteration 7733, Loss: 0.011531415395438671\n",
            "Iteration 7734, Loss: 0.0076990132220089436\n",
            "Iteration 7735, Loss: 0.009090489707887173\n",
            "Iteration 7736, Loss: 0.00923872459679842\n",
            "Iteration 7737, Loss: 0.009931675158441067\n",
            "Iteration 7738, Loss: 0.009536519646644592\n",
            "Iteration 7739, Loss: 0.011367003433406353\n",
            "Iteration 7740, Loss: 0.011568518355488777\n",
            "Iteration 7741, Loss: 0.007137314882129431\n",
            "Iteration 7742, Loss: 0.007065857294946909\n",
            "Iteration 7743, Loss: 0.01024495717138052\n",
            "Iteration 7744, Loss: 0.01337524689733982\n",
            "Iteration 7745, Loss: 0.013090919703245163\n",
            "Iteration 7746, Loss: 0.0072015803307294846\n",
            "Iteration 7747, Loss: 0.009725061245262623\n",
            "Iteration 7748, Loss: 0.010153778828680515\n",
            "Iteration 7749, Loss: 0.013689463026821613\n",
            "Iteration 7750, Loss: 0.012157212011516094\n",
            "Iteration 7751, Loss: 0.011127042584121227\n",
            "Iteration 7752, Loss: 0.009371256455779076\n",
            "Iteration 7753, Loss: 0.011939655989408493\n",
            "Iteration 7754, Loss: 0.010658313520252705\n",
            "Iteration 7755, Loss: 0.004718118347227573\n",
            "Iteration 7756, Loss: 0.010840749368071556\n",
            "Iteration 7757, Loss: 0.010599386878311634\n",
            "Iteration 7758, Loss: 0.008006766438484192\n",
            "Iteration 7759, Loss: 0.01230661105364561\n",
            "Iteration 7760, Loss: 0.00909596960991621\n",
            "Iteration 7761, Loss: 0.012692365795373917\n",
            "Iteration 7762, Loss: 0.007254197262227535\n",
            "Iteration 7763, Loss: 0.008076652884483337\n",
            "Iteration 7764, Loss: 0.010912359692156315\n",
            "Iteration 7765, Loss: 0.013638596050441265\n",
            "Iteration 7766, Loss: 0.010573198087513447\n",
            "Iteration 7767, Loss: 0.0047326115891337395\n",
            "Iteration 7768, Loss: 0.010673537850379944\n",
            "Iteration 7769, Loss: 0.00975340511649847\n",
            "Iteration 7770, Loss: 0.01091998815536499\n",
            "Iteration 7771, Loss: 0.00800412055104971\n",
            "Iteration 7772, Loss: 0.010085707530379295\n",
            "Iteration 7773, Loss: 0.008000688627362251\n",
            "Iteration 7774, Loss: 0.01591968722641468\n",
            "Iteration 7775, Loss: 0.011341112665832043\n",
            "Iteration 7776, Loss: 0.007600567303597927\n",
            "Iteration 7777, Loss: 0.00962307583540678\n",
            "Iteration 7778, Loss: 0.007834473624825478\n",
            "Iteration 7779, Loss: 0.013025619089603424\n",
            "Iteration 7780, Loss: 0.01205321867018938\n",
            "Iteration 7781, Loss: 0.01212778128683567\n",
            "Iteration 7782, Loss: 0.015253180637955666\n",
            "Iteration 7783, Loss: 0.009444504044950008\n",
            "Iteration 7784, Loss: 0.01596311293542385\n",
            "Iteration 7785, Loss: 0.014892377890646458\n",
            "Iteration 7786, Loss: 0.00908934511244297\n",
            "Iteration 7787, Loss: 0.013956732116639614\n",
            "Iteration 7788, Loss: 0.009149037301540375\n",
            "Iteration 7789, Loss: 0.014436514116823673\n",
            "Iteration 7790, Loss: 0.012309812009334564\n",
            "Iteration 7791, Loss: 0.008845983073115349\n",
            "Iteration 7792, Loss: 0.01481238380074501\n",
            "Iteration 7793, Loss: 0.008866159245371819\n",
            "Iteration 7794, Loss: 0.010931619442999363\n",
            "Iteration 7795, Loss: 0.009328542277216911\n",
            "Iteration 7796, Loss: 0.01678464561700821\n",
            "Iteration 7797, Loss: 0.009812751784920692\n",
            "Iteration 7798, Loss: 0.012234638445079327\n",
            "Iteration 7799, Loss: 0.01721491478383541\n",
            "Iteration 7800, Loss: 0.008117315359413624\n",
            "Iteration 7801, Loss: 0.006962762214243412\n",
            "Iteration 7802, Loss: 0.01527076493948698\n",
            "Iteration 7803, Loss: 0.0072948262095451355\n",
            "Iteration 7804, Loss: 0.008153333328664303\n",
            "Iteration 7805, Loss: 0.013327321037650108\n",
            "Iteration 7806, Loss: 0.013950411230325699\n",
            "Iteration 7807, Loss: 0.008405948057770729\n",
            "Iteration 7808, Loss: 0.006027464754879475\n",
            "Iteration 7809, Loss: 0.01368411909788847\n",
            "Iteration 7810, Loss: 0.007340376265347004\n",
            "Iteration 7811, Loss: 0.010653920471668243\n",
            "Iteration 7812, Loss: 0.01227029599249363\n",
            "Iteration 7813, Loss: 0.011897832155227661\n",
            "Iteration 7814, Loss: 0.007988227531313896\n",
            "Iteration 7815, Loss: 0.00864326674491167\n",
            "Iteration 7816, Loss: 0.009123225696384907\n",
            "Iteration 7817, Loss: 0.011683795601129532\n",
            "Iteration 7818, Loss: 0.01151536125689745\n",
            "Iteration 7819, Loss: 0.009274892508983612\n",
            "Iteration 7820, Loss: 0.009822052903473377\n",
            "Iteration 7821, Loss: 0.009194961749017239\n",
            "Iteration 7822, Loss: 0.013160621747374535\n",
            "Iteration 7823, Loss: 0.010039540007710457\n",
            "Iteration 7824, Loss: 0.017352445051074028\n",
            "Iteration 7825, Loss: 0.010422258637845516\n",
            "Iteration 7826, Loss: 0.011412431485950947\n",
            "Iteration 7827, Loss: 0.011218863539397717\n",
            "Iteration 7828, Loss: 0.00828217901289463\n",
            "Iteration 7829, Loss: 0.010427722707390785\n",
            "Iteration 7830, Loss: 0.014198307879269123\n",
            "Iteration 7831, Loss: 0.006473955232650042\n",
            "Iteration 7832, Loss: 0.013977202586829662\n",
            "Iteration 7833, Loss: 0.008414137177169323\n",
            "Iteration 7834, Loss: 0.009541702456772327\n",
            "Iteration 7835, Loss: 0.007078939117491245\n",
            "Iteration 7836, Loss: 0.006232930347323418\n",
            "Iteration 7837, Loss: 0.011294625699520111\n",
            "Iteration 7838, Loss: 0.008171088062226772\n",
            "Iteration 7839, Loss: 0.006427323911339045\n",
            "Iteration 7840, Loss: 0.014950323849916458\n",
            "Iteration 7841, Loss: 0.009675441309809685\n",
            "Iteration 7842, Loss: 0.010228521190583706\n",
            "Iteration 7843, Loss: 0.005122920032590628\n",
            "Iteration 7844, Loss: 0.00872370321303606\n",
            "Iteration 7845, Loss: 0.00790614727884531\n",
            "Iteration 7846, Loss: 0.009325578808784485\n",
            "Iteration 7847, Loss: 0.006322037894278765\n",
            "Iteration 7848, Loss: 0.013178492896258831\n",
            "Iteration 7849, Loss: 0.010541606694459915\n",
            "Iteration 7850, Loss: 0.008310262113809586\n",
            "Iteration 7851, Loss: 0.016425803303718567\n",
            "Iteration 7852, Loss: 0.010204709134995937\n",
            "Iteration 7853, Loss: 0.008985518477857113\n",
            "Iteration 7854, Loss: 0.013184771873056889\n",
            "Iteration 7855, Loss: 0.00865633599460125\n",
            "Iteration 7856, Loss: 0.005613802466541529\n",
            "Iteration 7857, Loss: 0.014991050586104393\n",
            "Iteration 7858, Loss: 0.012368123978376389\n",
            "Iteration 7859, Loss: 0.01174764521420002\n",
            "Iteration 7860, Loss: 0.008958373218774796\n",
            "Iteration 7861, Loss: 0.006906936876475811\n",
            "Iteration 7862, Loss: 0.00908263586461544\n",
            "Iteration 7863, Loss: 0.010068893432617188\n",
            "Iteration 7864, Loss: 0.009410270489752293\n",
            "Iteration 7865, Loss: 0.010773963294923306\n",
            "Iteration 7866, Loss: 0.00782044231891632\n",
            "Iteration 7867, Loss: 0.009287850931286812\n",
            "Iteration 7868, Loss: 0.017500273883342743\n",
            "Iteration 7869, Loss: 0.008531286381185055\n",
            "Iteration 7870, Loss: 0.008365052752196789\n",
            "Iteration 7871, Loss: 0.010610172525048256\n",
            "Iteration 7872, Loss: 0.00893203541636467\n",
            "Iteration 7873, Loss: 0.006461739540100098\n",
            "Iteration 7874, Loss: 0.010354447178542614\n",
            "Iteration 7875, Loss: 0.013666599057614803\n",
            "Iteration 7876, Loss: 0.011418181471526623\n",
            "Iteration 7877, Loss: 0.014980013482272625\n",
            "Iteration 7878, Loss: 0.010959611274302006\n",
            "Iteration 7879, Loss: 0.008874908089637756\n",
            "Iteration 7880, Loss: 0.015106025151908398\n",
            "Iteration 7881, Loss: 0.011436808854341507\n",
            "Iteration 7882, Loss: 0.00857975147664547\n",
            "Iteration 7883, Loss: 0.00937463529407978\n",
            "Iteration 7884, Loss: 0.010360071435570717\n",
            "Iteration 7885, Loss: 0.011186904273927212\n",
            "Iteration 7886, Loss: 0.012135636061429977\n",
            "Iteration 7887, Loss: 0.011609895154833794\n",
            "Iteration 7888, Loss: 0.013015043921768665\n",
            "Iteration 7889, Loss: 0.015989555045962334\n",
            "Iteration 7890, Loss: 0.009961048141121864\n",
            "Iteration 7891, Loss: 0.014020195230841637\n",
            "Iteration 7892, Loss: 0.013134372420608997\n",
            "Iteration 7893, Loss: 0.012167101725935936\n",
            "Iteration 7894, Loss: 0.010209104046225548\n",
            "Iteration 7895, Loss: 0.010408296249806881\n",
            "Iteration 7896, Loss: 0.009504646062850952\n",
            "Iteration 7897, Loss: 0.011818396858870983\n",
            "Iteration 7898, Loss: 0.010385459288954735\n",
            "Iteration 7899, Loss: 0.013230746611952782\n",
            "Iteration 7900, Loss: 0.014951689168810844\n",
            "Iteration 7901, Loss: 0.010700060985982418\n",
            "Iteration 7902, Loss: 0.007475655991584063\n",
            "Iteration 7903, Loss: 0.008385737426578999\n",
            "Iteration 7904, Loss: 0.007585580926388502\n",
            "Iteration 7905, Loss: 0.012419948354363441\n",
            "Iteration 7906, Loss: 0.01128807570785284\n",
            "Iteration 7907, Loss: 0.01191435195505619\n",
            "Iteration 7908, Loss: 0.014202270656824112\n",
            "Iteration 7909, Loss: 0.008790256455540657\n",
            "Iteration 7910, Loss: 0.014041551388800144\n",
            "Iteration 7911, Loss: 0.007611389271914959\n",
            "Iteration 7912, Loss: 0.005604980979114771\n",
            "Iteration 7913, Loss: 0.01732105202972889\n",
            "Iteration 7914, Loss: 0.013416635803878307\n",
            "Iteration 7915, Loss: 0.010997782461345196\n",
            "Iteration 7916, Loss: 0.00887653324753046\n",
            "Iteration 7917, Loss: 0.006345434579998255\n",
            "Iteration 7918, Loss: 0.01191607117652893\n",
            "Iteration 7919, Loss: 0.01218369510024786\n",
            "Iteration 7920, Loss: 0.010778478346765041\n",
            "Iteration 7921, Loss: 0.009709466248750687\n",
            "Iteration 7922, Loss: 0.010599490255117416\n",
            "Iteration 7923, Loss: 0.005642997566610575\n",
            "Iteration 7924, Loss: 0.01373352576047182\n",
            "Iteration 7925, Loss: 0.015205303207039833\n",
            "Iteration 7926, Loss: 0.011582305654883385\n",
            "Iteration 7927, Loss: 0.009948774240911007\n",
            "Iteration 7928, Loss: 0.01326788030564785\n",
            "Iteration 7929, Loss: 0.008413405157625675\n",
            "Iteration 7930, Loss: 0.014313926920294762\n",
            "Iteration 7931, Loss: 0.006061943713575602\n",
            "Iteration 7932, Loss: 0.011058558709919453\n",
            "Iteration 7933, Loss: 0.006531076971441507\n",
            "Iteration 7934, Loss: 0.01152890920639038\n",
            "Iteration 7935, Loss: 0.01605887897312641\n",
            "Iteration 7936, Loss: 0.014371579512953758\n",
            "Iteration 7937, Loss: 0.009416880086064339\n",
            "Iteration 7938, Loss: 0.013134036213159561\n",
            "Iteration 7939, Loss: 0.012294597923755646\n",
            "Iteration 7940, Loss: 0.009365242905914783\n",
            "Iteration 7941, Loss: 0.010238165967166424\n",
            "Iteration 7942, Loss: 0.009123499505221844\n",
            "Iteration 7943, Loss: 0.013024967163801193\n",
            "Iteration 7944, Loss: 0.013364770449697971\n",
            "Iteration 7945, Loss: 0.011116353794932365\n",
            "Iteration 7946, Loss: 0.009854774922132492\n",
            "Iteration 7947, Loss: 0.00687967287376523\n",
            "Iteration 7948, Loss: 0.012130144983530045\n",
            "Iteration 7949, Loss: 0.009475335478782654\n",
            "Iteration 7950, Loss: 0.013744844123721123\n",
            "Iteration 7951, Loss: 0.013781539164483547\n",
            "Iteration 7952, Loss: 0.014051615260541439\n",
            "Iteration 7953, Loss: 0.011513518169522285\n",
            "Iteration 7954, Loss: 0.012757688760757446\n",
            "Iteration 7955, Loss: 0.009766852483153343\n",
            "Iteration 7956, Loss: 0.011987688019871712\n",
            "Iteration 7957, Loss: 0.014654893428087234\n",
            "Iteration 7958, Loss: 0.0112235676497221\n",
            "Iteration 7959, Loss: 0.012271428480744362\n",
            "Iteration 7960, Loss: 0.007933170534670353\n",
            "Iteration 7961, Loss: 0.011211155913770199\n",
            "Iteration 7962, Loss: 0.011196330189704895\n",
            "Iteration 7963, Loss: 0.012259973213076591\n",
            "Iteration 7964, Loss: 0.008015741594135761\n",
            "Iteration 7965, Loss: 0.008764795027673244\n",
            "Iteration 7966, Loss: 0.007636908441781998\n",
            "Iteration 7967, Loss: 0.01034572720527649\n",
            "Iteration 7968, Loss: 0.0070786625146865845\n",
            "Iteration 7969, Loss: 0.007036855444312096\n",
            "Iteration 7970, Loss: 0.014307978563010693\n",
            "Iteration 7971, Loss: 0.012041560374200344\n",
            "Iteration 7972, Loss: 0.010240921750664711\n",
            "Iteration 7973, Loss: 0.011612099595367908\n",
            "Iteration 7974, Loss: 0.007043864112347364\n",
            "Iteration 7975, Loss: 0.010011788457632065\n",
            "Iteration 7976, Loss: 0.012168282642960548\n",
            "Iteration 7977, Loss: 0.012419318780303001\n",
            "Iteration 7978, Loss: 0.009815685451030731\n",
            "Iteration 7979, Loss: 0.005978671368211508\n",
            "Iteration 7980, Loss: 0.006660004146397114\n",
            "Iteration 7981, Loss: 0.010324707254767418\n",
            "Iteration 7982, Loss: 0.00518666161224246\n",
            "Iteration 7983, Loss: 0.01200028508901596\n",
            "Iteration 7984, Loss: 0.00995449349284172\n",
            "Iteration 7985, Loss: 0.007350720930844545\n",
            "Iteration 7986, Loss: 0.01103907823562622\n",
            "Iteration 7987, Loss: 0.010127688758075237\n",
            "Iteration 7988, Loss: 0.013727078214287758\n",
            "Iteration 7989, Loss: 0.011980763636529446\n",
            "Iteration 7990, Loss: 0.021745232865214348\n",
            "Iteration 7991, Loss: 0.01143274549394846\n",
            "Iteration 7992, Loss: 0.009775818325579166\n",
            "Iteration 7993, Loss: 0.012598281726241112\n",
            "Iteration 7994, Loss: 0.009911946952342987\n",
            "Iteration 7995, Loss: 0.009440572932362556\n",
            "Iteration 7996, Loss: 0.01141844131052494\n",
            "Iteration 7997, Loss: 0.011262771673500538\n",
            "Iteration 7998, Loss: 0.009433208033442497\n",
            "Iteration 7999, Loss: 0.011262196116149426\n",
            "Iteration 8000, Loss: 0.006523626856505871\n",
            "Test Loss: 0.031654857099056244\n",
            "Iteration 8001, Loss: 0.0060952831991016865\n",
            "Iteration 8002, Loss: 0.01067569199949503\n",
            "Iteration 8003, Loss: 0.008811458013951778\n",
            "Iteration 8004, Loss: 0.009620729833841324\n",
            "Iteration 8005, Loss: 0.008035830222070217\n",
            "Iteration 8006, Loss: 0.00871189683675766\n",
            "Iteration 8007, Loss: 0.008619440719485283\n",
            "Iteration 8008, Loss: 0.009509969502687454\n",
            "Iteration 8009, Loss: 0.010265085846185684\n",
            "Iteration 8010, Loss: 0.010360222309827805\n",
            "Iteration 8011, Loss: 0.00838934164494276\n",
            "Iteration 8012, Loss: 0.009075271897017956\n",
            "Iteration 8013, Loss: 0.013468168675899506\n",
            "Iteration 8014, Loss: 0.0072327591478824615\n",
            "Iteration 8015, Loss: 0.008663412183523178\n",
            "Iteration 8016, Loss: 0.005868482403457165\n",
            "Iteration 8017, Loss: 0.008783110417425632\n",
            "Iteration 8018, Loss: 0.008106599561870098\n",
            "Iteration 8019, Loss: 0.010730654001235962\n",
            "Iteration 8020, Loss: 0.012425079010426998\n",
            "Iteration 8021, Loss: 0.007980920374393463\n",
            "Iteration 8022, Loss: 0.01194762159138918\n",
            "Iteration 8023, Loss: 0.007776602637022734\n",
            "Iteration 8024, Loss: 0.007833024486899376\n",
            "Iteration 8025, Loss: 0.01192814763635397\n",
            "Iteration 8026, Loss: 0.008779497817158699\n",
            "Iteration 8027, Loss: 0.012422651052474976\n",
            "Iteration 8028, Loss: 0.008232343010604382\n",
            "Iteration 8029, Loss: 0.00926744844764471\n",
            "Iteration 8030, Loss: 0.010337406769394875\n",
            "Iteration 8031, Loss: 0.007919037714600563\n",
            "Iteration 8032, Loss: 0.012968866154551506\n",
            "Iteration 8033, Loss: 0.009209816344082355\n",
            "Iteration 8034, Loss: 0.007074280641973019\n",
            "Iteration 8035, Loss: 0.012439042329788208\n",
            "Iteration 8036, Loss: 0.009282202459871769\n",
            "Iteration 8037, Loss: 0.0065780337899923325\n",
            "Iteration 8038, Loss: 0.009033339098095894\n",
            "Iteration 8039, Loss: 0.013358984142541885\n",
            "Iteration 8040, Loss: 0.011694185435771942\n",
            "Iteration 8041, Loss: 0.014080003835260868\n",
            "Iteration 8042, Loss: 0.008818395435810089\n",
            "Iteration 8043, Loss: 0.012148212641477585\n",
            "Iteration 8044, Loss: 0.011892111040651798\n",
            "Iteration 8045, Loss: 0.015188433229923248\n",
            "Iteration 8046, Loss: 0.011728741228580475\n",
            "Iteration 8047, Loss: 0.006483934354037046\n",
            "Iteration 8048, Loss: 0.011486738920211792\n",
            "Iteration 8049, Loss: 0.006743863224983215\n",
            "Iteration 8050, Loss: 0.010647943243384361\n",
            "Iteration 8051, Loss: 0.006142071448266506\n",
            "Iteration 8052, Loss: 0.011523601599037647\n",
            "Iteration 8053, Loss: 0.00516606867313385\n",
            "Iteration 8054, Loss: 0.0073425909504294395\n",
            "Iteration 8055, Loss: 0.011269071139395237\n",
            "Iteration 8056, Loss: 0.007857363671064377\n",
            "Iteration 8057, Loss: 0.014677883125841618\n",
            "Iteration 8058, Loss: 0.012870210222899914\n",
            "Iteration 8059, Loss: 0.01810355670750141\n",
            "Iteration 8060, Loss: 0.012491864152252674\n",
            "Iteration 8061, Loss: 0.011994749307632446\n",
            "Iteration 8062, Loss: 0.011505357921123505\n",
            "Iteration 8063, Loss: 0.009852860122919083\n",
            "Iteration 8064, Loss: 0.012538570910692215\n",
            "Iteration 8065, Loss: 0.006710867863148451\n",
            "Iteration 8066, Loss: 0.006788930855691433\n",
            "Iteration 8067, Loss: 0.009286323562264442\n",
            "Iteration 8068, Loss: 0.00850209966301918\n",
            "Iteration 8069, Loss: 0.015408259816467762\n",
            "Iteration 8070, Loss: 0.0078114476054906845\n",
            "Iteration 8071, Loss: 0.015929292887449265\n",
            "Iteration 8072, Loss: 0.011482278816401958\n",
            "Iteration 8073, Loss: 0.010727927088737488\n",
            "Iteration 8074, Loss: 0.008043631911277771\n",
            "Iteration 8075, Loss: 0.015246886759996414\n",
            "Iteration 8076, Loss: 0.01132731232792139\n",
            "Iteration 8077, Loss: 0.012250132858753204\n",
            "Iteration 8078, Loss: 0.010077517479658127\n",
            "Iteration 8079, Loss: 0.013709396123886108\n",
            "Iteration 8080, Loss: 0.008440415374934673\n",
            "Iteration 8081, Loss: 0.010017004795372486\n",
            "Iteration 8082, Loss: 0.009646494872868061\n",
            "Iteration 8083, Loss: 0.009126730263233185\n",
            "Iteration 8084, Loss: 0.010107581503689289\n",
            "Iteration 8085, Loss: 0.011792419478297234\n",
            "Iteration 8086, Loss: 0.013415736146271229\n",
            "Iteration 8087, Loss: 0.008931105025112629\n",
            "Iteration 8088, Loss: 0.009148265235126019\n",
            "Iteration 8089, Loss: 0.011736956425011158\n",
            "Iteration 8090, Loss: 0.010684249922633171\n",
            "Iteration 8091, Loss: 0.011447535827755928\n",
            "Iteration 8092, Loss: 0.015844494104385376\n",
            "Iteration 8093, Loss: 0.008644018322229385\n",
            "Iteration 8094, Loss: 0.009847933426499367\n",
            "Iteration 8095, Loss: 0.010755723342299461\n",
            "Iteration 8096, Loss: 0.008789945393800735\n",
            "Iteration 8097, Loss: 0.006504709832370281\n",
            "Iteration 8098, Loss: 0.008626037277281284\n",
            "Iteration 8099, Loss: 0.008337017148733139\n",
            "Iteration 8100, Loss: 0.008096636272966862\n",
            "Iteration 8101, Loss: 0.00814842525869608\n",
            "Iteration 8102, Loss: 0.01045696809887886\n",
            "Iteration 8103, Loss: 0.012353850528597832\n",
            "Iteration 8104, Loss: 0.009582537226378918\n",
            "Iteration 8105, Loss: 0.00843850988894701\n",
            "Iteration 8106, Loss: 0.008555996231734753\n",
            "Iteration 8107, Loss: 0.005068046972155571\n",
            "Iteration 8108, Loss: 0.010400839149951935\n",
            "Iteration 8109, Loss: 0.008306235074996948\n",
            "Iteration 8110, Loss: 0.010953652672469616\n",
            "Iteration 8111, Loss: 0.011874919757246971\n",
            "Iteration 8112, Loss: 0.007682594005018473\n",
            "Iteration 8113, Loss: 0.009761318564414978\n",
            "Iteration 8114, Loss: 0.013162460178136826\n",
            "Iteration 8115, Loss: 0.01118569727987051\n",
            "Iteration 8116, Loss: 0.016282279044389725\n",
            "Iteration 8117, Loss: 0.01415124163031578\n",
            "Iteration 8118, Loss: 0.012078662402927876\n",
            "Iteration 8119, Loss: 0.01144927553832531\n",
            "Iteration 8120, Loss: 0.01107551995664835\n",
            "Iteration 8121, Loss: 0.008135689422488213\n",
            "Iteration 8122, Loss: 0.010550661943852901\n",
            "Iteration 8123, Loss: 0.017404403537511826\n",
            "Iteration 8124, Loss: 0.010340815410017967\n",
            "Iteration 8125, Loss: 0.011367558501660824\n",
            "Iteration 8126, Loss: 0.007749756798148155\n",
            "Iteration 8127, Loss: 0.009892450645565987\n",
            "Iteration 8128, Loss: 0.011133696883916855\n",
            "Iteration 8129, Loss: 0.009692179970443249\n",
            "Iteration 8130, Loss: 0.009626396931707859\n",
            "Iteration 8131, Loss: 0.008797080256044865\n",
            "Iteration 8132, Loss: 0.012987968511879444\n",
            "Iteration 8133, Loss: 0.010961469262838364\n",
            "Iteration 8134, Loss: 0.011812553741037846\n",
            "Iteration 8135, Loss: 0.0073548429645597935\n",
            "Iteration 8136, Loss: 0.007749131415039301\n",
            "Iteration 8137, Loss: 0.005124949850142002\n",
            "Iteration 8138, Loss: 0.009999103844165802\n",
            "Iteration 8139, Loss: 0.009740566834807396\n",
            "Iteration 8140, Loss: 0.012034683488309383\n",
            "Iteration 8141, Loss: 0.00877281092107296\n",
            "Iteration 8142, Loss: 0.011238708160817623\n",
            "Iteration 8143, Loss: 0.00915749091655016\n",
            "Iteration 8144, Loss: 0.010586509481072426\n",
            "Iteration 8145, Loss: 0.009561263956129551\n",
            "Iteration 8146, Loss: 0.010135353542864323\n",
            "Iteration 8147, Loss: 0.010618222877383232\n",
            "Iteration 8148, Loss: 0.007032476365566254\n",
            "Iteration 8149, Loss: 0.01160957757383585\n",
            "Iteration 8150, Loss: 0.011805742047727108\n",
            "Iteration 8151, Loss: 0.008061413653194904\n",
            "Iteration 8152, Loss: 0.007943220436573029\n",
            "Iteration 8153, Loss: 0.007172498852014542\n",
            "Iteration 8154, Loss: 0.008640091866254807\n",
            "Iteration 8155, Loss: 0.011371017433702946\n",
            "Iteration 8156, Loss: 0.007511795032769442\n",
            "Iteration 8157, Loss: 0.005669231992214918\n",
            "Iteration 8158, Loss: 0.012734136544167995\n",
            "Iteration 8159, Loss: 0.00972706824541092\n",
            "Iteration 8160, Loss: 0.006668186280876398\n",
            "Iteration 8161, Loss: 0.011266407556831837\n",
            "Iteration 8162, Loss: 0.009033669717609882\n",
            "Iteration 8163, Loss: 0.012683614157140255\n",
            "Iteration 8164, Loss: 0.013686834834516048\n",
            "Iteration 8165, Loss: 0.008162012323737144\n",
            "Iteration 8166, Loss: 0.01343683060258627\n",
            "Iteration 8167, Loss: 0.008587248623371124\n",
            "Iteration 8168, Loss: 0.008276652544736862\n",
            "Iteration 8169, Loss: 0.01122921984642744\n",
            "Iteration 8170, Loss: 0.008000342175364494\n",
            "Iteration 8171, Loss: 0.017478682100772858\n",
            "Iteration 8172, Loss: 0.013051304966211319\n",
            "Iteration 8173, Loss: 0.00929254014045\n",
            "Iteration 8174, Loss: 0.009774698875844479\n",
            "Iteration 8175, Loss: 0.009333377704024315\n",
            "Iteration 8176, Loss: 0.009794846177101135\n",
            "Iteration 8177, Loss: 0.014717393554747105\n",
            "Iteration 8178, Loss: 0.00969482958316803\n",
            "Iteration 8179, Loss: 0.012665913440287113\n",
            "Iteration 8180, Loss: 0.006795286666601896\n",
            "Iteration 8181, Loss: 0.011576313525438309\n",
            "Iteration 8182, Loss: 0.00839268323034048\n",
            "Iteration 8183, Loss: 0.016035130247473717\n",
            "Iteration 8184, Loss: 0.007160905282944441\n",
            "Iteration 8185, Loss: 0.005732366349548101\n",
            "Iteration 8186, Loss: 0.011797148734331131\n",
            "Iteration 8187, Loss: 0.0063697644509375095\n",
            "Iteration 8188, Loss: 0.012424410320818424\n",
            "Iteration 8189, Loss: 0.010503931902348995\n",
            "Iteration 8190, Loss: 0.011936698108911514\n",
            "Iteration 8191, Loss: 0.014037050306797028\n",
            "Iteration 8192, Loss: 0.012458611279726028\n",
            "Iteration 8193, Loss: 0.009921565651893616\n",
            "Iteration 8194, Loss: 0.013121364638209343\n",
            "Iteration 8195, Loss: 0.010347072966396809\n",
            "Iteration 8196, Loss: 0.016542213037610054\n",
            "Iteration 8197, Loss: 0.006475002039223909\n",
            "Iteration 8198, Loss: 0.009079976938664913\n",
            "Iteration 8199, Loss: 0.00995783880352974\n",
            "Iteration 8200, Loss: 0.006208030041307211\n",
            "Iteration 8201, Loss: 0.006750119850039482\n",
            "Iteration 8202, Loss: 0.012131894007325172\n",
            "Iteration 8203, Loss: 0.009698637761175632\n",
            "Iteration 8204, Loss: 0.006941379979252815\n",
            "Iteration 8205, Loss: 0.01359874289482832\n",
            "Iteration 8206, Loss: 0.006177790928632021\n",
            "Iteration 8207, Loss: 0.01105150394141674\n",
            "Iteration 8208, Loss: 0.012364882975816727\n",
            "Iteration 8209, Loss: 0.011793808080255985\n",
            "Iteration 8210, Loss: 0.011996185407042503\n",
            "Iteration 8211, Loss: 0.007603654637932777\n",
            "Iteration 8212, Loss: 0.006071350537240505\n",
            "Iteration 8213, Loss: 0.013869944959878922\n",
            "Iteration 8214, Loss: 0.008947482332587242\n",
            "Iteration 8215, Loss: 0.01078890822827816\n",
            "Iteration 8216, Loss: 0.005007901228964329\n",
            "Iteration 8217, Loss: 0.012955044396221638\n",
            "Iteration 8218, Loss: 0.00788208469748497\n",
            "Iteration 8219, Loss: 0.013612893410027027\n",
            "Iteration 8220, Loss: 0.01144418865442276\n",
            "Iteration 8221, Loss: 0.011465382762253284\n",
            "Iteration 8222, Loss: 0.00903965812176466\n",
            "Iteration 8223, Loss: 0.012777362018823624\n",
            "Iteration 8224, Loss: 0.00805681198835373\n",
            "Iteration 8225, Loss: 0.0093220891430974\n",
            "Iteration 8226, Loss: 0.007889006286859512\n",
            "Iteration 8227, Loss: 0.011304126121103764\n",
            "Iteration 8228, Loss: 0.006199065130203962\n",
            "Iteration 8229, Loss: 0.006789416074752808\n",
            "Iteration 8230, Loss: 0.01246690284460783\n",
            "Iteration 8231, Loss: 0.007316295523196459\n",
            "Iteration 8232, Loss: 0.010913163423538208\n",
            "Iteration 8233, Loss: 0.013484584167599678\n",
            "Iteration 8234, Loss: 0.01206943392753601\n",
            "Iteration 8235, Loss: 0.005970717407763004\n",
            "Iteration 8236, Loss: 0.00846005417406559\n",
            "Iteration 8237, Loss: 0.008309233002364635\n",
            "Iteration 8238, Loss: 0.008151515386998653\n",
            "Iteration 8239, Loss: 0.010882405564188957\n",
            "Iteration 8240, Loss: 0.009202088229358196\n",
            "Iteration 8241, Loss: 0.014815118163824081\n",
            "Iteration 8242, Loss: 0.009362626820802689\n",
            "Iteration 8243, Loss: 0.013669283129274845\n",
            "Iteration 8244, Loss: 0.007263171952217817\n",
            "Iteration 8245, Loss: 0.009730185382068157\n",
            "Iteration 8246, Loss: 0.00727592408657074\n",
            "Iteration 8247, Loss: 0.008523339405655861\n",
            "Iteration 8248, Loss: 0.012821580283343792\n",
            "Iteration 8249, Loss: 0.008922843262553215\n",
            "Iteration 8250, Loss: 0.006846904288977385\n",
            "Iteration 8251, Loss: 0.0133194075897336\n",
            "Iteration 8252, Loss: 0.01118993666023016\n",
            "Iteration 8253, Loss: 0.012466670013964176\n",
            "Iteration 8254, Loss: 0.008299710229039192\n",
            "Iteration 8255, Loss: 0.009093303233385086\n",
            "Iteration 8256, Loss: 0.010370336472988129\n",
            "Iteration 8257, Loss: 0.011725272051990032\n",
            "Iteration 8258, Loss: 0.010718450881540775\n",
            "Iteration 8259, Loss: 0.010298737324774265\n",
            "Iteration 8260, Loss: 0.00982279609888792\n",
            "Iteration 8261, Loss: 0.010508181527256966\n",
            "Iteration 8262, Loss: 0.005354337394237518\n",
            "Iteration 8263, Loss: 0.01078118197619915\n",
            "Iteration 8264, Loss: 0.008721464313566685\n",
            "Iteration 8265, Loss: 0.010202236473560333\n",
            "Iteration 8266, Loss: 0.010926240123808384\n",
            "Iteration 8267, Loss: 0.010794717818498611\n",
            "Iteration 8268, Loss: 0.008795615285634995\n",
            "Iteration 8269, Loss: 0.00954628735780716\n",
            "Iteration 8270, Loss: 0.016755759716033936\n",
            "Iteration 8271, Loss: 0.01104601938277483\n",
            "Iteration 8272, Loss: 0.010360871441662312\n",
            "Iteration 8273, Loss: 0.011174400337040424\n",
            "Iteration 8274, Loss: 0.008420132100582123\n",
            "Iteration 8275, Loss: 0.012857905589044094\n",
            "Iteration 8276, Loss: 0.010779043659567833\n",
            "Iteration 8277, Loss: 0.008870747871696949\n",
            "Iteration 8278, Loss: 0.011431804858148098\n",
            "Iteration 8279, Loss: 0.016427665948867798\n",
            "Iteration 8280, Loss: 0.008548162877559662\n",
            "Iteration 8281, Loss: 0.00960056483745575\n",
            "Iteration 8282, Loss: 0.011444930918514729\n",
            "Iteration 8283, Loss: 0.009457036852836609\n",
            "Iteration 8284, Loss: 0.008000344038009644\n",
            "Iteration 8285, Loss: 0.009707311168313026\n",
            "Iteration 8286, Loss: 0.006390497554093599\n",
            "Iteration 8287, Loss: 0.007555462885648012\n",
            "Iteration 8288, Loss: 0.010263440199196339\n",
            "Iteration 8289, Loss: 0.00917133316397667\n",
            "Iteration 8290, Loss: 0.006561456713825464\n",
            "Iteration 8291, Loss: 0.007965716533362865\n",
            "Iteration 8292, Loss: 0.008179896511137486\n",
            "Iteration 8293, Loss: 0.012786519713699818\n",
            "Iteration 8294, Loss: 0.009328560903668404\n",
            "Iteration 8295, Loss: 0.008695608004927635\n",
            "Iteration 8296, Loss: 0.00940841343253851\n",
            "Iteration 8297, Loss: 0.011031990870833397\n",
            "Iteration 8298, Loss: 0.009446889162063599\n",
            "Iteration 8299, Loss: 0.007665348704904318\n",
            "Iteration 8300, Loss: 0.012650981545448303\n",
            "Iteration 8301, Loss: 0.009031785652041435\n",
            "Iteration 8302, Loss: 0.01105158869177103\n",
            "Iteration 8303, Loss: 0.00787302479147911\n",
            "Iteration 8304, Loss: 0.012920246459543705\n",
            "Iteration 8305, Loss: 0.013788463547825813\n",
            "Iteration 8306, Loss: 0.006886661518365145\n",
            "Iteration 8307, Loss: 0.01133773848414421\n",
            "Iteration 8308, Loss: 0.006096099968999624\n",
            "Iteration 8309, Loss: 0.007597717922180891\n",
            "Iteration 8310, Loss: 0.009573429822921753\n",
            "Iteration 8311, Loss: 0.011171281337738037\n",
            "Iteration 8312, Loss: 0.008163287304341793\n",
            "Iteration 8313, Loss: 0.012713865377008915\n",
            "Iteration 8314, Loss: 0.013021399267017841\n",
            "Iteration 8315, Loss: 0.002799867419525981\n",
            "Iteration 8316, Loss: 0.008053532801568508\n",
            "Iteration 8317, Loss: 0.007721496745944023\n",
            "Iteration 8318, Loss: 0.011143277399241924\n",
            "Iteration 8319, Loss: 0.008869796991348267\n",
            "Iteration 8320, Loss: 0.015386275947093964\n",
            "Iteration 8321, Loss: 0.010663825087249279\n",
            "Iteration 8322, Loss: 0.0073694102466106415\n",
            "Iteration 8323, Loss: 0.011150685139000416\n",
            "Iteration 8324, Loss: 0.010006379336118698\n",
            "Iteration 8325, Loss: 0.008920861408114433\n",
            "Iteration 8326, Loss: 0.010908826254308224\n",
            "Iteration 8327, Loss: 0.011951899155974388\n",
            "Iteration 8328, Loss: 0.006918180733919144\n",
            "Iteration 8329, Loss: 0.010482019744813442\n",
            "Iteration 8330, Loss: 0.011429030448198318\n",
            "Iteration 8331, Loss: 0.01176514383405447\n",
            "Iteration 8332, Loss: 0.010847997851669788\n",
            "Iteration 8333, Loss: 0.011363843455910683\n",
            "Iteration 8334, Loss: 0.007688435260206461\n",
            "Iteration 8335, Loss: 0.005758459679782391\n",
            "Iteration 8336, Loss: 0.011090600863099098\n",
            "Iteration 8337, Loss: 0.00714898994192481\n",
            "Iteration 8338, Loss: 0.006583972368389368\n",
            "Iteration 8339, Loss: 0.008274259977042675\n",
            "Iteration 8340, Loss: 0.009858359582722187\n",
            "Iteration 8341, Loss: 0.007061646785587072\n",
            "Iteration 8342, Loss: 0.011627843603491783\n",
            "Iteration 8343, Loss: 0.008179285563528538\n",
            "Iteration 8344, Loss: 0.006963802967220545\n",
            "Iteration 8345, Loss: 0.012044350616633892\n",
            "Iteration 8346, Loss: 0.008073807694017887\n",
            "Iteration 8347, Loss: 0.011957714334130287\n",
            "Iteration 8348, Loss: 0.003908568527549505\n",
            "Iteration 8349, Loss: 0.009303322993218899\n",
            "Iteration 8350, Loss: 0.01084912195801735\n",
            "Iteration 8351, Loss: 0.012169556692242622\n",
            "Iteration 8352, Loss: 0.010133880190551281\n",
            "Iteration 8353, Loss: 0.012713291682302952\n",
            "Iteration 8354, Loss: 0.007598482072353363\n",
            "Iteration 8355, Loss: 0.006509092170745134\n",
            "Iteration 8356, Loss: 0.009501812048256397\n",
            "Iteration 8357, Loss: 0.00811608973890543\n",
            "Iteration 8358, Loss: 0.009674605913460255\n",
            "Iteration 8359, Loss: 0.010142719373106956\n",
            "Iteration 8360, Loss: 0.007983733899891376\n",
            "Iteration 8361, Loss: 0.011457356624305248\n",
            "Iteration 8362, Loss: 0.008879348635673523\n",
            "Iteration 8363, Loss: 0.00965728797018528\n",
            "Iteration 8364, Loss: 0.007354958448559046\n",
            "Iteration 8365, Loss: 0.01119025144726038\n",
            "Iteration 8366, Loss: 0.00814894400537014\n",
            "Iteration 8367, Loss: 0.01028593722730875\n",
            "Iteration 8368, Loss: 0.0125100277364254\n",
            "Iteration 8369, Loss: 0.008190571330487728\n",
            "Iteration 8370, Loss: 0.010695656761527061\n",
            "Iteration 8371, Loss: 0.008863100782036781\n",
            "Iteration 8372, Loss: 0.007284568622708321\n",
            "Iteration 8373, Loss: 0.009603649377822876\n",
            "Iteration 8374, Loss: 0.004557526204735041\n",
            "Iteration 8375, Loss: 0.0069899363443255424\n",
            "Iteration 8376, Loss: 0.01205717958509922\n",
            "Iteration 8377, Loss: 0.009176054038107395\n",
            "Iteration 8378, Loss: 0.007720968686044216\n",
            "Iteration 8379, Loss: 0.00651808874681592\n",
            "Iteration 8380, Loss: 0.008173821493983269\n",
            "Iteration 8381, Loss: 0.0038968881126493216\n",
            "Iteration 8382, Loss: 0.01026916690170765\n",
            "Iteration 8383, Loss: 0.011396205052733421\n",
            "Iteration 8384, Loss: 0.01137586124241352\n",
            "Iteration 8385, Loss: 0.010178728960454464\n",
            "Iteration 8386, Loss: 0.00878795888274908\n",
            "Iteration 8387, Loss: 0.00680508092045784\n",
            "Iteration 8388, Loss: 0.012999100610613823\n",
            "Iteration 8389, Loss: 0.011150624603033066\n",
            "Iteration 8390, Loss: 0.007144985254853964\n",
            "Iteration 8391, Loss: 0.010436858050525188\n",
            "Iteration 8392, Loss: 0.010192334651947021\n",
            "Iteration 8393, Loss: 0.010985405184328556\n",
            "Iteration 8394, Loss: 0.008876113221049309\n",
            "Iteration 8395, Loss: 0.012082993052899837\n",
            "Iteration 8396, Loss: 0.008400737307965755\n",
            "Iteration 8397, Loss: 0.012610665522515774\n",
            "Iteration 8398, Loss: 0.006587444804608822\n",
            "Iteration 8399, Loss: 0.010348713956773281\n",
            "Iteration 8400, Loss: 0.006385936867445707\n",
            "Iteration 8401, Loss: 0.01218042615801096\n",
            "Iteration 8402, Loss: 0.011508425697684288\n",
            "Iteration 8403, Loss: 0.012907295487821102\n",
            "Iteration 8404, Loss: 0.009889188222587109\n",
            "Iteration 8405, Loss: 0.010843520984053612\n",
            "Iteration 8406, Loss: 0.008076566271483898\n",
            "Iteration 8407, Loss: 0.014167778193950653\n",
            "Iteration 8408, Loss: 0.015377378091216087\n",
            "Iteration 8409, Loss: 0.01016565877944231\n",
            "Iteration 8410, Loss: 0.010343964211642742\n",
            "Iteration 8411, Loss: 0.006296253297477961\n",
            "Iteration 8412, Loss: 0.0055776676163077354\n",
            "Iteration 8413, Loss: 0.011466155759990215\n",
            "Iteration 8414, Loss: 0.008902794681489468\n",
            "Iteration 8415, Loss: 0.005526700522750616\n",
            "Iteration 8416, Loss: 0.009194975718855858\n",
            "Iteration 8417, Loss: 0.008971788920462132\n",
            "Iteration 8418, Loss: 0.009245073422789574\n",
            "Iteration 8419, Loss: 0.008527467958629131\n",
            "Iteration 8420, Loss: 0.008363643661141396\n",
            "Iteration 8421, Loss: 0.008480573073029518\n",
            "Iteration 8422, Loss: 0.007124230731278658\n",
            "Iteration 8423, Loss: 0.008967670612037182\n",
            "Iteration 8424, Loss: 0.009816448204219341\n",
            "Iteration 8425, Loss: 0.007582050748169422\n",
            "Iteration 8426, Loss: 0.010887146927416325\n",
            "Iteration 8427, Loss: 0.010799532756209373\n",
            "Iteration 8428, Loss: 0.0064936853013932705\n",
            "Iteration 8429, Loss: 0.008945899084210396\n",
            "Iteration 8430, Loss: 0.011412975378334522\n",
            "Iteration 8431, Loss: 0.009159152396023273\n",
            "Iteration 8432, Loss: 0.013762558810412884\n",
            "Iteration 8433, Loss: 0.012614049948751926\n",
            "Iteration 8434, Loss: 0.010761869139969349\n",
            "Iteration 8435, Loss: 0.008724463172256947\n",
            "Iteration 8436, Loss: 0.008614875376224518\n",
            "Iteration 8437, Loss: 0.011049767956137657\n",
            "Iteration 8438, Loss: 0.0099220871925354\n",
            "Iteration 8439, Loss: 0.010239655151963234\n",
            "Iteration 8440, Loss: 0.009061946533620358\n",
            "Iteration 8441, Loss: 0.009551363065838814\n",
            "Iteration 8442, Loss: 0.010811646468937397\n",
            "Iteration 8443, Loss: 0.009012483060359955\n",
            "Iteration 8444, Loss: 0.00860982108861208\n",
            "Iteration 8445, Loss: 0.008555721491575241\n",
            "Iteration 8446, Loss: 0.008067024871706963\n",
            "Iteration 8447, Loss: 0.008412325754761696\n",
            "Iteration 8448, Loss: 0.012547051534056664\n",
            "Iteration 8449, Loss: 0.01016074325889349\n",
            "Iteration 8450, Loss: 0.010794906876981258\n",
            "Iteration 8451, Loss: 0.009428104385733604\n",
            "Iteration 8452, Loss: 0.009989097714424133\n",
            "Iteration 8453, Loss: 0.00860307551920414\n",
            "Iteration 8454, Loss: 0.009565353393554688\n",
            "Iteration 8455, Loss: 0.0052398755215108395\n",
            "Iteration 8456, Loss: 0.014637977816164494\n",
            "Iteration 8457, Loss: 0.008917310275137424\n",
            "Iteration 8458, Loss: 0.011028194800019264\n",
            "Iteration 8459, Loss: 0.008799740113317966\n",
            "Iteration 8460, Loss: 0.008282660506665707\n",
            "Iteration 8461, Loss: 0.009310619905591011\n",
            "Iteration 8462, Loss: 0.006411845330148935\n",
            "Iteration 8463, Loss: 0.012263095006346703\n",
            "Iteration 8464, Loss: 0.004112120717763901\n",
            "Iteration 8465, Loss: 0.013972421176731586\n",
            "Iteration 8466, Loss: 0.011061848141252995\n",
            "Iteration 8467, Loss: 0.008400022983551025\n",
            "Iteration 8468, Loss: 0.007116801105439663\n",
            "Iteration 8469, Loss: 0.010536439716815948\n",
            "Iteration 8470, Loss: 0.011551382020115852\n",
            "Iteration 8471, Loss: 0.00676551042124629\n",
            "Iteration 8472, Loss: 0.009905293583869934\n",
            "Iteration 8473, Loss: 0.011033830232918262\n",
            "Iteration 8474, Loss: 0.00905157532542944\n",
            "Iteration 8475, Loss: 0.012002536095678806\n",
            "Iteration 8476, Loss: 0.009708620607852936\n",
            "Iteration 8477, Loss: 0.012168309651315212\n",
            "Iteration 8478, Loss: 0.012193896807730198\n",
            "Iteration 8479, Loss: 0.006811725907027721\n",
            "Iteration 8480, Loss: 0.007062137592583895\n",
            "Iteration 8481, Loss: 0.0118674011901021\n",
            "Iteration 8482, Loss: 0.010981057770550251\n",
            "Iteration 8483, Loss: 0.009609154425561428\n",
            "Iteration 8484, Loss: 0.013638612814247608\n",
            "Iteration 8485, Loss: 0.014128136448562145\n",
            "Iteration 8486, Loss: 0.007458576932549477\n",
            "Iteration 8487, Loss: 0.008493363857269287\n",
            "Iteration 8488, Loss: 0.0098307766020298\n",
            "Iteration 8489, Loss: 0.007659165654331446\n",
            "Iteration 8490, Loss: 0.007958960719406605\n",
            "Iteration 8491, Loss: 0.007360420189797878\n",
            "Iteration 8492, Loss: 0.005666551645845175\n",
            "Iteration 8493, Loss: 0.010567894205451012\n",
            "Iteration 8494, Loss: 0.014136663638055325\n",
            "Iteration 8495, Loss: 0.01045938953757286\n",
            "Iteration 8496, Loss: 0.012460713274776936\n",
            "Iteration 8497, Loss: 0.008862805552780628\n",
            "Iteration 8498, Loss: 0.009719393216073513\n",
            "Iteration 8499, Loss: 0.01165168359875679\n",
            "Iteration 8500, Loss: 0.008516930043697357\n",
            "Iteration 8501, Loss: 0.003576825372874737\n",
            "Iteration 8502, Loss: 0.010381488129496574\n",
            "Iteration 8503, Loss: 0.008236407302320004\n",
            "Iteration 8504, Loss: 0.008388613350689411\n",
            "Iteration 8505, Loss: 0.013021044433116913\n",
            "Iteration 8506, Loss: 0.008366874419152737\n",
            "Iteration 8507, Loss: 0.010481765493750572\n",
            "Iteration 8508, Loss: 0.010949641466140747\n",
            "Iteration 8509, Loss: 0.016483446583151817\n",
            "Iteration 8510, Loss: 0.011100029572844505\n",
            "Iteration 8511, Loss: 0.010639668442308903\n",
            "Iteration 8512, Loss: 0.009849565103650093\n",
            "Iteration 8513, Loss: 0.012403393164277077\n",
            "Iteration 8514, Loss: 0.007895855233073235\n",
            "Iteration 8515, Loss: 0.01075021643191576\n",
            "Iteration 8516, Loss: 0.009716542437672615\n",
            "Iteration 8517, Loss: 0.007204962894320488\n",
            "Iteration 8518, Loss: 0.011407637037336826\n",
            "Iteration 8519, Loss: 0.010776267386972904\n",
            "Iteration 8520, Loss: 0.0060930498875677586\n",
            "Iteration 8521, Loss: 0.0071588498540222645\n",
            "Iteration 8522, Loss: 0.009846534579992294\n",
            "Iteration 8523, Loss: 0.008826722390949726\n",
            "Iteration 8524, Loss: 0.008438155986368656\n",
            "Iteration 8525, Loss: 0.012084977701306343\n",
            "Iteration 8526, Loss: 0.00828911829739809\n",
            "Iteration 8527, Loss: 0.008617338724434376\n",
            "Iteration 8528, Loss: 0.01004253514111042\n",
            "Iteration 8529, Loss: 0.012376858852803707\n",
            "Iteration 8530, Loss: 0.006309065967798233\n",
            "Iteration 8531, Loss: 0.009465976618230343\n",
            "Iteration 8532, Loss: 0.01030148845165968\n",
            "Iteration 8533, Loss: 0.008370582014322281\n",
            "Iteration 8534, Loss: 0.01062967162579298\n",
            "Iteration 8535, Loss: 0.006594357080757618\n",
            "Iteration 8536, Loss: 0.009230045601725578\n",
            "Iteration 8537, Loss: 0.0069009242579340935\n",
            "Iteration 8538, Loss: 0.013127087615430355\n",
            "Iteration 8539, Loss: 0.008593219332396984\n",
            "Iteration 8540, Loss: 0.0085328733548522\n",
            "Iteration 8541, Loss: 0.010430335067212582\n",
            "Iteration 8542, Loss: 0.013207025825977325\n",
            "Iteration 8543, Loss: 0.01199615653604269\n",
            "Iteration 8544, Loss: 0.007337029092013836\n",
            "Iteration 8545, Loss: 0.011596493422985077\n",
            "Iteration 8546, Loss: 0.008645779453217983\n",
            "Iteration 8547, Loss: 0.010113329626619816\n",
            "Iteration 8548, Loss: 0.007704739924520254\n",
            "Iteration 8549, Loss: 0.008091888390481472\n",
            "Iteration 8550, Loss: 0.00873029325157404\n",
            "Iteration 8551, Loss: 0.005506381392478943\n",
            "Iteration 8552, Loss: 0.011248993687331676\n",
            "Iteration 8553, Loss: 0.012520892545580864\n",
            "Iteration 8554, Loss: 0.010672652162611485\n",
            "Iteration 8555, Loss: 0.010783640667796135\n",
            "Iteration 8556, Loss: 0.007757337298244238\n",
            "Iteration 8557, Loss: 0.0097908740863204\n",
            "Iteration 8558, Loss: 0.00938924215734005\n",
            "Iteration 8559, Loss: 0.013015109114348888\n",
            "Iteration 8560, Loss: 0.008129196241497993\n",
            "Iteration 8561, Loss: 0.014755305834114552\n",
            "Iteration 8562, Loss: 0.009317290037870407\n",
            "Iteration 8563, Loss: 0.006049139425158501\n",
            "Iteration 8564, Loss: 0.010137739591300488\n",
            "Iteration 8565, Loss: 0.012119070626795292\n",
            "Iteration 8566, Loss: 0.007758543360978365\n",
            "Iteration 8567, Loss: 0.011304796673357487\n",
            "Iteration 8568, Loss: 0.01072615385055542\n",
            "Iteration 8569, Loss: 0.011118527501821518\n",
            "Iteration 8570, Loss: 0.0073054488748312\n",
            "Iteration 8571, Loss: 0.006166892126202583\n",
            "Iteration 8572, Loss: 0.007720094174146652\n",
            "Iteration 8573, Loss: 0.010428178124129772\n",
            "Iteration 8574, Loss: 0.008748726919293404\n",
            "Iteration 8575, Loss: 0.005347904283553362\n",
            "Iteration 8576, Loss: 0.011983068659901619\n",
            "Iteration 8577, Loss: 0.01204211451113224\n",
            "Iteration 8578, Loss: 0.008207648992538452\n",
            "Iteration 8579, Loss: 0.008939174003899097\n",
            "Iteration 8580, Loss: 0.010070186108350754\n",
            "Iteration 8581, Loss: 0.010266894474625587\n",
            "Iteration 8582, Loss: 0.007175687234848738\n",
            "Iteration 8583, Loss: 0.009444466792047024\n",
            "Iteration 8584, Loss: 0.005840064492076635\n",
            "Iteration 8585, Loss: 0.010261936113238335\n",
            "Iteration 8586, Loss: 0.007871788926422596\n",
            "Iteration 8587, Loss: 0.009695803746581078\n",
            "Iteration 8588, Loss: 0.01159459538757801\n",
            "Iteration 8589, Loss: 0.014789978973567486\n",
            "Iteration 8590, Loss: 0.007878093980252743\n",
            "Iteration 8591, Loss: 0.008970648050308228\n",
            "Iteration 8592, Loss: 0.01222548633813858\n",
            "Iteration 8593, Loss: 0.011418119072914124\n",
            "Iteration 8594, Loss: 0.00916427094489336\n",
            "Iteration 8595, Loss: 0.013401332311332226\n",
            "Iteration 8596, Loss: 0.009073426946997643\n",
            "Iteration 8597, Loss: 0.008507500402629375\n",
            "Iteration 8598, Loss: 0.012054710648953915\n",
            "Iteration 8599, Loss: 0.014671255834400654\n",
            "Iteration 8600, Loss: 0.010110864415764809\n",
            "Iteration 8601, Loss: 0.013739596121013165\n",
            "Iteration 8602, Loss: 0.0071707833558321\n",
            "Iteration 8603, Loss: 0.00782748218625784\n",
            "Iteration 8604, Loss: 0.011773214675486088\n",
            "Iteration 8605, Loss: 0.01008556503802538\n",
            "Iteration 8606, Loss: 0.008921030908823013\n",
            "Iteration 8607, Loss: 0.007879001088440418\n",
            "Iteration 8608, Loss: 0.012895602732896805\n",
            "Iteration 8609, Loss: 0.007277599535882473\n",
            "Iteration 8610, Loss: 0.009182849898934364\n",
            "Iteration 8611, Loss: 0.009046124294400215\n",
            "Iteration 8612, Loss: 0.007840095087885857\n",
            "Iteration 8613, Loss: 0.011222606524825096\n",
            "Iteration 8614, Loss: 0.00916766095906496\n",
            "Iteration 8615, Loss: 0.007412676699459553\n",
            "Iteration 8616, Loss: 0.009323514066636562\n",
            "Iteration 8617, Loss: 0.009075559675693512\n",
            "Iteration 8618, Loss: 0.010875357314944267\n",
            "Iteration 8619, Loss: 0.009142161346971989\n",
            "Iteration 8620, Loss: 0.008058503270149231\n",
            "Iteration 8621, Loss: 0.009410771541297436\n",
            "Iteration 8622, Loss: 0.01408234890550375\n",
            "Iteration 8623, Loss: 0.009198109619319439\n",
            "Iteration 8624, Loss: 0.01565486565232277\n",
            "Iteration 8625, Loss: 0.011472005397081375\n",
            "Iteration 8626, Loss: 0.006634480319917202\n",
            "Iteration 8627, Loss: 0.007013702765107155\n",
            "Iteration 8628, Loss: 0.010640472173690796\n",
            "Iteration 8629, Loss: 0.006758854258805513\n",
            "Iteration 8630, Loss: 0.009419901296496391\n",
            "Iteration 8631, Loss: 0.0128925247117877\n",
            "Iteration 8632, Loss: 0.010540351271629333\n",
            "Iteration 8633, Loss: 0.006835772190243006\n",
            "Iteration 8634, Loss: 0.008223527111113071\n",
            "Iteration 8635, Loss: 0.0063859848305583\n",
            "Iteration 8636, Loss: 0.007659096270799637\n",
            "Iteration 8637, Loss: 0.007273404859006405\n",
            "Iteration 8638, Loss: 0.008676654659211636\n",
            "Iteration 8639, Loss: 0.00934840738773346\n",
            "Iteration 8640, Loss: 0.010359750129282475\n",
            "Iteration 8641, Loss: 0.005061242263764143\n",
            "Iteration 8642, Loss: 0.006164328660815954\n",
            "Iteration 8643, Loss: 0.010855795815587044\n",
            "Iteration 8644, Loss: 0.01142014842480421\n",
            "Iteration 8645, Loss: 0.009795264340937138\n",
            "Iteration 8646, Loss: 0.011094414629042149\n",
            "Iteration 8647, Loss: 0.011779039166867733\n",
            "Iteration 8648, Loss: 0.010614126920700073\n",
            "Iteration 8649, Loss: 0.007925531826913357\n",
            "Iteration 8650, Loss: 0.011169254779815674\n",
            "Iteration 8651, Loss: 0.013000781647861004\n",
            "Iteration 8652, Loss: 0.006935192737728357\n",
            "Iteration 8653, Loss: 0.010881997644901276\n",
            "Iteration 8654, Loss: 0.011477665044367313\n",
            "Iteration 8655, Loss: 0.01082968432456255\n",
            "Iteration 8656, Loss: 0.011108244769275188\n",
            "Iteration 8657, Loss: 0.008156082592904568\n",
            "Iteration 8658, Loss: 0.009145613759756088\n",
            "Iteration 8659, Loss: 0.0063325813971459866\n",
            "Iteration 8660, Loss: 0.010237381793558598\n",
            "Iteration 8661, Loss: 0.013520901091396809\n",
            "Iteration 8662, Loss: 0.010220078751444817\n",
            "Iteration 8663, Loss: 0.007783201988786459\n",
            "Iteration 8664, Loss: 0.011402128264307976\n",
            "Iteration 8665, Loss: 0.01046653464436531\n",
            "Iteration 8666, Loss: 0.011154112406075\n",
            "Iteration 8667, Loss: 0.007698277942836285\n",
            "Iteration 8668, Loss: 0.010943768545985222\n",
            "Iteration 8669, Loss: 0.007758824620395899\n",
            "Iteration 8670, Loss: 0.009919214993715286\n",
            "Iteration 8671, Loss: 0.011203433386981487\n",
            "Iteration 8672, Loss: 0.006831742823123932\n",
            "Iteration 8673, Loss: 0.008134155534207821\n",
            "Iteration 8674, Loss: 0.011388729326426983\n",
            "Iteration 8675, Loss: 0.008438874036073685\n",
            "Iteration 8676, Loss: 0.008887283504009247\n",
            "Iteration 8677, Loss: 0.008028559386730194\n",
            "Iteration 8678, Loss: 0.009003803133964539\n",
            "Iteration 8679, Loss: 0.01160955149680376\n",
            "Iteration 8680, Loss: 0.007647598162293434\n",
            "Iteration 8681, Loss: 0.010589754208922386\n",
            "Iteration 8682, Loss: 0.007115072570741177\n",
            "Iteration 8683, Loss: 0.010435360483825207\n",
            "Iteration 8684, Loss: 0.0077211251482367516\n",
            "Iteration 8685, Loss: 0.008525921031832695\n",
            "Iteration 8686, Loss: 0.005753972567617893\n",
            "Iteration 8687, Loss: 0.0134478360414505\n",
            "Iteration 8688, Loss: 0.008247465826570988\n",
            "Iteration 8689, Loss: 0.005731836427003145\n",
            "Iteration 8690, Loss: 0.009886170737445354\n",
            "Iteration 8691, Loss: 0.01363122183829546\n",
            "Iteration 8692, Loss: 0.013570721261203289\n",
            "Iteration 8693, Loss: 0.008999372832477093\n",
            "Iteration 8694, Loss: 0.013689770363271236\n",
            "Iteration 8695, Loss: 0.008237002417445183\n",
            "Iteration 8696, Loss: 0.008244762197136879\n",
            "Iteration 8697, Loss: 0.010475712828338146\n",
            "Iteration 8698, Loss: 0.008921220898628235\n",
            "Iteration 8699, Loss: 0.010867358185350895\n",
            "Iteration 8700, Loss: 0.008270453661680222\n",
            "Iteration 8701, Loss: 0.007920668460428715\n",
            "Iteration 8702, Loss: 0.014930801466107368\n",
            "Iteration 8703, Loss: 0.009015051648020744\n",
            "Iteration 8704, Loss: 0.006634239107370377\n",
            "Iteration 8705, Loss: 0.007346206810325384\n",
            "Iteration 8706, Loss: 0.012147956527769566\n",
            "Iteration 8707, Loss: 0.00590786337852478\n",
            "Iteration 8708, Loss: 0.011882987804710865\n",
            "Iteration 8709, Loss: 0.009861943311989307\n",
            "Iteration 8710, Loss: 0.007390821352601051\n",
            "Iteration 8711, Loss: 0.009918591938912868\n",
            "Iteration 8712, Loss: 0.00844519305974245\n",
            "Iteration 8713, Loss: 0.011156449094414711\n",
            "Iteration 8714, Loss: 0.01165796723216772\n",
            "Iteration 8715, Loss: 0.0064534093253314495\n",
            "Iteration 8716, Loss: 0.009483824484050274\n",
            "Iteration 8717, Loss: 0.010392743162810802\n",
            "Iteration 8718, Loss: 0.006701843347400427\n",
            "Iteration 8719, Loss: 0.007934898138046265\n",
            "Iteration 8720, Loss: 0.011300520971417427\n",
            "Iteration 8721, Loss: 0.007792850490659475\n",
            "Iteration 8722, Loss: 0.015003904700279236\n",
            "Iteration 8723, Loss: 0.011087115854024887\n",
            "Iteration 8724, Loss: 0.007460173685103655\n",
            "Iteration 8725, Loss: 0.01206143107265234\n",
            "Iteration 8726, Loss: 0.008146317675709724\n",
            "Iteration 8727, Loss: 0.00459653977304697\n",
            "Iteration 8728, Loss: 0.010242721997201443\n",
            "Iteration 8729, Loss: 0.006949231028556824\n",
            "Iteration 8730, Loss: 0.011954930610954762\n",
            "Iteration 8731, Loss: 0.007181328721344471\n",
            "Iteration 8732, Loss: 0.005927989259362221\n",
            "Iteration 8733, Loss: 0.005882900208234787\n",
            "Iteration 8734, Loss: 0.011611289344727993\n",
            "Iteration 8735, Loss: 0.010383382439613342\n",
            "Iteration 8736, Loss: 0.011328899301588535\n",
            "Iteration 8737, Loss: 0.007492006290704012\n",
            "Iteration 8738, Loss: 0.005087395664304495\n",
            "Iteration 8739, Loss: 0.01127813383936882\n",
            "Iteration 8740, Loss: 0.006704196333885193\n",
            "Iteration 8741, Loss: 0.011230630800127983\n",
            "Iteration 8742, Loss: 0.010582557879388332\n",
            "Iteration 8743, Loss: 0.0073669664561748505\n",
            "Iteration 8744, Loss: 0.00932155642658472\n",
            "Iteration 8745, Loss: 0.013049229979515076\n",
            "Iteration 8746, Loss: 0.007065390702337027\n",
            "Iteration 8747, Loss: 0.008258511312305927\n",
            "Iteration 8748, Loss: 0.008033601567149162\n",
            "Iteration 8749, Loss: 0.009129870682954788\n",
            "Iteration 8750, Loss: 0.013262934051454067\n",
            "Iteration 8751, Loss: 0.01186851691454649\n",
            "Iteration 8752, Loss: 0.008258688263595104\n",
            "Iteration 8753, Loss: 0.008979210630059242\n",
            "Iteration 8754, Loss: 0.009552735835313797\n",
            "Iteration 8755, Loss: 0.010375762358307838\n",
            "Iteration 8756, Loss: 0.011525485664606094\n",
            "Iteration 8757, Loss: 0.009552017785608768\n",
            "Iteration 8758, Loss: 0.010851360857486725\n",
            "Iteration 8759, Loss: 0.009876605123281479\n",
            "Iteration 8760, Loss: 0.010750352405011654\n",
            "Iteration 8761, Loss: 0.015403085388243198\n",
            "Iteration 8762, Loss: 0.010715608485043049\n",
            "Iteration 8763, Loss: 0.012721178121864796\n",
            "Iteration 8764, Loss: 0.009006845764815807\n",
            "Iteration 8765, Loss: 0.010546894744038582\n",
            "Iteration 8766, Loss: 0.009660761803388596\n",
            "Iteration 8767, Loss: 0.01150471717119217\n",
            "Iteration 8768, Loss: 0.006128915585577488\n",
            "Iteration 8769, Loss: 0.014389927498996258\n",
            "Iteration 8770, Loss: 0.011320477351546288\n",
            "Iteration 8771, Loss: 0.010790563188493252\n",
            "Iteration 8772, Loss: 0.007448011077940464\n",
            "Iteration 8773, Loss: 0.010937929153442383\n",
            "Iteration 8774, Loss: 0.011652183718979359\n",
            "Iteration 8775, Loss: 0.00997556746006012\n",
            "Iteration 8776, Loss: 0.00883706659078598\n",
            "Iteration 8777, Loss: 0.010096875950694084\n",
            "Iteration 8778, Loss: 0.009647739119827747\n",
            "Iteration 8779, Loss: 0.007128048688173294\n",
            "Iteration 8780, Loss: 0.010531997308135033\n",
            "Iteration 8781, Loss: 0.012051460333168507\n",
            "Iteration 8782, Loss: 0.0072986893355846405\n",
            "Iteration 8783, Loss: 0.007053529843688011\n",
            "Iteration 8784, Loss: 0.00879797525703907\n",
            "Iteration 8785, Loss: 0.00709221325814724\n",
            "Iteration 8786, Loss: 0.011073506437242031\n",
            "Iteration 8787, Loss: 0.008409674279391766\n",
            "Iteration 8788, Loss: 0.005285268183797598\n",
            "Iteration 8789, Loss: 0.007096541579812765\n",
            "Iteration 8790, Loss: 0.00828239880502224\n",
            "Iteration 8791, Loss: 0.0063458401709795\n",
            "Iteration 8792, Loss: 0.008466370403766632\n",
            "Iteration 8793, Loss: 0.009202235378324986\n",
            "Iteration 8794, Loss: 0.010610798373818398\n",
            "Iteration 8795, Loss: 0.009146549738943577\n",
            "Iteration 8796, Loss: 0.007703905459493399\n",
            "Iteration 8797, Loss: 0.011648102663457394\n",
            "Iteration 8798, Loss: 0.012350287288427353\n",
            "Iteration 8799, Loss: 0.009742246940732002\n",
            "Iteration 8800, Loss: 0.01187677402049303\n",
            "Iteration 8801, Loss: 0.006752179469913244\n",
            "Iteration 8802, Loss: 0.009534153155982494\n",
            "Iteration 8803, Loss: 0.014878334477543831\n",
            "Iteration 8804, Loss: 0.009978472255170345\n",
            "Iteration 8805, Loss: 0.011248674243688583\n",
            "Iteration 8806, Loss: 0.011445311829447746\n",
            "Iteration 8807, Loss: 0.008337276987731457\n",
            "Iteration 8808, Loss: 0.010351995006203651\n",
            "Iteration 8809, Loss: 0.00947772990912199\n",
            "Iteration 8810, Loss: 0.011299303732812405\n",
            "Iteration 8811, Loss: 0.00961556751281023\n",
            "Iteration 8812, Loss: 0.009824812412261963\n",
            "Iteration 8813, Loss: 0.010600214824080467\n",
            "Iteration 8814, Loss: 0.004719033371657133\n",
            "Iteration 8815, Loss: 0.008288382552564144\n",
            "Iteration 8816, Loss: 0.006886269897222519\n",
            "Iteration 8817, Loss: 0.010825625620782375\n",
            "Iteration 8818, Loss: 0.008590820245444775\n",
            "Iteration 8819, Loss: 0.00842094887048006\n",
            "Iteration 8820, Loss: 0.009065777063369751\n",
            "Iteration 8821, Loss: 0.009386219084262848\n",
            "Iteration 8822, Loss: 0.01103286538273096\n",
            "Iteration 8823, Loss: 0.012945162132382393\n",
            "Iteration 8824, Loss: 0.009171419776976109\n",
            "Iteration 8825, Loss: 0.01156921312212944\n",
            "Iteration 8826, Loss: 0.008353454992175102\n",
            "Iteration 8827, Loss: 0.009407350793480873\n",
            "Iteration 8828, Loss: 0.008791008032858372\n",
            "Iteration 8829, Loss: 0.007319544907659292\n",
            "Iteration 8830, Loss: 0.010293807834386826\n",
            "Iteration 8831, Loss: 0.007321659941226244\n",
            "Iteration 8832, Loss: 0.008559093810617924\n",
            "Iteration 8833, Loss: 0.005789839196950197\n",
            "Iteration 8834, Loss: 0.009844992309808731\n",
            "Iteration 8835, Loss: 0.007970966398715973\n",
            "Iteration 8836, Loss: 0.009652846492826939\n",
            "Iteration 8837, Loss: 0.010475065559148788\n",
            "Iteration 8838, Loss: 0.007205368019640446\n",
            "Iteration 8839, Loss: 0.012669041752815247\n",
            "Iteration 8840, Loss: 0.009042254649102688\n",
            "Iteration 8841, Loss: 0.007787599228322506\n",
            "Iteration 8842, Loss: 0.007498998660594225\n",
            "Iteration 8843, Loss: 0.014261712320148945\n",
            "Iteration 8844, Loss: 0.00836833007633686\n",
            "Iteration 8845, Loss: 0.010051504708826542\n",
            "Iteration 8846, Loss: 0.009884452447295189\n",
            "Iteration 8847, Loss: 0.009868054650723934\n",
            "Iteration 8848, Loss: 0.01045972015708685\n",
            "Iteration 8849, Loss: 0.012057126499712467\n",
            "Iteration 8850, Loss: 0.0070728156715631485\n",
            "Iteration 8851, Loss: 0.010468936525285244\n",
            "Iteration 8852, Loss: 0.00991969183087349\n",
            "Iteration 8853, Loss: 0.012495470233261585\n",
            "Iteration 8854, Loss: 0.0070708743296563625\n",
            "Iteration 8855, Loss: 0.006637607701122761\n",
            "Iteration 8856, Loss: 0.006120854523032904\n",
            "Iteration 8857, Loss: 0.008695287629961967\n",
            "Iteration 8858, Loss: 0.008206114172935486\n",
            "Iteration 8859, Loss: 0.007244101259857416\n",
            "Iteration 8860, Loss: 0.012768021784722805\n",
            "Iteration 8861, Loss: 0.011866633780300617\n",
            "Iteration 8862, Loss: 0.007588925305753946\n",
            "Iteration 8863, Loss: 0.0063514080829918385\n",
            "Iteration 8864, Loss: 0.00799060333520174\n",
            "Iteration 8865, Loss: 0.011948124505579472\n",
            "Iteration 8866, Loss: 0.00890545453876257\n",
            "Iteration 8867, Loss: 0.005693830084055662\n",
            "Iteration 8868, Loss: 0.008993147872388363\n",
            "Iteration 8869, Loss: 0.005510580260306597\n",
            "Iteration 8870, Loss: 0.009757785126566887\n",
            "Iteration 8871, Loss: 0.006765053607523441\n",
            "Iteration 8872, Loss: 0.007232632953673601\n",
            "Iteration 8873, Loss: 0.010783658362925053\n",
            "Iteration 8874, Loss: 0.010517257265746593\n",
            "Iteration 8875, Loss: 0.0064608086831867695\n",
            "Iteration 8876, Loss: 0.009623968042433262\n",
            "Iteration 8877, Loss: 0.007891073822975159\n",
            "Iteration 8878, Loss: 0.011025789193809032\n",
            "Iteration 8879, Loss: 0.01040441170334816\n",
            "Iteration 8880, Loss: 0.014507816173136234\n",
            "Iteration 8881, Loss: 0.008376384153962135\n",
            "Iteration 8882, Loss: 0.0052388496696949005\n",
            "Iteration 8883, Loss: 0.009079642593860626\n",
            "Iteration 8884, Loss: 0.012163029983639717\n",
            "Iteration 8885, Loss: 0.009149291552603245\n",
            "Iteration 8886, Loss: 0.006686040665954351\n",
            "Iteration 8887, Loss: 0.006447739899158478\n",
            "Iteration 8888, Loss: 0.007189191412180662\n",
            "Iteration 8889, Loss: 0.007495249155908823\n",
            "Iteration 8890, Loss: 0.010131077840924263\n",
            "Iteration 8891, Loss: 0.00793906394392252\n",
            "Iteration 8892, Loss: 0.007279011886566877\n",
            "Iteration 8893, Loss: 0.011996833607554436\n",
            "Iteration 8894, Loss: 0.0061468034982681274\n",
            "Iteration 8895, Loss: 0.009829713962972164\n",
            "Iteration 8896, Loss: 0.007931734435260296\n",
            "Iteration 8897, Loss: 0.010290269739925861\n",
            "Iteration 8898, Loss: 0.009075391106307507\n",
            "Iteration 8899, Loss: 0.00709969038143754\n",
            "Iteration 8900, Loss: 0.010684357024729252\n",
            "Iteration 8901, Loss: 0.008429378271102905\n",
            "Iteration 8902, Loss: 0.007357239257544279\n",
            "Iteration 8903, Loss: 0.007180010434240103\n",
            "Iteration 8904, Loss: 0.01160221267491579\n",
            "Iteration 8905, Loss: 0.0064251781441271305\n",
            "Iteration 8906, Loss: 0.013158059678971767\n",
            "Iteration 8907, Loss: 0.010008134879171848\n",
            "Iteration 8908, Loss: 0.008580058813095093\n",
            "Iteration 8909, Loss: 0.008081434294581413\n",
            "Iteration 8910, Loss: 0.00858963094651699\n",
            "Iteration 8911, Loss: 0.009311976842582226\n",
            "Iteration 8912, Loss: 0.01076916977763176\n",
            "Iteration 8913, Loss: 0.009572160430252552\n",
            "Iteration 8914, Loss: 0.007376804482191801\n",
            "Iteration 8915, Loss: 0.010701868683099747\n",
            "Iteration 8916, Loss: 0.01224656030535698\n",
            "Iteration 8917, Loss: 0.00931297056376934\n",
            "Iteration 8918, Loss: 0.007986221462488174\n",
            "Iteration 8919, Loss: 0.011422413401305676\n",
            "Iteration 8920, Loss: 0.009132587350904942\n",
            "Iteration 8921, Loss: 0.005141077563166618\n",
            "Iteration 8922, Loss: 0.008142858743667603\n",
            "Iteration 8923, Loss: 0.012068144045770168\n",
            "Iteration 8924, Loss: 0.010540633462369442\n",
            "Iteration 8925, Loss: 0.006060902960598469\n",
            "Iteration 8926, Loss: 0.007136967498809099\n",
            "Iteration 8927, Loss: 0.012615477666258812\n",
            "Iteration 8928, Loss: 0.010928998701274395\n",
            "Iteration 8929, Loss: 0.012829767540097237\n",
            "Iteration 8930, Loss: 0.007319935597479343\n",
            "Iteration 8931, Loss: 0.01034657284617424\n",
            "Iteration 8932, Loss: 0.005928834900259972\n",
            "Iteration 8933, Loss: 0.010824866592884064\n",
            "Iteration 8934, Loss: 0.008399687707424164\n",
            "Iteration 8935, Loss: 0.008066573180258274\n",
            "Iteration 8936, Loss: 0.011973237618803978\n",
            "Iteration 8937, Loss: 0.008652272634208202\n",
            "Iteration 8938, Loss: 0.006392495706677437\n",
            "Iteration 8939, Loss: 0.010684177279472351\n",
            "Iteration 8940, Loss: 0.0080155273899436\n",
            "Iteration 8941, Loss: 0.010089422576129436\n",
            "Iteration 8942, Loss: 0.006942472420632839\n",
            "Iteration 8943, Loss: 0.009256795048713684\n",
            "Iteration 8944, Loss: 0.011878781951963902\n",
            "Iteration 8945, Loss: 0.00991237536072731\n",
            "Iteration 8946, Loss: 0.006640365347266197\n",
            "Iteration 8947, Loss: 0.008538782596588135\n",
            "Iteration 8948, Loss: 0.011740194633603096\n",
            "Iteration 8949, Loss: 0.0067134276032447815\n",
            "Iteration 8950, Loss: 0.009460578672587872\n",
            "Iteration 8951, Loss: 0.011737532913684845\n",
            "Iteration 8952, Loss: 0.010280069895088673\n",
            "Iteration 8953, Loss: 0.010469499044120312\n",
            "Iteration 8954, Loss: 0.005350732710212469\n",
            "Iteration 8955, Loss: 0.00981265027076006\n",
            "Iteration 8956, Loss: 0.008032208308577538\n",
            "Iteration 8957, Loss: 0.013140733353793621\n",
            "Iteration 8958, Loss: 0.010597302578389645\n",
            "Iteration 8959, Loss: 0.008560075424611568\n",
            "Iteration 8960, Loss: 0.006846589967608452\n",
            "Iteration 8961, Loss: 0.006948447320610285\n",
            "Iteration 8962, Loss: 0.010209005326032639\n",
            "Iteration 8963, Loss: 0.011737389490008354\n",
            "Iteration 8964, Loss: 0.00860885065048933\n",
            "Iteration 8965, Loss: 0.010342429392039776\n",
            "Iteration 8966, Loss: 0.007847688160836697\n",
            "Iteration 8967, Loss: 0.006835476495325565\n",
            "Iteration 8968, Loss: 0.009514903649687767\n",
            "Iteration 8969, Loss: 0.008212322369217873\n",
            "Iteration 8970, Loss: 0.012132514268159866\n",
            "Iteration 8971, Loss: 0.00827278383076191\n",
            "Iteration 8972, Loss: 0.009395413100719452\n",
            "Iteration 8973, Loss: 0.007674041204154491\n",
            "Iteration 8974, Loss: 0.009369750507175922\n",
            "Iteration 8975, Loss: 0.010175535455346107\n",
            "Iteration 8976, Loss: 0.014827284961938858\n",
            "Iteration 8977, Loss: 0.013137845322489738\n",
            "Iteration 8978, Loss: 0.011550047434866428\n",
            "Iteration 8979, Loss: 0.014009797014296055\n",
            "Iteration 8980, Loss: 0.007895420305430889\n",
            "Iteration 8981, Loss: 0.013778152875602245\n",
            "Iteration 8982, Loss: 0.009925374761223793\n",
            "Iteration 8983, Loss: 0.01034634094685316\n",
            "Iteration 8984, Loss: 0.009193625301122665\n",
            "Iteration 8985, Loss: 0.010272940620779991\n",
            "Iteration 8986, Loss: 0.008820721879601479\n",
            "Iteration 8987, Loss: 0.005664526950567961\n",
            "Iteration 8988, Loss: 0.013054080307483673\n",
            "Iteration 8989, Loss: 0.010467804968357086\n",
            "Iteration 8990, Loss: 0.006418278440833092\n",
            "Iteration 8991, Loss: 0.005830906797200441\n",
            "Iteration 8992, Loss: 0.011905521154403687\n",
            "Iteration 8993, Loss: 0.010161047801375389\n",
            "Iteration 8994, Loss: 0.011095218360424042\n",
            "Iteration 8995, Loss: 0.006498794071376324\n",
            "Iteration 8996, Loss: 0.006447714753448963\n",
            "Iteration 8997, Loss: 0.008078126236796379\n",
            "Iteration 8998, Loss: 0.005469330120831728\n",
            "Iteration 8999, Loss: 0.009044951759278774\n",
            "Iteration 9000, Loss: 0.00799773633480072\n",
            "Test Loss: 0.0627245381474495\n",
            "Iteration 9001, Loss: 0.008176536299288273\n",
            "Iteration 9002, Loss: 0.010938619263470173\n",
            "Iteration 9003, Loss: 0.011408605612814426\n",
            "Iteration 9004, Loss: 0.0076866853050887585\n",
            "Iteration 9005, Loss: 0.00867471657693386\n",
            "Iteration 9006, Loss: 0.008111421950161457\n",
            "Iteration 9007, Loss: 0.011543774977326393\n",
            "Iteration 9008, Loss: 0.008335663937032223\n",
            "Iteration 9009, Loss: 0.011791635304689407\n",
            "Iteration 9010, Loss: 0.004707955289632082\n",
            "Iteration 9011, Loss: 0.010815585032105446\n",
            "Iteration 9012, Loss: 0.006631534546613693\n",
            "Iteration 9013, Loss: 0.008883239701390266\n",
            "Iteration 9014, Loss: 0.007964917458593845\n",
            "Iteration 9015, Loss: 0.010068155825138092\n",
            "Iteration 9016, Loss: 0.006515082437545061\n",
            "Iteration 9017, Loss: 0.006955564953386784\n",
            "Iteration 9018, Loss: 0.010634837672114372\n",
            "Iteration 9019, Loss: 0.00973935890942812\n",
            "Iteration 9020, Loss: 0.00796942226588726\n",
            "Iteration 9021, Loss: 0.009058750234544277\n",
            "Iteration 9022, Loss: 0.008758693002164364\n",
            "Iteration 9023, Loss: 0.003981185145676136\n",
            "Iteration 9024, Loss: 0.006978393066674471\n",
            "Iteration 9025, Loss: 0.009623057208955288\n",
            "Iteration 9026, Loss: 0.005954039748758078\n",
            "Iteration 9027, Loss: 0.006936919875442982\n",
            "Iteration 9028, Loss: 0.009088464081287384\n",
            "Iteration 9029, Loss: 0.012229139916598797\n",
            "Iteration 9030, Loss: 0.008488422259688377\n",
            "Iteration 9031, Loss: 0.012376914732158184\n",
            "Iteration 9032, Loss: 0.004079483449459076\n",
            "Iteration 9033, Loss: 0.009084673598408699\n",
            "Iteration 9034, Loss: 0.0034669514279812574\n",
            "Iteration 9035, Loss: 0.010802274569869041\n",
            "Iteration 9036, Loss: 0.009662426076829433\n",
            "Iteration 9037, Loss: 0.01191945280879736\n",
            "Iteration 9038, Loss: 0.009200270287692547\n",
            "Iteration 9039, Loss: 0.01254312228411436\n",
            "Iteration 9040, Loss: 0.006480427924543619\n",
            "Iteration 9041, Loss: 0.010967233218252659\n",
            "Iteration 9042, Loss: 0.0113209318369627\n",
            "Iteration 9043, Loss: 0.0062128277495503426\n",
            "Iteration 9044, Loss: 0.008575716987252235\n",
            "Iteration 9045, Loss: 0.007305289153009653\n",
            "Iteration 9046, Loss: 0.010047130286693573\n",
            "Iteration 9047, Loss: 0.008655965328216553\n",
            "Iteration 9048, Loss: 0.016171041876077652\n",
            "Iteration 9049, Loss: 0.006894266232848167\n",
            "Iteration 9050, Loss: 0.009822981432080269\n",
            "Iteration 9051, Loss: 0.007582344580441713\n",
            "Iteration 9052, Loss: 0.008908051997423172\n",
            "Iteration 9053, Loss: 0.010663051158189774\n",
            "Iteration 9054, Loss: 0.01295027881860733\n",
            "Iteration 9055, Loss: 0.010588450357317924\n",
            "Iteration 9056, Loss: 0.009873236529529095\n",
            "Iteration 9057, Loss: 0.009594783186912537\n",
            "Iteration 9058, Loss: 0.009661066345870495\n",
            "Iteration 9059, Loss: 0.013269786722958088\n",
            "Iteration 9060, Loss: 0.005409715231508017\n",
            "Iteration 9061, Loss: 0.009684944525361061\n",
            "Iteration 9062, Loss: 0.008080675266683102\n",
            "Iteration 9063, Loss: 0.009577264077961445\n",
            "Iteration 9064, Loss: 0.008394594304263592\n",
            "Iteration 9065, Loss: 0.010947058908641338\n",
            "Iteration 9066, Loss: 0.008547068573534489\n",
            "Iteration 9067, Loss: 0.010787180624902248\n",
            "Iteration 9068, Loss: 0.011780370026826859\n",
            "Iteration 9069, Loss: 0.00804784707725048\n",
            "Iteration 9070, Loss: 0.008967013098299503\n",
            "Iteration 9071, Loss: 0.01755552552640438\n",
            "Iteration 9072, Loss: 0.007377327419817448\n",
            "Iteration 9073, Loss: 0.006399231962859631\n",
            "Iteration 9074, Loss: 0.010521694086492062\n",
            "Iteration 9075, Loss: 0.00826010387390852\n",
            "Iteration 9076, Loss: 0.008755138143897057\n",
            "Iteration 9077, Loss: 0.01726658083498478\n",
            "Iteration 9078, Loss: 0.011231820099055767\n",
            "Iteration 9079, Loss: 0.010680647566914558\n",
            "Iteration 9080, Loss: 0.010564067400991917\n",
            "Iteration 9081, Loss: 0.0076883225701749325\n",
            "Iteration 9082, Loss: 0.00722438795492053\n",
            "Iteration 9083, Loss: 0.013428730890154839\n",
            "Iteration 9084, Loss: 0.00924493558704853\n",
            "Iteration 9085, Loss: 0.009729423560202122\n",
            "Iteration 9086, Loss: 0.009054073132574558\n",
            "Iteration 9087, Loss: 0.011526614427566528\n",
            "Iteration 9088, Loss: 0.00959430169314146\n",
            "Iteration 9089, Loss: 0.006217805668711662\n",
            "Iteration 9090, Loss: 0.0059670438058674335\n",
            "Iteration 9091, Loss: 0.009480856359004974\n",
            "Iteration 9092, Loss: 0.01584971696138382\n",
            "Iteration 9093, Loss: 0.009400373324751854\n",
            "Iteration 9094, Loss: 0.00992134865373373\n",
            "Iteration 9095, Loss: 0.01149233989417553\n",
            "Iteration 9096, Loss: 0.009157243184745312\n",
            "Iteration 9097, Loss: 0.011149370111525059\n",
            "Iteration 9098, Loss: 0.01039773691445589\n",
            "Iteration 9099, Loss: 0.012193194590508938\n",
            "Iteration 9100, Loss: 0.01103003229945898\n",
            "Iteration 9101, Loss: 0.009203127585351467\n",
            "Iteration 9102, Loss: 0.009829910472035408\n",
            "Iteration 9103, Loss: 0.014943799003958702\n",
            "Iteration 9104, Loss: 0.011022523045539856\n",
            "Iteration 9105, Loss: 0.009082470089197159\n",
            "Iteration 9106, Loss: 0.00930787529796362\n",
            "Iteration 9107, Loss: 0.007429169956594706\n",
            "Iteration 9108, Loss: 0.007648121565580368\n",
            "Iteration 9109, Loss: 0.01192541141062975\n",
            "Iteration 9110, Loss: 0.006531021557748318\n",
            "Iteration 9111, Loss: 0.008264338597655296\n",
            "Iteration 9112, Loss: 0.009771975688636303\n",
            "Iteration 9113, Loss: 0.007378099951893091\n",
            "Iteration 9114, Loss: 0.007464087102562189\n",
            "Iteration 9115, Loss: 0.003909846767783165\n",
            "Iteration 9116, Loss: 0.009211089462041855\n",
            "Iteration 9117, Loss: 0.010132563300430775\n",
            "Iteration 9118, Loss: 0.016211161389946938\n",
            "Iteration 9119, Loss: 0.0069919792003929615\n",
            "Iteration 9120, Loss: 0.008333205245435238\n",
            "Iteration 9121, Loss: 0.013376648537814617\n",
            "Iteration 9122, Loss: 0.006824270822107792\n",
            "Iteration 9123, Loss: 0.01522418949753046\n",
            "Iteration 9124, Loss: 0.007532954216003418\n",
            "Iteration 9125, Loss: 0.010296942666172981\n",
            "Iteration 9126, Loss: 0.0069414288736879826\n",
            "Iteration 9127, Loss: 0.011503136716783047\n",
            "Iteration 9128, Loss: 0.007723446004092693\n",
            "Iteration 9129, Loss: 0.010785565711557865\n",
            "Iteration 9130, Loss: 0.009035738185048103\n",
            "Iteration 9131, Loss: 0.007234422490000725\n",
            "Iteration 9132, Loss: 0.008042244240641594\n",
            "Iteration 9133, Loss: 0.009464365430176258\n",
            "Iteration 9134, Loss: 0.008857965469360352\n",
            "Iteration 9135, Loss: 0.010752556845545769\n",
            "Iteration 9136, Loss: 0.0070328242145478725\n",
            "Iteration 9137, Loss: 0.01152181439101696\n",
            "Iteration 9138, Loss: 0.011081595905125141\n",
            "Iteration 9139, Loss: 0.01077517680823803\n",
            "Iteration 9140, Loss: 0.00938029121607542\n",
            "Iteration 9141, Loss: 0.013145776465535164\n",
            "Iteration 9142, Loss: 0.013450675643980503\n",
            "Iteration 9143, Loss: 0.0119396410882473\n",
            "Iteration 9144, Loss: 0.008795050904154778\n",
            "Iteration 9145, Loss: 0.012690916657447815\n",
            "Iteration 9146, Loss: 0.012192738242447376\n",
            "Iteration 9147, Loss: 0.008108701556921005\n",
            "Iteration 9148, Loss: 0.006706410087645054\n",
            "Iteration 9149, Loss: 0.005719373468309641\n",
            "Iteration 9150, Loss: 0.00768635468557477\n",
            "Iteration 9151, Loss: 0.010469922795891762\n",
            "Iteration 9152, Loss: 0.008014132268726826\n",
            "Iteration 9153, Loss: 0.00982616562396288\n",
            "Iteration 9154, Loss: 0.014109711162745953\n",
            "Iteration 9155, Loss: 0.010352147743105888\n",
            "Iteration 9156, Loss: 0.008566279895603657\n",
            "Iteration 9157, Loss: 0.011937766335904598\n",
            "Iteration 9158, Loss: 0.008438076823949814\n",
            "Iteration 9159, Loss: 0.006848354823887348\n",
            "Iteration 9160, Loss: 0.015443461015820503\n",
            "Iteration 9161, Loss: 0.006831456441432238\n",
            "Iteration 9162, Loss: 0.009154082275927067\n",
            "Iteration 9163, Loss: 0.011952006258070469\n",
            "Iteration 9164, Loss: 0.010055306367576122\n",
            "Iteration 9165, Loss: 0.013921321369707584\n",
            "Iteration 9166, Loss: 0.009650763124227524\n",
            "Iteration 9167, Loss: 0.012760622426867485\n",
            "Iteration 9168, Loss: 0.00759065942838788\n",
            "Iteration 9169, Loss: 0.009552206844091415\n",
            "Iteration 9170, Loss: 0.00757745373994112\n",
            "Iteration 9171, Loss: 0.009900166653096676\n",
            "Iteration 9172, Loss: 0.007557434029877186\n",
            "Iteration 9173, Loss: 0.00788435060530901\n",
            "Iteration 9174, Loss: 0.010651134885847569\n",
            "Iteration 9175, Loss: 0.013833701610565186\n",
            "Iteration 9176, Loss: 0.011951247230172157\n",
            "Iteration 9177, Loss: 0.01036813948303461\n",
            "Iteration 9178, Loss: 0.013697990216314793\n",
            "Iteration 9179, Loss: 0.0057444460690021515\n",
            "Iteration 9180, Loss: 0.009783908724784851\n",
            "Iteration 9181, Loss: 0.01286181528121233\n",
            "Iteration 9182, Loss: 0.01004871353507042\n",
            "Iteration 9183, Loss: 0.012423533014953136\n",
            "Iteration 9184, Loss: 0.007936610840260983\n",
            "Iteration 9185, Loss: 0.010146155022084713\n",
            "Iteration 9186, Loss: 0.011170836165547371\n",
            "Iteration 9187, Loss: 0.006317028775811195\n",
            "Iteration 9188, Loss: 0.00935319159179926\n",
            "Iteration 9189, Loss: 0.007402581628412008\n",
            "Iteration 9190, Loss: 0.006767776794731617\n",
            "Iteration 9191, Loss: 0.00896076112985611\n",
            "Iteration 9192, Loss: 0.007264337036758661\n",
            "Iteration 9193, Loss: 0.012064171023666859\n",
            "Iteration 9194, Loss: 0.009783116169273853\n",
            "Iteration 9195, Loss: 0.006311165168881416\n",
            "Iteration 9196, Loss: 0.009116081520915031\n",
            "Iteration 9197, Loss: 0.012314703315496445\n",
            "Iteration 9198, Loss: 0.010575396940112114\n",
            "Iteration 9199, Loss: 0.007403101772069931\n",
            "Iteration 9200, Loss: 0.007116662338376045\n",
            "Iteration 9201, Loss: 0.008921985514461994\n",
            "Iteration 9202, Loss: 0.014044707641005516\n",
            "Iteration 9203, Loss: 0.007125568576157093\n",
            "Iteration 9204, Loss: 0.01161192636936903\n",
            "Iteration 9205, Loss: 0.0075864591635763645\n",
            "Iteration 9206, Loss: 0.00616091676056385\n",
            "Iteration 9207, Loss: 0.008267696015536785\n",
            "Iteration 9208, Loss: 0.008840428665280342\n",
            "Iteration 9209, Loss: 0.009496618993580341\n",
            "Iteration 9210, Loss: 0.010050018317997456\n",
            "Iteration 9211, Loss: 0.010597056709229946\n",
            "Iteration 9212, Loss: 0.009902171790599823\n",
            "Iteration 9213, Loss: 0.008766901679337025\n",
            "Iteration 9214, Loss: 0.008592412807047367\n",
            "Iteration 9215, Loss: 0.007880212739109993\n",
            "Iteration 9216, Loss: 0.013686221092939377\n",
            "Iteration 9217, Loss: 0.00654074689373374\n",
            "Iteration 9218, Loss: 0.00747474143281579\n",
            "Iteration 9219, Loss: 0.008965527638792992\n",
            "Iteration 9220, Loss: 0.01023516058921814\n",
            "Iteration 9221, Loss: 0.011442213319242\n",
            "Iteration 9222, Loss: 0.010121234692633152\n",
            "Iteration 9223, Loss: 0.010799156501889229\n",
            "Iteration 9224, Loss: 0.009254341013729572\n",
            "Iteration 9225, Loss: 0.006564231589436531\n",
            "Iteration 9226, Loss: 0.007126173935830593\n",
            "Iteration 9227, Loss: 0.009384334087371826\n",
            "Iteration 9228, Loss: 0.008501168340444565\n",
            "Iteration 9229, Loss: 0.010154924355447292\n",
            "Iteration 9230, Loss: 0.010998607613146305\n",
            "Iteration 9231, Loss: 0.007120428141206503\n",
            "Iteration 9232, Loss: 0.009763441979885101\n",
            "Iteration 9233, Loss: 0.010886192321777344\n",
            "Iteration 9234, Loss: 0.008268414996564388\n",
            "Iteration 9235, Loss: 0.008841894567012787\n",
            "Iteration 9236, Loss: 0.007309625390917063\n",
            "Iteration 9237, Loss: 0.008850903250277042\n",
            "Iteration 9238, Loss: 0.00862579233944416\n",
            "Iteration 9239, Loss: 0.008226219564676285\n",
            "Iteration 9240, Loss: 0.010799359530210495\n",
            "Iteration 9241, Loss: 0.009380168281495571\n",
            "Iteration 9242, Loss: 0.01383710652589798\n",
            "Iteration 9243, Loss: 0.006294047925621271\n",
            "Iteration 9244, Loss: 0.007631273940205574\n",
            "Iteration 9245, Loss: 0.012906665913760662\n",
            "Iteration 9246, Loss: 0.0075255585834383965\n",
            "Iteration 9247, Loss: 0.01030447892844677\n",
            "Iteration 9248, Loss: 0.00706237880513072\n",
            "Iteration 9249, Loss: 0.010156565345823765\n",
            "Iteration 9250, Loss: 0.014001315459609032\n",
            "Iteration 9251, Loss: 0.0055479672737419605\n",
            "Iteration 9252, Loss: 0.009545931592583656\n",
            "Iteration 9253, Loss: 0.015796782448887825\n",
            "Iteration 9254, Loss: 0.009059668518602848\n",
            "Iteration 9255, Loss: 0.009272868745028973\n",
            "Iteration 9256, Loss: 0.003884838195517659\n",
            "Iteration 9257, Loss: 0.013727971352636814\n",
            "Iteration 9258, Loss: 0.010527187027037144\n",
            "Iteration 9259, Loss: 0.011477726511657238\n",
            "Iteration 9260, Loss: 0.007437732070684433\n",
            "Iteration 9261, Loss: 0.007217532955110073\n",
            "Iteration 9262, Loss: 0.008189579471945763\n",
            "Iteration 9263, Loss: 0.00787986908107996\n",
            "Iteration 9264, Loss: 0.007089951075613499\n",
            "Iteration 9265, Loss: 0.006179360672831535\n",
            "Iteration 9266, Loss: 0.007919303141534328\n",
            "Iteration 9267, Loss: 0.006241532973945141\n",
            "Iteration 9268, Loss: 0.009755050763487816\n",
            "Iteration 9269, Loss: 0.010888839140534401\n",
            "Iteration 9270, Loss: 0.01082602608948946\n",
            "Iteration 9271, Loss: 0.006635911297053099\n",
            "Iteration 9272, Loss: 0.011572962626814842\n",
            "Iteration 9273, Loss: 0.007000656332820654\n",
            "Iteration 9274, Loss: 0.011614738963544369\n",
            "Iteration 9275, Loss: 0.011210082098841667\n",
            "Iteration 9276, Loss: 0.009617730975151062\n",
            "Iteration 9277, Loss: 0.01130551565438509\n",
            "Iteration 9278, Loss: 0.010623974725604057\n",
            "Iteration 9279, Loss: 0.011201029643416405\n",
            "Iteration 9280, Loss: 0.007037361618131399\n",
            "Iteration 9281, Loss: 0.008043787442147732\n",
            "Iteration 9282, Loss: 0.009788738563656807\n",
            "Iteration 9283, Loss: 0.0167576652020216\n",
            "Iteration 9284, Loss: 0.010148740373551846\n",
            "Iteration 9285, Loss: 0.008615934289991856\n",
            "Iteration 9286, Loss: 0.01223091222345829\n",
            "Iteration 9287, Loss: 0.009732872247695923\n",
            "Iteration 9288, Loss: 0.009027206338942051\n",
            "Iteration 9289, Loss: 0.011979515664279461\n",
            "Iteration 9290, Loss: 0.007315818686038256\n",
            "Iteration 9291, Loss: 0.011002601124346256\n",
            "Iteration 9292, Loss: 0.009294974617660046\n",
            "Iteration 9293, Loss: 0.007181572262197733\n",
            "Iteration 9294, Loss: 0.010002441704273224\n",
            "Iteration 9295, Loss: 0.008631769567728043\n",
            "Iteration 9296, Loss: 0.007103197276592255\n",
            "Iteration 9297, Loss: 0.008868048898875713\n",
            "Iteration 9298, Loss: 0.00886110495775938\n",
            "Iteration 9299, Loss: 0.012587332166731358\n",
            "Iteration 9300, Loss: 0.009361487813293934\n",
            "Iteration 9301, Loss: 0.006941366009414196\n",
            "Iteration 9302, Loss: 0.007977195084095001\n",
            "Iteration 9303, Loss: 0.00692337192595005\n",
            "Iteration 9304, Loss: 0.010420598089694977\n",
            "Iteration 9305, Loss: 0.009725627489387989\n",
            "Iteration 9306, Loss: 0.011128615587949753\n",
            "Iteration 9307, Loss: 0.009328790009021759\n",
            "Iteration 9308, Loss: 0.013132703490555286\n",
            "Iteration 9309, Loss: 0.006862035486847162\n",
            "Iteration 9310, Loss: 0.01146859023720026\n",
            "Iteration 9311, Loss: 0.008986560627818108\n",
            "Iteration 9312, Loss: 0.013123881071805954\n",
            "Iteration 9313, Loss: 0.01016449835151434\n",
            "Iteration 9314, Loss: 0.01003543846309185\n",
            "Iteration 9315, Loss: 0.009432559832930565\n",
            "Iteration 9316, Loss: 0.005914734676480293\n",
            "Iteration 9317, Loss: 0.011015277355909348\n",
            "Iteration 9318, Loss: 0.004047037102282047\n",
            "Iteration 9319, Loss: 0.008241843432188034\n",
            "Iteration 9320, Loss: 0.009048020467162132\n",
            "Iteration 9321, Loss: 0.008445417508482933\n",
            "Iteration 9322, Loss: 0.009661833755671978\n",
            "Iteration 9323, Loss: 0.011192222125828266\n",
            "Iteration 9324, Loss: 0.011175940744578838\n",
            "Iteration 9325, Loss: 0.009466640651226044\n",
            "Iteration 9326, Loss: 0.009664527140557766\n",
            "Iteration 9327, Loss: 0.009150815196335316\n",
            "Iteration 9328, Loss: 0.009911470115184784\n",
            "Iteration 9329, Loss: 0.012040941044688225\n",
            "Iteration 9330, Loss: 0.010569210164248943\n",
            "Iteration 9331, Loss: 0.006876857485622168\n",
            "Iteration 9332, Loss: 0.00990974809974432\n",
            "Iteration 9333, Loss: 0.014863734133541584\n",
            "Iteration 9334, Loss: 0.006102808751165867\n",
            "Iteration 9335, Loss: 0.011219076812267303\n",
            "Iteration 9336, Loss: 0.011126026511192322\n",
            "Iteration 9337, Loss: 0.004766805097460747\n",
            "Iteration 9338, Loss: 0.00735987164080143\n",
            "Iteration 9339, Loss: 0.010010491125285625\n",
            "Iteration 9340, Loss: 0.0065656788647174835\n",
            "Iteration 9341, Loss: 0.010145140811800957\n",
            "Iteration 9342, Loss: 0.00990209449082613\n",
            "Iteration 9343, Loss: 0.008388101123273373\n",
            "Iteration 9344, Loss: 0.005370164755731821\n",
            "Iteration 9345, Loss: 0.012717166915535927\n",
            "Iteration 9346, Loss: 0.01081240363419056\n",
            "Iteration 9347, Loss: 0.007238698657602072\n",
            "Iteration 9348, Loss: 0.007141555659472942\n",
            "Iteration 9349, Loss: 0.009200637228786945\n",
            "Iteration 9350, Loss: 0.009409218095242977\n",
            "Iteration 9351, Loss: 0.006938287988305092\n",
            "Iteration 9352, Loss: 0.006814290303736925\n",
            "Iteration 9353, Loss: 0.009127655066549778\n",
            "Iteration 9354, Loss: 0.004900441505014896\n",
            "Iteration 9355, Loss: 0.009814723394811153\n",
            "Iteration 9356, Loss: 0.011583405546844006\n",
            "Iteration 9357, Loss: 0.007891063578426838\n",
            "Iteration 9358, Loss: 0.00650325883179903\n",
            "Iteration 9359, Loss: 0.00894004013389349\n",
            "Iteration 9360, Loss: 0.01024241279810667\n",
            "Iteration 9361, Loss: 0.007474385667592287\n",
            "Iteration 9362, Loss: 0.008668238297104836\n",
            "Iteration 9363, Loss: 0.008799653500318527\n",
            "Iteration 9364, Loss: 0.012352968566119671\n",
            "Iteration 9365, Loss: 0.008333519101142883\n",
            "Iteration 9366, Loss: 0.009528552182018757\n",
            "Iteration 9367, Loss: 0.007660241331905127\n",
            "Iteration 9368, Loss: 0.00944716576486826\n",
            "Iteration 9369, Loss: 0.009055396541953087\n",
            "Iteration 9370, Loss: 0.012338843196630478\n",
            "Iteration 9371, Loss: 0.009711505845189095\n",
            "Iteration 9372, Loss: 0.007048475556075573\n",
            "Iteration 9373, Loss: 0.006588958669453859\n",
            "Iteration 9374, Loss: 0.0077951946295797825\n",
            "Iteration 9375, Loss: 0.008837578818202019\n",
            "Iteration 9376, Loss: 0.010562178678810596\n",
            "Iteration 9377, Loss: 0.009022527374327183\n",
            "Iteration 9378, Loss: 0.012926463037729263\n",
            "Iteration 9379, Loss: 0.007771749049425125\n",
            "Iteration 9380, Loss: 0.0071718511171638966\n",
            "Iteration 9381, Loss: 0.00510443327948451\n",
            "Iteration 9382, Loss: 0.009076613932847977\n",
            "Iteration 9383, Loss: 0.009124853648245335\n",
            "Iteration 9384, Loss: 0.009204487316310406\n",
            "Iteration 9385, Loss: 0.008479935117065907\n",
            "Iteration 9386, Loss: 0.007933441549539566\n",
            "Iteration 9387, Loss: 0.006485339719802141\n",
            "Iteration 9388, Loss: 0.004178252071142197\n",
            "Iteration 9389, Loss: 0.01028756145387888\n",
            "Iteration 9390, Loss: 0.011462473310530186\n",
            "Iteration 9391, Loss: 0.010072370991110802\n",
            "Iteration 9392, Loss: 0.00742553872987628\n",
            "Iteration 9393, Loss: 0.007672616746276617\n",
            "Iteration 9394, Loss: 0.008426166139543056\n",
            "Iteration 9395, Loss: 0.012524809688329697\n",
            "Iteration 9396, Loss: 0.007098716218024492\n",
            "Iteration 9397, Loss: 0.010081698186695576\n",
            "Iteration 9398, Loss: 0.008644046261906624\n",
            "Iteration 9399, Loss: 0.010804029181599617\n",
            "Iteration 9400, Loss: 0.008968466892838478\n",
            "Iteration 9401, Loss: 0.004958705976605415\n",
            "Iteration 9402, Loss: 0.007141509559005499\n",
            "Iteration 9403, Loss: 0.006675640121102333\n",
            "Iteration 9404, Loss: 0.008728898130357265\n",
            "Iteration 9405, Loss: 0.007535016629844904\n",
            "Iteration 9406, Loss: 0.009646695107221603\n",
            "Iteration 9407, Loss: 0.010307191871106625\n",
            "Iteration 9408, Loss: 0.00928699504584074\n",
            "Iteration 9409, Loss: 0.008866227231919765\n",
            "Iteration 9410, Loss: 0.006845235824584961\n",
            "Iteration 9411, Loss: 0.010454495437443256\n",
            "Iteration 9412, Loss: 0.006569235119968653\n",
            "Iteration 9413, Loss: 0.01485531311482191\n",
            "Iteration 9414, Loss: 0.0076025729067623615\n",
            "Iteration 9415, Loss: 0.006032994482666254\n",
            "Iteration 9416, Loss: 0.015653911978006363\n",
            "Iteration 9417, Loss: 0.009764587506651878\n",
            "Iteration 9418, Loss: 0.006952268071472645\n",
            "Iteration 9419, Loss: 0.010365806519985199\n",
            "Iteration 9420, Loss: 0.008646160364151001\n",
            "Iteration 9421, Loss: 0.004299079068005085\n",
            "Iteration 9422, Loss: 0.017192669212818146\n",
            "Iteration 9423, Loss: 0.0077956500463187695\n",
            "Iteration 9424, Loss: 0.008062828332185745\n",
            "Iteration 9425, Loss: 0.010169091634452343\n",
            "Iteration 9426, Loss: 0.005540760699659586\n",
            "Iteration 9427, Loss: 0.007752005476504564\n",
            "Iteration 9428, Loss: 0.006959561724215746\n",
            "Iteration 9429, Loss: 0.010754345916211605\n",
            "Iteration 9430, Loss: 0.008530767634510994\n",
            "Iteration 9431, Loss: 0.007244056556373835\n",
            "Iteration 9432, Loss: 0.011718163266777992\n",
            "Iteration 9433, Loss: 0.009051989763975143\n",
            "Iteration 9434, Loss: 0.011906060390174389\n",
            "Iteration 9435, Loss: 0.014461089856922626\n",
            "Iteration 9436, Loss: 0.0044786506332457066\n",
            "Iteration 9437, Loss: 0.009712929837405682\n",
            "Iteration 9438, Loss: 0.012308400124311447\n",
            "Iteration 9439, Loss: 0.010015125386416912\n",
            "Iteration 9440, Loss: 0.00974922627210617\n",
            "Iteration 9441, Loss: 0.011372946202754974\n",
            "Iteration 9442, Loss: 0.003708684118464589\n",
            "Iteration 9443, Loss: 0.006927024107426405\n",
            "Iteration 9444, Loss: 0.009677394293248653\n",
            "Iteration 9445, Loss: 0.008260706439614296\n",
            "Iteration 9446, Loss: 0.013592314906418324\n",
            "Iteration 9447, Loss: 0.005550771486014128\n",
            "Iteration 9448, Loss: 0.011433963663876057\n",
            "Iteration 9449, Loss: 0.011440803296864033\n",
            "Iteration 9450, Loss: 0.00939248502254486\n",
            "Iteration 9451, Loss: 0.0063106464222073555\n",
            "Iteration 9452, Loss: 0.011656305752694607\n",
            "Iteration 9453, Loss: 0.0069030653685331345\n",
            "Iteration 9454, Loss: 0.009194787591695786\n",
            "Iteration 9455, Loss: 0.008864123374223709\n",
            "Iteration 9456, Loss: 0.00942762941122055\n",
            "Iteration 9457, Loss: 0.010119837708771229\n",
            "Iteration 9458, Loss: 0.009221884422004223\n",
            "Iteration 9459, Loss: 0.006911266129463911\n",
            "Iteration 9460, Loss: 0.010208262130618095\n",
            "Iteration 9461, Loss: 0.008924153633415699\n",
            "Iteration 9462, Loss: 0.011126058176159859\n",
            "Iteration 9463, Loss: 0.01211088988929987\n",
            "Iteration 9464, Loss: 0.011520707979798317\n",
            "Iteration 9465, Loss: 0.010296395979821682\n",
            "Iteration 9466, Loss: 0.008154367096722126\n",
            "Iteration 9467, Loss: 0.0067792232148349285\n",
            "Iteration 9468, Loss: 0.00859104748815298\n",
            "Iteration 9469, Loss: 0.007528743240982294\n",
            "Iteration 9470, Loss: 0.009292400442063808\n",
            "Iteration 9471, Loss: 0.010098203085362911\n",
            "Iteration 9472, Loss: 0.005991332232952118\n",
            "Iteration 9473, Loss: 0.013384255580604076\n",
            "Iteration 9474, Loss: 0.00927900429815054\n",
            "Iteration 9475, Loss: 0.0057309214025735855\n",
            "Iteration 9476, Loss: 0.01211167685687542\n",
            "Iteration 9477, Loss: 0.00939326360821724\n",
            "Iteration 9478, Loss: 0.00956260971724987\n",
            "Iteration 9479, Loss: 0.01248242985457182\n",
            "Iteration 9480, Loss: 0.0108949588611722\n",
            "Iteration 9481, Loss: 0.010286628268659115\n",
            "Iteration 9482, Loss: 0.007014458067715168\n",
            "Iteration 9483, Loss: 0.006741160061210394\n",
            "Iteration 9484, Loss: 0.009660018607974052\n",
            "Iteration 9485, Loss: 0.008219473995268345\n",
            "Iteration 9486, Loss: 0.008738096803426743\n",
            "Iteration 9487, Loss: 0.007437641266733408\n",
            "Iteration 9488, Loss: 0.009521233849227428\n",
            "Iteration 9489, Loss: 0.010986302979290485\n",
            "Iteration 9490, Loss: 0.010898199863731861\n",
            "Iteration 9491, Loss: 0.005635899491608143\n",
            "Iteration 9492, Loss: 0.009867901913821697\n",
            "Iteration 9493, Loss: 0.008826108649373055\n",
            "Iteration 9494, Loss: 0.00576021708548069\n",
            "Iteration 9495, Loss: 0.0093449167907238\n",
            "Iteration 9496, Loss: 0.010995019227266312\n",
            "Iteration 9497, Loss: 0.008326073177158833\n",
            "Iteration 9498, Loss: 0.009533350355923176\n",
            "Iteration 9499, Loss: 0.010482082143425941\n",
            "Iteration 9500, Loss: 0.011294175870716572\n",
            "Iteration 9501, Loss: 0.005959086120128632\n",
            "Iteration 9502, Loss: 0.011375929228961468\n",
            "Iteration 9503, Loss: 0.008948784321546555\n",
            "Iteration 9504, Loss: 0.011231262236833572\n",
            "Iteration 9505, Loss: 0.005892894230782986\n",
            "Iteration 9506, Loss: 0.01092207245528698\n",
            "Iteration 9507, Loss: 0.00922294333577156\n",
            "Iteration 9508, Loss: 0.00883902981877327\n",
            "Iteration 9509, Loss: 0.0028404463082551956\n",
            "Iteration 9510, Loss: 0.006721947342157364\n",
            "Iteration 9511, Loss: 0.009192783385515213\n",
            "Iteration 9512, Loss: 0.005318013485521078\n",
            "Iteration 9513, Loss: 0.011964413337409496\n",
            "Iteration 9514, Loss: 0.0076993671245872974\n",
            "Iteration 9515, Loss: 0.013641051016747952\n",
            "Iteration 9516, Loss: 0.010901902802288532\n",
            "Iteration 9517, Loss: 0.008960018865764141\n",
            "Iteration 9518, Loss: 0.010767788626253605\n",
            "Iteration 9519, Loss: 0.008730005472898483\n",
            "Iteration 9520, Loss: 0.009080317802727222\n",
            "Iteration 9521, Loss: 0.0077317566610872746\n",
            "Iteration 9522, Loss: 0.010522611439228058\n",
            "Iteration 9523, Loss: 0.009279735386371613\n",
            "Iteration 9524, Loss: 0.010214319452643394\n",
            "Iteration 9525, Loss: 0.009402605704963207\n",
            "Iteration 9526, Loss: 0.008564994670450687\n",
            "Iteration 9527, Loss: 0.011308601126074791\n",
            "Iteration 9528, Loss: 0.005918191745877266\n",
            "Iteration 9529, Loss: 0.008688177913427353\n",
            "Iteration 9530, Loss: 0.00790206529200077\n",
            "Iteration 9531, Loss: 0.0060259513556957245\n",
            "Iteration 9532, Loss: 0.0035575462970882654\n",
            "Iteration 9533, Loss: 0.008022748865187168\n",
            "Iteration 9534, Loss: 0.007874233648180962\n",
            "Iteration 9535, Loss: 0.009651489555835724\n",
            "Iteration 9536, Loss: 0.007922856137156487\n",
            "Iteration 9537, Loss: 0.009143837727606297\n",
            "Iteration 9538, Loss: 0.0092208506539464\n",
            "Iteration 9539, Loss: 0.010067922994494438\n",
            "Iteration 9540, Loss: 0.007493399549275637\n",
            "Iteration 9541, Loss: 0.011175869032740593\n",
            "Iteration 9542, Loss: 0.009346291422843933\n",
            "Iteration 9543, Loss: 0.012616883032023907\n",
            "Iteration 9544, Loss: 0.008254874497652054\n",
            "Iteration 9545, Loss: 0.013527102768421173\n",
            "Iteration 9546, Loss: 0.006879008840769529\n",
            "Iteration 9547, Loss: 0.010459576733410358\n",
            "Iteration 9548, Loss: 0.00794354360550642\n",
            "Iteration 9549, Loss: 0.010503907687962055\n",
            "Iteration 9550, Loss: 0.009280071593821049\n",
            "Iteration 9551, Loss: 0.0066805677488446236\n",
            "Iteration 9552, Loss: 0.010353117249906063\n",
            "Iteration 9553, Loss: 0.009472347795963287\n",
            "Iteration 9554, Loss: 0.006774762645363808\n",
            "Iteration 9555, Loss: 0.00754363089799881\n",
            "Iteration 9556, Loss: 0.006838957313448191\n",
            "Iteration 9557, Loss: 0.00951350573450327\n",
            "Iteration 9558, Loss: 0.011260807514190674\n",
            "Iteration 9559, Loss: 0.008221402764320374\n",
            "Iteration 9560, Loss: 0.006712648551911116\n",
            "Iteration 9561, Loss: 0.009343509562313557\n",
            "Iteration 9562, Loss: 0.0073701441287994385\n",
            "Iteration 9563, Loss: 0.009908162988722324\n",
            "Iteration 9564, Loss: 0.006500736344605684\n",
            "Iteration 9565, Loss: 0.006589867174625397\n",
            "Iteration 9566, Loss: 0.008417771197855473\n",
            "Iteration 9567, Loss: 0.00681309774518013\n",
            "Iteration 9568, Loss: 0.011295201256871223\n",
            "Iteration 9569, Loss: 0.008572360500693321\n",
            "Iteration 9570, Loss: 0.008032587356865406\n",
            "Iteration 9571, Loss: 0.010251838713884354\n",
            "Iteration 9572, Loss: 0.012364807538688183\n",
            "Iteration 9573, Loss: 0.007701448164880276\n",
            "Iteration 9574, Loss: 0.007843506522476673\n",
            "Iteration 9575, Loss: 0.010019119828939438\n",
            "Iteration 9576, Loss: 0.0061721536330878735\n",
            "Iteration 9577, Loss: 0.009195519611239433\n",
            "Iteration 9578, Loss: 0.01010076142847538\n",
            "Iteration 9579, Loss: 0.008254303596913815\n",
            "Iteration 9580, Loss: 0.009228555485606194\n",
            "Iteration 9581, Loss: 0.012390787713229656\n",
            "Iteration 9582, Loss: 0.01006667222827673\n",
            "Iteration 9583, Loss: 0.0075952401384711266\n",
            "Iteration 9584, Loss: 0.008068092167377472\n",
            "Iteration 9585, Loss: 0.011490259319543839\n",
            "Iteration 9586, Loss: 0.007736783474683762\n",
            "Iteration 9587, Loss: 0.0109469685703516\n",
            "Iteration 9588, Loss: 0.012158971279859543\n",
            "Iteration 9589, Loss: 0.007747904863208532\n",
            "Iteration 9590, Loss: 0.008158989250659943\n",
            "Iteration 9591, Loss: 0.006730546243488789\n",
            "Iteration 9592, Loss: 0.009848091751337051\n",
            "Iteration 9593, Loss: 0.008624235168099403\n",
            "Iteration 9594, Loss: 0.0046143801882863045\n",
            "Iteration 9595, Loss: 0.009621066972613335\n",
            "Iteration 9596, Loss: 0.002538652392104268\n",
            "Iteration 9597, Loss: 0.009069501422345638\n",
            "Iteration 9598, Loss: 0.007756639271974564\n",
            "Iteration 9599, Loss: 0.00844881497323513\n",
            "Iteration 9600, Loss: 0.010507812723517418\n",
            "Iteration 9601, Loss: 0.013128701597452164\n",
            "Iteration 9602, Loss: 0.009915613569319248\n",
            "Iteration 9603, Loss: 0.006359240971505642\n",
            "Iteration 9604, Loss: 0.009173574857413769\n",
            "Iteration 9605, Loss: 0.006978173740208149\n",
            "Iteration 9606, Loss: 0.010264089331030846\n",
            "Iteration 9607, Loss: 0.008285792544484138\n",
            "Iteration 9608, Loss: 0.0139662129804492\n",
            "Iteration 9609, Loss: 0.010136527940630913\n",
            "Iteration 9610, Loss: 0.004621736705303192\n",
            "Iteration 9611, Loss: 0.01313654612749815\n",
            "Iteration 9612, Loss: 0.005425522569566965\n",
            "Iteration 9613, Loss: 0.010414713062345982\n",
            "Iteration 9614, Loss: 0.010818909853696823\n",
            "Iteration 9615, Loss: 0.007325374986976385\n",
            "Iteration 9616, Loss: 0.008443810977041721\n",
            "Iteration 9617, Loss: 0.0076362332329154015\n",
            "Iteration 9618, Loss: 0.009332213550806046\n",
            "Iteration 9619, Loss: 0.009077240712940693\n",
            "Iteration 9620, Loss: 0.009256839752197266\n",
            "Iteration 9621, Loss: 0.008877361193299294\n",
            "Iteration 9622, Loss: 0.006093617994338274\n",
            "Iteration 9623, Loss: 0.01266087032854557\n",
            "Iteration 9624, Loss: 0.006760422606021166\n",
            "Iteration 9625, Loss: 0.01647173799574375\n",
            "Iteration 9626, Loss: 0.00804250780493021\n",
            "Iteration 9627, Loss: 0.008134843781590462\n",
            "Iteration 9628, Loss: 0.009312915615737438\n",
            "Iteration 9629, Loss: 0.010882016271352768\n",
            "Iteration 9630, Loss: 0.010207545943558216\n",
            "Iteration 9631, Loss: 0.013701233081519604\n",
            "Iteration 9632, Loss: 0.009042561985552311\n",
            "Iteration 9633, Loss: 0.010597589425742626\n",
            "Iteration 9634, Loss: 0.011030175723135471\n",
            "Iteration 9635, Loss: 0.005912588909268379\n",
            "Iteration 9636, Loss: 0.008689248003065586\n",
            "Iteration 9637, Loss: 0.009615940973162651\n",
            "Iteration 9638, Loss: 0.01045016385614872\n",
            "Iteration 9639, Loss: 0.008020440116524696\n",
            "Iteration 9640, Loss: 0.007648528553545475\n",
            "Iteration 9641, Loss: 0.008017580024898052\n",
            "Iteration 9642, Loss: 0.010604827664792538\n",
            "Iteration 9643, Loss: 0.009166058152914047\n",
            "Iteration 9644, Loss: 0.012479683384299278\n",
            "Iteration 9645, Loss: 0.010635546408593655\n",
            "Iteration 9646, Loss: 0.00782128144055605\n",
            "Iteration 9647, Loss: 0.006950642913579941\n",
            "Iteration 9648, Loss: 0.007563992403447628\n",
            "Iteration 9649, Loss: 0.008908679708838463\n",
            "Iteration 9650, Loss: 0.007011987268924713\n",
            "Iteration 9651, Loss: 0.012297168374061584\n",
            "Iteration 9652, Loss: 0.01089229341596365\n",
            "Iteration 9653, Loss: 0.006261605303734541\n",
            "Iteration 9654, Loss: 0.017258407548069954\n",
            "Iteration 9655, Loss: 0.008562707342207432\n",
            "Iteration 9656, Loss: 0.007622233591973782\n",
            "Iteration 9657, Loss: 0.009178572334349155\n",
            "Iteration 9658, Loss: 0.007290958426892757\n",
            "Iteration 9659, Loss: 0.010851560160517693\n",
            "Iteration 9660, Loss: 0.010397050529718399\n",
            "Iteration 9661, Loss: 0.0066977194510400295\n",
            "Iteration 9662, Loss: 0.005515933036804199\n",
            "Iteration 9663, Loss: 0.00832722894847393\n",
            "Iteration 9664, Loss: 0.00940149836242199\n",
            "Iteration 9665, Loss: 0.012292039580643177\n",
            "Iteration 9666, Loss: 0.011229909025132656\n",
            "Iteration 9667, Loss: 0.010935450904071331\n",
            "Iteration 9668, Loss: 0.007222841493785381\n",
            "Iteration 9669, Loss: 0.012074057944118977\n",
            "Iteration 9670, Loss: 0.006735186092555523\n",
            "Iteration 9671, Loss: 0.0076675559394061565\n",
            "Iteration 9672, Loss: 0.007146396208554506\n",
            "Iteration 9673, Loss: 0.008641666732728481\n",
            "Iteration 9674, Loss: 0.009480757638812065\n",
            "Iteration 9675, Loss: 0.011088825762271881\n",
            "Iteration 9676, Loss: 0.010096698068082333\n",
            "Iteration 9677, Loss: 0.00784118752926588\n",
            "Iteration 9678, Loss: 0.010142611339688301\n",
            "Iteration 9679, Loss: 0.005676297936588526\n",
            "Iteration 9680, Loss: 0.006821715738624334\n",
            "Iteration 9681, Loss: 0.008116244338452816\n",
            "Iteration 9682, Loss: 0.007796395570039749\n",
            "Iteration 9683, Loss: 0.0047419099137187\n",
            "Iteration 9684, Loss: 0.007142935413867235\n",
            "Iteration 9685, Loss: 0.011545020155608654\n",
            "Iteration 9686, Loss: 0.00866516213864088\n",
            "Iteration 9687, Loss: 0.010500689968466759\n",
            "Iteration 9688, Loss: 0.008071278221905231\n",
            "Iteration 9689, Loss: 0.01006310060620308\n",
            "Iteration 9690, Loss: 0.006877301726490259\n",
            "Iteration 9691, Loss: 0.00604618713259697\n",
            "Iteration 9692, Loss: 0.008045545779168606\n",
            "Iteration 9693, Loss: 0.011578867211937904\n",
            "Iteration 9694, Loss: 0.009235159493982792\n",
            "Iteration 9695, Loss: 0.00962450448423624\n",
            "Iteration 9696, Loss: 0.010373823344707489\n",
            "Iteration 9697, Loss: 0.00922596175223589\n",
            "Iteration 9698, Loss: 0.009667557664215565\n",
            "Iteration 9699, Loss: 0.00767399650067091\n",
            "Iteration 9700, Loss: 0.011983689852058887\n",
            "Iteration 9701, Loss: 0.00948289129883051\n",
            "Iteration 9702, Loss: 0.013078145682811737\n",
            "Iteration 9703, Loss: 0.010571539402008057\n",
            "Iteration 9704, Loss: 0.009361122734844685\n",
            "Iteration 9705, Loss: 0.009278817102313042\n",
            "Iteration 9706, Loss: 0.01158980093896389\n",
            "Iteration 9707, Loss: 0.007288535125553608\n",
            "Iteration 9708, Loss: 0.007176261395215988\n",
            "Iteration 9709, Loss: 0.008412814699113369\n",
            "Iteration 9710, Loss: 0.012617389671504498\n",
            "Iteration 9711, Loss: 0.00954196322709322\n",
            "Iteration 9712, Loss: 0.006571383681148291\n",
            "Iteration 9713, Loss: 0.011651023291051388\n",
            "Iteration 9714, Loss: 0.009573056362569332\n",
            "Iteration 9715, Loss: 0.008666165173053741\n",
            "Iteration 9716, Loss: 0.00758320139721036\n",
            "Iteration 9717, Loss: 0.009688199497759342\n",
            "Iteration 9718, Loss: 0.011249846778810024\n",
            "Iteration 9719, Loss: 0.006551762577146292\n",
            "Iteration 9720, Loss: 0.009875829331576824\n",
            "Iteration 9721, Loss: 0.009037034586071968\n",
            "Iteration 9722, Loss: 0.008191145025193691\n",
            "Iteration 9723, Loss: 0.008719871751964092\n",
            "Iteration 9724, Loss: 0.005797529127448797\n",
            "Iteration 9725, Loss: 0.012742367573082447\n",
            "Iteration 9726, Loss: 0.011666559614241123\n",
            "Iteration 9727, Loss: 0.009018855169415474\n",
            "Iteration 9728, Loss: 0.012692773714661598\n",
            "Iteration 9729, Loss: 0.011753734201192856\n",
            "Iteration 9730, Loss: 0.010268395766615868\n",
            "Iteration 9731, Loss: 0.008155097253620625\n",
            "Iteration 9732, Loss: 0.0036578162107616663\n",
            "Iteration 9733, Loss: 0.008333167992532253\n",
            "Iteration 9734, Loss: 0.005655202548950911\n",
            "Iteration 9735, Loss: 0.00873484741896391\n",
            "Iteration 9736, Loss: 0.007619401440024376\n",
            "Iteration 9737, Loss: 0.009570361115038395\n",
            "Iteration 9738, Loss: 0.006963800173252821\n",
            "Iteration 9739, Loss: 0.015457134693861008\n",
            "Iteration 9740, Loss: 0.013316177763044834\n",
            "Iteration 9741, Loss: 0.009118610993027687\n",
            "Iteration 9742, Loss: 0.011372296139597893\n",
            "Iteration 9743, Loss: 0.009521436877548695\n",
            "Iteration 9744, Loss: 0.009936118498444557\n",
            "Iteration 9745, Loss: 0.00991114042699337\n",
            "Iteration 9746, Loss: 0.005697212181985378\n",
            "Iteration 9747, Loss: 0.005806518718600273\n",
            "Iteration 9748, Loss: 0.009217695333063602\n",
            "Iteration 9749, Loss: 0.009682407602667809\n",
            "Iteration 9750, Loss: 0.010624056681990623\n",
            "Iteration 9751, Loss: 0.008729513734579086\n",
            "Iteration 9752, Loss: 0.007116095628589392\n",
            "Iteration 9753, Loss: 0.01111513003706932\n",
            "Iteration 9754, Loss: 0.007668437901884317\n",
            "Iteration 9755, Loss: 0.00415631290525198\n",
            "Iteration 9756, Loss: 0.009654357098042965\n",
            "Iteration 9757, Loss: 0.008006292395293713\n",
            "Iteration 9758, Loss: 0.008524767123162746\n",
            "Iteration 9759, Loss: 0.005921920295804739\n",
            "Iteration 9760, Loss: 0.011382881551980972\n",
            "Iteration 9761, Loss: 0.00990445539355278\n",
            "Iteration 9762, Loss: 0.009787957184016705\n",
            "Iteration 9763, Loss: 0.009333433583378792\n",
            "Iteration 9764, Loss: 0.006631375290453434\n",
            "Iteration 9765, Loss: 0.007640284486114979\n",
            "Iteration 9766, Loss: 0.010436403565108776\n",
            "Iteration 9767, Loss: 0.012013981118798256\n",
            "Iteration 9768, Loss: 0.010953118093311787\n",
            "Iteration 9769, Loss: 0.009118109941482544\n",
            "Iteration 9770, Loss: 0.008777374401688576\n",
            "Iteration 9771, Loss: 0.011683385819196701\n",
            "Iteration 9772, Loss: 0.008398100733757019\n",
            "Iteration 9773, Loss: 0.009901786223053932\n",
            "Iteration 9774, Loss: 0.005330638028681278\n",
            "Iteration 9775, Loss: 0.009558129124343395\n",
            "Iteration 9776, Loss: 0.008008718490600586\n",
            "Iteration 9777, Loss: 0.01071486622095108\n",
            "Iteration 9778, Loss: 0.007363456767052412\n",
            "Iteration 9779, Loss: 0.005831721238791943\n",
            "Iteration 9780, Loss: 0.006184911821037531\n",
            "Iteration 9781, Loss: 0.008704520761966705\n",
            "Iteration 9782, Loss: 0.00848192349076271\n",
            "Iteration 9783, Loss: 0.009226778522133827\n",
            "Iteration 9784, Loss: 0.005999953486025333\n",
            "Iteration 9785, Loss: 0.008292845450341702\n",
            "Iteration 9786, Loss: 0.00789699237793684\n",
            "Iteration 9787, Loss: 0.00931518990546465\n",
            "Iteration 9788, Loss: 0.008823986165225506\n",
            "Iteration 9789, Loss: 0.011267052963376045\n",
            "Iteration 9790, Loss: 0.008546722121536732\n",
            "Iteration 9791, Loss: 0.007923843339085579\n",
            "Iteration 9792, Loss: 0.009979561902582645\n",
            "Iteration 9793, Loss: 0.005636670161038637\n",
            "Iteration 9794, Loss: 0.012303665280342102\n",
            "Iteration 9795, Loss: 0.005377814639359713\n",
            "Iteration 9796, Loss: 0.010043717920780182\n",
            "Iteration 9797, Loss: 0.008156782947480679\n",
            "Iteration 9798, Loss: 0.009219636209309101\n",
            "Iteration 9799, Loss: 0.011532677337527275\n",
            "Iteration 9800, Loss: 0.006002975627779961\n",
            "Iteration 9801, Loss: 0.008907264098525047\n",
            "Iteration 9802, Loss: 0.005145235452800989\n",
            "Iteration 9803, Loss: 0.009776994585990906\n",
            "Iteration 9804, Loss: 0.01041800994426012\n",
            "Iteration 9805, Loss: 0.008813748136162758\n",
            "Iteration 9806, Loss: 0.009114425629377365\n",
            "Iteration 9807, Loss: 0.0077646514400839806\n",
            "Iteration 9808, Loss: 0.006425588391721249\n",
            "Iteration 9809, Loss: 0.008185201324522495\n",
            "Iteration 9810, Loss: 0.00791632104665041\n",
            "Iteration 9811, Loss: 0.007453294470906258\n",
            "Iteration 9812, Loss: 0.0066060395911335945\n",
            "Iteration 9813, Loss: 0.00851360708475113\n",
            "Iteration 9814, Loss: 0.005623659119009972\n",
            "Iteration 9815, Loss: 0.012392216362059116\n",
            "Iteration 9816, Loss: 0.008876284584403038\n",
            "Iteration 9817, Loss: 0.010774304158985615\n",
            "Iteration 9818, Loss: 0.006868541706353426\n",
            "Iteration 9819, Loss: 0.008264306001365185\n",
            "Iteration 9820, Loss: 0.007102352567017078\n",
            "Iteration 9821, Loss: 0.006391480099409819\n",
            "Iteration 9822, Loss: 0.00714305741712451\n",
            "Iteration 9823, Loss: 0.01096666045486927\n",
            "Iteration 9824, Loss: 0.009794233366847038\n",
            "Iteration 9825, Loss: 0.010689915157854557\n",
            "Iteration 9826, Loss: 0.008101500570774078\n",
            "Iteration 9827, Loss: 0.012034332379698753\n",
            "Iteration 9828, Loss: 0.006874444428831339\n",
            "Iteration 9829, Loss: 0.011317098513245583\n",
            "Iteration 9830, Loss: 0.00512167951092124\n",
            "Iteration 9831, Loss: 0.003906011115759611\n",
            "Iteration 9832, Loss: 0.011613108217716217\n",
            "Iteration 9833, Loss: 0.008022603578865528\n",
            "Iteration 9834, Loss: 0.008332882076501846\n",
            "Iteration 9835, Loss: 0.008657648228108883\n",
            "Iteration 9836, Loss: 0.0076303486712276936\n",
            "Iteration 9837, Loss: 0.010384936816990376\n",
            "Iteration 9838, Loss: 0.007317355368286371\n",
            "Iteration 9839, Loss: 0.007746828719973564\n",
            "Iteration 9840, Loss: 0.010862271301448345\n",
            "Iteration 9841, Loss: 0.010104541666805744\n",
            "Iteration 9842, Loss: 0.008110269904136658\n",
            "Iteration 9843, Loss: 0.01538379117846489\n",
            "Iteration 9844, Loss: 0.008536114357411861\n",
            "Iteration 9845, Loss: 0.006320896092802286\n",
            "Iteration 9846, Loss: 0.004957600496709347\n",
            "Iteration 9847, Loss: 0.008498192764818668\n",
            "Iteration 9848, Loss: 0.007120978552848101\n",
            "Iteration 9849, Loss: 0.012727035209536552\n",
            "Iteration 9850, Loss: 0.01057128794491291\n",
            "Iteration 9851, Loss: 0.01261147391051054\n",
            "Iteration 9852, Loss: 0.006234607193619013\n",
            "Iteration 9853, Loss: 0.010944589041173458\n",
            "Iteration 9854, Loss: 0.007380148395895958\n",
            "Iteration 9855, Loss: 0.014228856191039085\n",
            "Iteration 9856, Loss: 0.0063778492622077465\n",
            "Iteration 9857, Loss: 0.010640214197337627\n",
            "Iteration 9858, Loss: 0.011071090586483479\n",
            "Iteration 9859, Loss: 0.01701870560646057\n",
            "Iteration 9860, Loss: 0.011686984449625015\n",
            "Iteration 9861, Loss: 0.00738272862508893\n",
            "Iteration 9862, Loss: 0.011353801935911179\n",
            "Iteration 9863, Loss: 0.013561177998781204\n",
            "Iteration 9864, Loss: 0.011862767860293388\n",
            "Iteration 9865, Loss: 0.008491828106343746\n",
            "Iteration 9866, Loss: 0.010150112211704254\n",
            "Iteration 9867, Loss: 0.009583347477018833\n",
            "Iteration 9868, Loss: 0.0123594393953681\n",
            "Iteration 9869, Loss: 0.007762713823467493\n",
            "Iteration 9870, Loss: 0.009970355778932571\n",
            "Iteration 9871, Loss: 0.0030737039633095264\n",
            "Iteration 9872, Loss: 0.00665624812245369\n",
            "Iteration 9873, Loss: 0.014099356718361378\n",
            "Iteration 9874, Loss: 0.010128520429134369\n",
            "Iteration 9875, Loss: 0.007714740000665188\n",
            "Iteration 9876, Loss: 0.0070001534186303616\n",
            "Iteration 9877, Loss: 0.0096054058521986\n",
            "Iteration 9878, Loss: 0.010048235766589642\n",
            "Iteration 9879, Loss: 0.004150704946368933\n",
            "Iteration 9880, Loss: 0.011574970558285713\n",
            "Iteration 9881, Loss: 0.011761459521949291\n",
            "Iteration 9882, Loss: 0.00750957103446126\n",
            "Iteration 9883, Loss: 0.00904762465506792\n",
            "Iteration 9884, Loss: 0.008467192761600018\n",
            "Iteration 9885, Loss: 0.009281383827328682\n",
            "Iteration 9886, Loss: 0.006555452477186918\n",
            "Iteration 9887, Loss: 0.007611292414367199\n",
            "Iteration 9888, Loss: 0.009646597318351269\n",
            "Iteration 9889, Loss: 0.0048093535006046295\n",
            "Iteration 9890, Loss: 0.012370701879262924\n",
            "Iteration 9891, Loss: 0.00981389544904232\n",
            "Iteration 9892, Loss: 0.006640840321779251\n",
            "Iteration 9893, Loss: 0.01080318447202444\n",
            "Iteration 9894, Loss: 0.008411559276282787\n",
            "Iteration 9895, Loss: 0.011374281719326973\n",
            "Iteration 9896, Loss: 0.008953954093158245\n",
            "Iteration 9897, Loss: 0.011833836324512959\n",
            "Iteration 9898, Loss: 0.008201571181416512\n",
            "Iteration 9899, Loss: 0.006782693322747946\n",
            "Iteration 9900, Loss: 0.007685353513807058\n",
            "Iteration 9901, Loss: 0.008519381284713745\n",
            "Iteration 9902, Loss: 0.009898439049720764\n",
            "Iteration 9903, Loss: 0.010158686898648739\n",
            "Iteration 9904, Loss: 0.007375442888587713\n",
            "Iteration 9905, Loss: 0.006680769845843315\n",
            "Iteration 9906, Loss: 0.007028831169009209\n",
            "Iteration 9907, Loss: 0.010396886616945267\n",
            "Iteration 9908, Loss: 0.008906036615371704\n",
            "Iteration 9909, Loss: 0.0066242399625480175\n",
            "Iteration 9910, Loss: 0.009855513460934162\n",
            "Iteration 9911, Loss: 0.006631064228713512\n",
            "Iteration 9912, Loss: 0.009239929728209972\n",
            "Iteration 9913, Loss: 0.0064096031710505486\n",
            "Iteration 9914, Loss: 0.01335175521671772\n",
            "Iteration 9915, Loss: 0.012376641854643822\n",
            "Iteration 9916, Loss: 0.004403640050441027\n",
            "Iteration 9917, Loss: 0.008785014972090721\n",
            "Iteration 9918, Loss: 0.010934590362012386\n",
            "Iteration 9919, Loss: 0.0101473955437541\n",
            "Iteration 9920, Loss: 0.012544493190944195\n",
            "Iteration 9921, Loss: 0.004255978390574455\n",
            "Iteration 9922, Loss: 0.009273620322346687\n",
            "Iteration 9923, Loss: 0.01187595259398222\n",
            "Iteration 9924, Loss: 0.006069930735975504\n",
            "Iteration 9925, Loss: 0.011643829755485058\n",
            "Iteration 9926, Loss: 0.007615736220031977\n",
            "Iteration 9927, Loss: 0.010181475430727005\n",
            "Iteration 9928, Loss: 0.015314347110688686\n",
            "Iteration 9929, Loss: 0.005887444131076336\n",
            "Iteration 9930, Loss: 0.009718024171888828\n",
            "Iteration 9931, Loss: 0.009121828712522984\n",
            "Iteration 9932, Loss: 0.007627204991877079\n",
            "Iteration 9933, Loss: 0.009552315808832645\n",
            "Iteration 9934, Loss: 0.00994476955384016\n",
            "Iteration 9935, Loss: 0.009302852675318718\n",
            "Iteration 9936, Loss: 0.00790288858115673\n",
            "Iteration 9937, Loss: 0.010773327201604843\n",
            "Iteration 9938, Loss: 0.011664903722703457\n",
            "Iteration 9939, Loss: 0.009916354902088642\n",
            "Iteration 9940, Loss: 0.010227262042462826\n",
            "Iteration 9941, Loss: 0.010654585435986519\n",
            "Iteration 9942, Loss: 0.009163795039057732\n",
            "Iteration 9943, Loss: 0.00924868043512106\n",
            "Iteration 9944, Loss: 0.0104012256488204\n",
            "Iteration 9945, Loss: 0.009962804615497589\n",
            "Iteration 9946, Loss: 0.008631006814539433\n",
            "Iteration 9947, Loss: 0.009495542384684086\n",
            "Iteration 9948, Loss: 0.010724031366407871\n",
            "Iteration 9949, Loss: 0.0045574018731713295\n",
            "Iteration 9950, Loss: 0.007553780451416969\n",
            "Iteration 9951, Loss: 0.00866122730076313\n",
            "Iteration 9952, Loss: 0.008227402344346046\n",
            "Iteration 9953, Loss: 0.014865067787468433\n",
            "Iteration 9954, Loss: 0.0064000533893704414\n",
            "Iteration 9955, Loss: 0.008397265337407589\n",
            "Iteration 9956, Loss: 0.01046093087643385\n",
            "Iteration 9957, Loss: 0.0045007262378931046\n",
            "Iteration 9958, Loss: 0.007154908496886492\n",
            "Iteration 9959, Loss: 0.006501738913357258\n",
            "Iteration 9960, Loss: 0.009336329996585846\n",
            "Iteration 9961, Loss: 0.010377027094364166\n",
            "Iteration 9962, Loss: 0.011522061191499233\n",
            "Iteration 9963, Loss: 0.010413079522550106\n",
            "Iteration 9964, Loss: 0.010457338765263557\n",
            "Iteration 9965, Loss: 0.007837940007448196\n",
            "Iteration 9966, Loss: 0.004997483920305967\n",
            "Iteration 9967, Loss: 0.008394709788262844\n",
            "Iteration 9968, Loss: 0.010933403857052326\n",
            "Iteration 9969, Loss: 0.007895407266914845\n",
            "Iteration 9970, Loss: 0.006709670182317495\n",
            "Iteration 9971, Loss: 0.010752860456705093\n",
            "Iteration 9972, Loss: 0.011510180309414864\n",
            "Iteration 9973, Loss: 0.004444156773388386\n",
            "Iteration 9974, Loss: 0.007132013328373432\n",
            "Iteration 9975, Loss: 0.006638502702116966\n",
            "Iteration 9976, Loss: 0.007990526966750622\n",
            "Iteration 9977, Loss: 0.009653216227889061\n",
            "Iteration 9978, Loss: 0.008304736576974392\n",
            "Iteration 9979, Loss: 0.006271891761571169\n",
            "Iteration 9980, Loss: 0.008224335499107838\n",
            "Iteration 9981, Loss: 0.004633071832358837\n",
            "Iteration 9982, Loss: 0.008626693859696388\n",
            "Iteration 9983, Loss: 0.00682698143646121\n",
            "Iteration 9984, Loss: 0.009331037290394306\n",
            "Iteration 9985, Loss: 0.007218165788799524\n",
            "Iteration 9986, Loss: 0.0061472319066524506\n",
            "Iteration 9987, Loss: 0.009377880953252316\n",
            "Iteration 9988, Loss: 0.008665528148412704\n",
            "Iteration 9989, Loss: 0.004883171524852514\n",
            "Iteration 9990, Loss: 0.007229719776660204\n",
            "Iteration 9991, Loss: 0.008619437925517559\n",
            "Iteration 9992, Loss: 0.008982009254395962\n",
            "Iteration 9993, Loss: 0.01191392820328474\n",
            "Iteration 9994, Loss: 0.00808749720454216\n",
            "Iteration 9995, Loss: 0.00823309924453497\n",
            "Iteration 9996, Loss: 0.00871167704463005\n",
            "Iteration 9997, Loss: 0.01013678964227438\n",
            "Iteration 9998, Loss: 0.01109697762876749\n",
            "Iteration 9999, Loss: 0.00867087859660387\n",
            "Iteration 10000, Loss: 0.00622665835544467\n",
            "Test Loss: 0.03186797723174095\n",
            "Training took 275.441s in total.\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# CUDA device (to use GPU: install PyTorch with CUDA from https://pytorch.org)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Device: {0}'.format(device))\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# Build the model\n",
        "num_class = 4\n",
        "model = UNet(input_channel=1, output_channel=num_class, num_filter=16)\n",
        "model = model.to(device)\n",
        "params = list(model.parameters())\n",
        "\n",
        "model_dir = 'saved_models'\n",
        "if not os.path.exists(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(params, lr=1e-3)\n",
        "\n",
        "# Segmentation loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Datasets\n",
        "train_set = BrainImageSet('Task01_BrainTumour_2D/training_images', 'Task01_BrainTumour_2D/training_labels')\n",
        "test_set = BrainImageSet('Task01_BrainTumour_2D/test_images', 'Task01_BrainTumour_2D/test_labels')\n",
        "\n",
        "# Train the model\n",
        "# Note: when you debug the model, you may reduce the number of iterations or batch size to save time.\n",
        "num_iter = 10000\n",
        "train_batch_size = 16\n",
        "eval_batch_size = 16\n",
        "start = time.time()\n",
        "for it in range(1, 1 + num_iter):\n",
        "    # Set the modules in training mode, which will have effects on certain modules, e.g. dropout or batchnorm.\n",
        "    start_iter = time.time()\n",
        "    model.train()\n",
        "\n",
        "    # Get a batch of images and labels\n",
        "    images, labels = train_set.get_random_batch(train_batch_size)\n",
        "    images, labels = torch.from_numpy(images), torch.from_numpy(labels)\n",
        "    images, labels = images.to(device, dtype=torch.float32), labels.to(device, dtype=torch.long)\n",
        "    logits = model(images)\n",
        "\n",
        "    # Perform optimisation and print out the training loss\n",
        "    ### Insert your code ###\n",
        "    loss = criterion(logits, labels)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Iteration {it}, Loss: {loss.item()}\")\n",
        "    ### End of your code ###\n",
        "\n",
        "    # Evaluate\n",
        "    if it % 1000 == 0:\n",
        "        model.eval()\n",
        "        # Disabling gradient calculation during reference to reduce memory consumption\n",
        "        with torch.no_grad():\n",
        "            # Evaluate on a batch of test images and print out the test loss\n",
        "            ### Insert your code ###\n",
        "            test_images, test_labels = test_set.get_random_batch(eval_batch_size)\n",
        "            test_images, test_labels = torch.from_numpy(test_images), torch.from_numpy(test_labels)\n",
        "            test_images, test_labels = test_images.to(device, dtype=torch.float32), test_labels.to(device, dtype=torch.long)\n",
        "            test_logits = model(test_images)\n",
        "            test_loss = criterion(test_logits, test_labels)\n",
        "            print(f\"Test Loss: {test_loss.item()}\")\n",
        "            ### End of your code ###\n",
        "\n",
        "    # Save the model\n",
        "    if it % 5000 == 0:\n",
        "        torch.save(model.state_dict(), os.path.join(model_dir, 'model_{0}.pt'.format(it)))\n",
        "print('Training took {:.3f}s in total.'.format(time.time() - start))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89yjxjGyb6yT"
      },
      "source": [
        "## Q5. Deploy the trained model to a random set of 4 test images and visualise the automated segmentation.\n",
        "\n",
        "You can show the images as a 4 x 3 panel. Each row shows one example, with the 3 columns being the test image, automated segmentation and ground truth segmentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wZeLE0qZjd2j"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m model_path = \u001b[33m\"\u001b[39m\u001b[33msaved_models/model_10000.pt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# load the trained model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mmodel\u001b[49m.load_state_dict(torch.load(model_path, map_location = torch.device(\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m)))\n\u001b[32m      6\u001b[39m model.eval()\n\u001b[32m      8\u001b[39m num_samples = \u001b[32m4\u001b[39m\n",
            "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "### Insert your code ###\n",
        "model_path = \"saved_models/model_10000.pt\"\n",
        "\n",
        "# load the trained model\n",
        "model.load_state_dict(torch.load(model_path, map_location = torch.device('cuda' if torch.cuda.is_available() else 'cpu')))\n",
        "model.eval()\n",
        "\n",
        "num_samples = 4\n",
        "#get our 4 test images and labels\n",
        "test_images, test_labels = test_set.get_random_batch(num_samples)\n",
        "\n",
        "# move to correct device and type\n",
        "test_images = torch.from_numpy(test_images).to(device, dtype=torch.float32)\n",
        "test_labels = torch.from_numpy(test_labels).to(device, dtype=torch.long)\n",
        "\n",
        "# run the model at inference time with no backprop\n",
        "with torch.no_grad():\n",
        "    predictions = model(test_images)\n",
        "\n",
        "# get the predictions\n",
        "predictions = torch.argmax(predictions, dim=1).cpu().numpy()\n",
        "\n",
        "# format test images + labels\n",
        "test_images = test_images.cpu().numpy()\n",
        "test_labels = test_labels.cpu().numpy()\n",
        "\n",
        "# segmentation map\n",
        "segmentation_cmap = colors.ListedColormap(['black', 'green', 'blue', 'red'])\n",
        "\n",
        "# plot the results\n",
        "fig, axes = plt.subplots(num_samples, 3, figsize=(10, 12))\n",
        "\n",
        "for i in range(num_samples):\n",
        "    # plot the test image\n",
        "    axes[i, 0].imshow(test_images[i, 0], cmap='gray')\n",
        "    axes[i, 0].set_title('Test Image')\n",
        "    axes[i, 0].axis('off')\n",
        "\n",
        "    # model prediction\n",
        "    axes[i, 1].imshow(predictions[i], cmap=segmentation_cmap)\n",
        "    axes[i, 1].set_title('Model Prediction')\n",
        "    axes[i, 1].axis('off')\n",
        "\n",
        "    # plot the ground truth segmentation\n",
        "    axes[i, 2].imshow(test_labels[i], cmap=segmentation_cmap)\n",
        "    axes[i, 2].set_title('Ground Truth')\n",
        "    axes[i, 2].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### End of your code ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cj3Qusin_s_r"
      },
      "source": [
        "## Q6. Discussion. Does your trained model work well? How would you improve this model so it can be deployed to the real clinic?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVwEtDKIdTRs"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
